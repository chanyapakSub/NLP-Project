{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4aff11",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c2228",
   "metadata": {},
   "source": [
    "## Environment & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd19806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 0)  Install (first‐time only) & import libs\n",
    "# ---------------------------------------------\n",
    "# !pip install -q datasets transformers emoji==2.10.0 tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import emoji\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef5e48",
   "metadata": {},
   "source": [
    "## Load SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1a8833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 14732/14732 [00:00<00:00, 42644.42 examples/s]\n",
      "Generating test split: 100%|██████████| 819/819 [00:00<00:00, 7962.06 examples/s]\n",
      "Generating validation split: 100%|██████████| 818/818 [00:00<00:00, 8153.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1) Load SAMSum — 14 732 / 819 / 818 dialogues\n",
    "# ---------------------------------------------------------\n",
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35306eb1",
   "metadata": {},
   "source": [
    "## Build an emoji vocabulary and speaker token & Build / extend the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa85040",
   "metadata": {},
   "source": [
    "count [UNK] occurrences in one HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80902ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def count_unk(ds, tokenizer, field=\"dialogue\", batch_size=1024):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    total_unk, total_tokens = 0, 0\n",
    "\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc=\"Tokenising\"):\n",
    "        batch_texts = ds[i : i + batch_size][field]\n",
    "        enc = tokenizer(batch_texts, add_special_tokens=True, padding=False, truncation=False)\n",
    "        for ids in enc[\"input_ids\"]:\n",
    "            arr = np.array(ids)\n",
    "            total_unk += np.sum(arr == unk_id)\n",
    "            total_tokens += len(arr)\n",
    "    return total_unk, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468907",
   "metadata": {},
   "source": [
    "BEFORE adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0622d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising:   0%|          | 0/15 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 21.67it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 35.38it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 33.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts BEFORE adding emoji tokens\n",
      "train     :     3758  (0.185% of tokens)\n",
      "validation:      191  (0.174% of tokens)\n",
      "test      :      195  (0.170% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_base = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "unk_stats_before = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_before[split] = count_unk(raw_ds[split], tok_base)\n",
    "print(\"\\n[UNK] counts BEFORE adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_before.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc3a6b",
   "metadata": {},
   "source": [
    "สร้าง EMOJI_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30aeea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emojis found: 305\n"
     ]
    }
   ],
   "source": [
    "# ถ้า kernel เพิ่งรีสตาร์ต ตัวแปรจะหายหมด\n",
    "# สร้างชุด emoji ใหม่จาก raw_ds\n",
    "from typing import List\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    return [ch for ch in text if ch in emoji.EMOJI_DATA]\n",
    "\n",
    "emoji_set = set()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for dlg in raw_ds[split][\"dialogue\"]:\n",
    "        emoji_set.update(extract_emojis(dlg))\n",
    "\n",
    "EMOJI_TOKENS = sorted(emoji_set)          # ≈ 300-320 รายการ\n",
    "print(f\"Unique emojis found: {len(EMOJI_TOKENS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aebd7e",
   "metadata": {},
   "source": [
    "Extend tokenizer with emojis + speaker tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef28354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size : 30522\n",
      "Added new tokens     : 315  (emoji = 305, speaker = 10)\n",
      "New vocab size       : 30836\n",
      "\n",
      "First 20 emoji tokens: ['‼', '⏱', '☀', '☂', '☔', '☕', '☘', '☝', '☠', '☢', '☹', '☺', '♀', '♂', '♥', '♻', '⚪', '⚫', '⚰', '⚽']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su/vocab.txt',\n",
       " 'tokenizer_samsum_su/added_tokens.json',\n",
       " 'tokenizer_samsum_su/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------- 1) โหลด tokenizer ดั้งเดิม ----------\n",
    "tok_base = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_orig = len(tok_base)\n",
    "\n",
    "# ---------- 2) เตรียมชุด token ใหม่ ----------\n",
    "#   • EMOJI_TOKENS  : ทุกอิโมจิที่ “พบอย่างน้อย 1 ครั้ง” ใน SAMSum\n",
    "#   • SPEAKER_TOKENS: [S1] – [S10]\n",
    "SPEAKER_TOKENS = [f\"[S{i}]\" for i in range(1, 11)]\n",
    "new_tokens = EMOJI_TOKENS + SPEAKER_TOKENS\n",
    "\n",
    "# ---------- 3) สร้าง tokenizer สำเนาแล้วเพิ่ม token ----------\n",
    "tok_ext = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "added = tok_ext.add_tokens(new_tokens)\n",
    "vocab_new = len(tok_ext)\n",
    "\n",
    "# ---------- 4) แสดงผล ----------\n",
    "print(f\"Original vocab size : {vocab_orig}\")\n",
    "print(f\"Added new tokens     : {added}  \"\n",
    "      f\"(emoji = {len(EMOJI_TOKENS)}, speaker = {len(SPEAKER_TOKENS)})\")\n",
    "print(f\"New vocab size       : {vocab_new}\")\n",
    "\n",
    "# (Optional) พิมพ์ตัวอย่างอิโมจิ 20 ตัวแรก\n",
    "print(\"\\nFirst 20 emoji tokens:\", EMOJI_TOKENS[:20])\n",
    "\n",
    "tok_ext.save_pretrained(\"tokenizer_samsum_su\")   # โฟลเดอร์ใหม่"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc38ac",
   "metadata": {},
   "source": [
    "AFTER adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6bc9fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising:   0%|          | 0/15 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 21.41it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 33.54it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 31.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts AFTER adding emoji tokens\n",
      "train     :      451  (0.022% of tokens)\n",
      "validation:        4  (0.004% of tokens)\n",
      "test      :       24  (0.021% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unk_stats_after = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_after[split] = count_unk(raw_ds[split], tok_ext)\n",
    "print(\"\\n[UNK] counts AFTER adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_after.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb039",
   "metadata": {},
   "source": [
    "reduction check in UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8480431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Δ [UNK] (before ➜ after):\n",
      "train     : +3307  fewer UNKs  (↓88.00%)\n",
      "validation: +187  fewer UNKs  (↓97.91%)\n",
      "test      : +171  fewer UNKs  (↓87.69%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nΔ [UNK] (before ➜ after):\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    u0, _ = unk_stats_before[split]\n",
    "    u1, _ = unk_stats_after[split]\n",
    "    print(f\"{split:<10}: {u0-u1:+d}  fewer UNKs  (↓{(u0-u1)/u0:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd47d8f",
   "metadata": {},
   "source": [
    "## Preprocess SAMSum Dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d4138",
   "metadata": {},
   "source": [
    "Speaker-name mapping → [S#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20077039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) Helper to replace speaker names by [S#]\n",
    "# ---------------------------------------------------------\n",
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10\n",
    "                 ) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns dialogue with names replaced by [S#] and a mapping dict.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:                # safety – keep line as is\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:      # truncate extra speakers\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        new_lines.append(f\"{speaker_map.get(name, '[SUNK]')}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad9bc",
   "metadata": {},
   "source": [
    "Insert [SEP] after every utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c65bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02e888",
   "metadata": {},
   "source": [
    "Switching-Utterance corruption\n",
    "- Hyper-parameters: Pu = 1.0, Pn = 0/1\n",
    "\n",
    "โดยที่\n",
    "\n",
    "Pu (permute-utterance prob.) ความน่าจะเป็นที่ แต่ละ utterance จะถูกเลือก ใส่ลงในชุดที่นำไปสับตำแหน่ง\n",
    "\n",
    "- pu = 1.0 แสดงว่าบังคับเลือกทุกบรรทัดแล้วค่อยสับคำแบบสุ่ม\n",
    "\n",
    "Pn (name-mask prob.) ความน่าจะเป็นที่ token [S#] ด้านหน้าจะถูกเปลี่ยนเป็น [MASK]\n",
    "\n",
    "- pn = 0.0 แสดงว่า ไม่ mask, โมเดลเห็น speaker tag\n",
    "\n",
    "- pn = 1.0 แสดงว่า mask หมด, บังคับดู context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ef01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_switching_utterance(dialogue: str,\n",
    "                             pu: float = 1.0,\n",
    "                             pn: float = 0.0,\n",
    "                             rng: random.Random = random\n",
    "                            ) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    • dialogue  - speaker-tokenised, SEP-inserted string\n",
    "    • pu        - prob. an utterance is selected for permutation\n",
    "    • pn        - prob. we MASK the speaker token (⇒ [MASK])\n",
    "    Returns:\n",
    "        corrupted_dialogue, labels_per_utt  (1 = permuted (สลับบทพูด), 0 = original)\n",
    "    \"\"\"\n",
    "    # 1) split back into utterances\n",
    "    utts = [u.strip() for u in dialogue.split(\"[SEP]\") if u.strip()]\n",
    "    idxs = list(range(len(utts)))\n",
    "\n",
    "    # 2) pick indices to permute\n",
    "    perm_idx = [i for i in idxs if rng.random() < pu]\n",
    "    shuffled = perm_idx.copy()\n",
    "    rng.shuffle(shuffled)                 # in-place\n",
    "    perm_map = dict(zip(perm_idx, shuffled))\n",
    "\n",
    "    # 3) build new utterance list, labels\n",
    "    new_utts, labels = [], []\n",
    "    for i in idxs:\n",
    "        src = perm_map.get(i, i)          # swapped or same\n",
    "        u = utts[src]\n",
    "        # optionally mask speaker token ([S#]: → [MASK]:)\n",
    "        if rng.random() < pn:\n",
    "            u = re.sub(r\"^\\[S\\d+\\]\", \"[MASK]\", u)\n",
    "        new_utts.append(u)\n",
    "        labels.append(int(src != i))      # 1 if permuted\n",
    "    corrupted = \" [SEP] \".join(new_utts) + \" [SEP]\"\n",
    "    return corrupted, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d05b",
   "metadata": {},
   "source": [
    "## Switching-Utterance (SU) pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5949ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building SU train: 100%|██████████| 14732/14732 [00:07<00:00, 1850.04 examples/s]\n",
      "Building SU validation: 100%|██████████| 818/818 [00:00<00:00, 1841.35 examples/s]\n",
      "Building SU test: 100%|██████████| 819/819 [00:00<00:00, 1815.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 581300.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 204161.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 201261.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7) Create HF Datasets with tokenised inputs, attention,\n",
    "#    SEP positions, and per-utterance labels\n",
    "# ---------------------------------------------------------\n",
    "MAX_LEN = 512                          # paper setting\n",
    "Pu, Pn = 1.0, 0.0                      # best config in Table 2\n",
    "\n",
    "def preprocess_example(example, split):\n",
    "    # a) replace speakers & add SEP\n",
    "    dlg, _ = map_speakers(example[\"dialogue\"])\n",
    "    dlg = add_sep_every_utt(dlg)\n",
    "\n",
    "    # b) corruption\n",
    "    corrupted, labels = make_switching_utterance(dlg, Pu, Pn)\n",
    "\n",
    "    # c) tokenize (truncate if >512 tokens)\n",
    "    enc = tok(corrupted,\n",
    "              truncation=True, max_length=MAX_LEN,\n",
    "              padding=\"max_length\")\n",
    "    \n",
    "    # d) find SEP token positions (needed for loss later)\n",
    "    sep_id = tok(\"[SEP]\")[\"input_ids\"][0]\n",
    "    sep_positions = [i for i, id_ in enumerate(enc[\"input_ids\"])\n",
    "                     if id_ == sep_id][:len(labels)]  # clip if truncated\n",
    "\n",
    "    enc[\"labels\"] = labels[:len(sep_positions)]\n",
    "    enc[\"sep_positions\"] = sep_positions\n",
    "    enc[\"dialogue_len\"] = len(labels)\n",
    "    return enc\n",
    "\n",
    "su_ds = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    su_ds[split] = raw_ds[split].map(\n",
    "        preprocess_example,\n",
    "        fn_kwargs={\"split\": split},\n",
    "        remove_columns=raw_ds[split].column_names,\n",
    "        desc=f\"Building SU {split}\"\n",
    "    )\n",
    "\n",
    "su_ds.save_to_disk(\"data/samsum_switching_utterance\")\n",
    "print(su_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4605bbc",
   "metadata": {},
   "source": [
    "ไฟล์ Arrow ถูกบันทึกไว้ที่ data/samsum_switching_utterance/ พร้อมฟิลด์ input_ids / attention_mask / labels / sep_positions / dialogue_len."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b54f7",
   "metadata": {},
   "source": [
    "## Self-supervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c9c0",
   "metadata": {},
   "source": [
    "ใช้ Dataset เฉพาะส่วนของ train ของ SAMSum มาทำการ pre_train แล้วใช้ validation ไว้ดู early-stopping / tuning ส่วน test ต้องไม่ถูกแตะ เพื่อไม่ให้โมเดล “เห็น” บทสนทนาที่จะใช้วัด ROUGE ภายหลัง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363e9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc246a",
   "metadata": {},
   "source": [
    "Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da87342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # paper 128\n",
    "MAX_LEN    = 512 # paper 512\n",
    "LR         = 3e-5\n",
    "WARMUP     = 500\n",
    "MAX_STEPS  = 5000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc898da",
   "metadata": {},
   "source": [
    "Dataset & collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5875dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_switching_utterance\")\n",
    "\n",
    "# -------------------------------\n",
    "# COLLATE FUNCTION\n",
    "# -------------------------------\n",
    "def collate_fn(batch):\n",
    "    keys = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "    inputs = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
    "    labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "    sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "    return inputs, labels, sep_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ca8c2",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f94a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MODEL\n",
    "# -------------------------------\n",
    "class SepClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, sep_positions):\n",
    "        hidden_states = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # Collect hidden states at each [SEP] position\n",
    "        # sep_vecs = [hidden_states[i, pos] for i, pos in enumerate(sep_positions)]\n",
    "        sep_vecs = []\n",
    "        for i, pos_tensor in enumerate(sep_positions):\n",
    "            pos_tensor = pos_tensor.to(hidden_states.device).long()  # <-- เพิ่มการ cast\n",
    "            sep_vecs.append(hidden_states[i].index_select(0, pos_tensor))  # (U_i, H)\n",
    "\n",
    "        sep_vecs = torch.cat(sep_vecs, dim=0)  # Shape: (total_seps, hidden_size)\n",
    "        logits = self.classifier(self.dropout(sep_vecs)).squeeze(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307a9de",
   "metadata": {},
   "source": [
    "Training loop (train model until the train loss converged (upper bounded by 5k steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26145b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# INITIALIZATION\n",
    "# -------------------------------\n",
    "model = SepClassifier().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "model.bert.resize_token_embeddings(len(tokenizer)) # Adjust embedding size for extended tokens\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = min(MAX_STEPS, len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):  # loop until MAX_STEPS reached\n",
    "    for inputs, label_lists, sep_lists in train_loader:\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        flat_labels = torch.cat(label_lists).to(DEVICE)\n",
    "\n",
    "        logits = model(**inputs, sep_positions=sep_lists)\n",
    "        loss = loss_fn(logits, flat_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step:4d}/{total_steps} | Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    if step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE MODEL\n",
    "# -------------------------------\n",
    "torch.save(model.state_dict(), \"bert_su_pretrained.pt\")\n",
    "print(\"Model saved to 'bert_su_pretrained.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459a3fb",
   "metadata": {},
   "source": [
    "Validation & early-stop (optional)\n",
    "\n",
    "- Use the same DataLoader/loop on su_ds[\"validation\"], compute average BCE loss; if it plateaus you can stop earlier than 5 k steps (what the authors mean by “until train loss converged”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b890a7",
   "metadata": {},
   "source": [
    "# Create Summarization Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e9327",
   "metadata": {},
   "source": [
    "# Fine-tuning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsum-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
