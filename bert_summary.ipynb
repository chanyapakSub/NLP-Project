{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4aff11",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c2228",
   "metadata": {},
   "source": [
    "## Environment & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd19806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 0)  Install (first‐time only) & import libs\n",
    "# ---------------------------------------------\n",
    "# !pip install -q datasets transformers emoji==2.10.0 tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import emoji\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef5e48",
   "metadata": {},
   "source": [
    "## Load SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1a8833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/drl-68/.cache/huggingface/modules/datasets_modules/datasets/samsum/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e (last modified on Mon May  5 10:16:55 2025) since it couldn't be found locally at samsum, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1) Load SAMSum — 14 732 / 819 / 818 dialogues\n",
    "# ---------------------------------------------------------\n",
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35306eb1",
   "metadata": {},
   "source": [
    "## Build an emoji vocabulary and speaker token & Build / extend the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa85040",
   "metadata": {},
   "source": [
    "count [UNK] occurrences in one HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80902ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def count_unk(ds, tokenizer, field=\"dialogue\", batch_size=1024):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    total_unk, total_tokens = 0, 0\n",
    "\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc=\"Tokenising\"):\n",
    "        batch_texts = ds[i : i + batch_size][field]\n",
    "        enc = tokenizer(batch_texts, add_special_tokens=True, padding=False, truncation=False)\n",
    "        for ids in enc[\"input_ids\"]:\n",
    "            arr = np.array(ids)\n",
    "            total_unk += np.sum(arr == unk_id)\n",
    "            total_tokens += len(arr)\n",
    "    return total_unk, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468907",
   "metadata": {},
   "source": [
    "BEFORE adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0622d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising:   0%|          | 0/15 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 22.91it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 38.01it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 35.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts BEFORE adding emoji tokens\n",
      "train     :     3758  (0.185% of tokens)\n",
      "validation:      191  (0.174% of tokens)\n",
      "test      :      195  (0.170% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_base = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "unk_stats_before = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_before[split] = count_unk(raw_ds[split], tok_base)\n",
    "print(\"\\n[UNK] counts BEFORE adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_before.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc3a6b",
   "metadata": {},
   "source": [
    "สร้าง EMOJI_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aeea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emojis found: 305\n"
     ]
    }
   ],
   "source": [
    "# ถ้า kernel เพิ่งรีสตาร์ต ตัวแปรจะหายหมด\n",
    "# สร้างชุด emoji ใหม่จาก raw_ds\n",
    "from typing import List\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    return [ch for ch in text if ch in emoji.EMOJI_DATA]\n",
    "\n",
    "emoji_set = set()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for dlg in raw_ds[split][\"dialogue\"]:\n",
    "        emoji_set.update(extract_emojis(dlg))\n",
    "\n",
    "EMOJI_TOKENS = sorted(emoji_set)          # ≈ 300-320 รายการ\n",
    "print(f\"Unique emojis found: {len(EMOJI_TOKENS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aebd7e",
   "metadata": {},
   "source": [
    "Extend tokenizer with emojis + speaker tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef28354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size : 30522\n",
      "Added new tokens     : 315  (emoji = 305, speaker = 10)\n",
      "New vocab size       : 30836\n",
      "\n",
      "First 20 emoji tokens: ['‼', '⏱', '☀', '☂', '☔', '☕', '☘', '☝', '☠', '☢', '☹', '☺', '♀', '♂', '♥', '♻', '⚪', '⚫', '⚰', '⚽']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su/vocab.txt',\n",
       " 'tokenizer_samsum_su/added_tokens.json',\n",
       " 'tokenizer_samsum_su/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------- 1) โหลด tokenizer ดั้งเดิม ----------\n",
    "tok_base = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_orig = len(tok_base)\n",
    "\n",
    "# ---------- 2) เตรียมชุด token ใหม่ ----------\n",
    "#   • EMOJI_TOKENS  : ทุกอิโมจิที่ “พบอย่างน้อย 1 ครั้ง” ใน SAMSum\n",
    "#   • SPEAKER_TOKENS: [S1] – [S10]\n",
    "SPEAKER_TOKENS = [f\"[S{i}]\" for i in range(1, 11)]\n",
    "new_tokens = EMOJI_TOKENS + SPEAKER_TOKENS\n",
    "\n",
    "# ---------- 3) สร้าง tokenizer สำเนาแล้วเพิ่ม token ----------\n",
    "tok_ext = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "added = tok_ext.add_tokens(new_tokens)\n",
    "vocab_new = len(tok_ext)\n",
    "\n",
    "# ---------- 4) แสดงผล ----------\n",
    "print(f\"Original vocab size : {vocab_orig}\")\n",
    "print(f\"Added new tokens     : {added}  \"\n",
    "      f\"(emoji = {len(EMOJI_TOKENS)}, speaker = {len(SPEAKER_TOKENS)})\")\n",
    "print(f\"New vocab size       : {vocab_new}\")\n",
    "\n",
    "# (Optional) พิมพ์ตัวอย่างอิโมจิ 20 ตัวแรก\n",
    "print(\"\\nFirst 20 emoji tokens:\", EMOJI_TOKENS[:20])\n",
    "\n",
    "tok_ext.save_pretrained(\"tokenizer_samsum_su\")   # โฟลเดอร์ใหม่"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc38ac",
   "metadata": {},
   "source": [
    "AFTER adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bc9fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising:   0%|          | 0/15 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 22.52it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 36.58it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 36.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts AFTER adding emoji tokens\n",
      "train     :      451  (0.022% of tokens)\n",
      "validation:        4  (0.004% of tokens)\n",
      "test      :       24  (0.021% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unk_stats_after = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_after[split] = count_unk(raw_ds[split], tok_ext)\n",
    "print(\"\\n[UNK] counts AFTER adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_after.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb039",
   "metadata": {},
   "source": [
    "reduction check in UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8480431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Δ [UNK] (before ➜ after):\n",
      "train     : +3307  fewer UNKs  (↓88.00%)\n",
      "validation: +187  fewer UNKs  (↓97.91%)\n",
      "test      : +171  fewer UNKs  (↓87.69%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nΔ [UNK] (before ➜ after):\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    u0, _ = unk_stats_before[split]\n",
    "    u1, _ = unk_stats_after[split]\n",
    "    print(f\"{split:<10}: {u0-u1:+d}  fewer UNKs  (↓{(u0-u1)/u0:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd47d8f",
   "metadata": {},
   "source": [
    "## Preprocess SAMSum Dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d4138",
   "metadata": {},
   "source": [
    "Speaker-name mapping → [S#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20077039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) Helper to replace speaker names by [S#]\n",
    "# ---------------------------------------------------------\n",
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10\n",
    "                 ) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns dialogue with names replaced by [S#] and a mapping dict.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:                # safety – keep line as is\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:      # truncate extra speakers\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        new_lines.append(f\"{speaker_map.get(name, '[SUNK]')}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad9bc",
   "metadata": {},
   "source": [
    "Insert [SEP] after every utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c65bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02e888",
   "metadata": {},
   "source": [
    "Switching-Utterance corruption\n",
    "- Hyper-parameters: Pu = 1.0, Pn = 0/1\n",
    "\n",
    "โดยที่\n",
    "\n",
    "Pu (permute-utterance prob.) ความน่าจะเป็นที่ แต่ละ utterance จะถูกเลือก ใส่ลงในชุดที่นำไปสับตำแหน่ง\n",
    "\n",
    "- pu = 1.0 แสดงว่าบังคับเลือกทุกบรรทัดแล้วค่อยสับคำแบบสุ่ม\n",
    "\n",
    "Pn (name-mask prob.) ความน่าจะเป็นที่ token [S#] ด้านหน้าจะถูกเปลี่ยนเป็น [MASK]\n",
    "\n",
    "- pn = 0.0 แสดงว่า ไม่ mask, โมเดลเห็น speaker tag\n",
    "\n",
    "- pn = 1.0 แสดงว่า mask หมด, บังคับดู context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ef01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_switching_utterance(dialogue: str,\n",
    "                             pu: float = 1.0,\n",
    "                             pn: float = 0.0,\n",
    "                             rng: random.Random = random\n",
    "                            ) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    • dialogue  - speaker-tokenised, SEP-inserted string\n",
    "    • pu        - prob. an utterance is selected for permutation\n",
    "    • pn        - prob. we MASK the speaker token (⇒ [MASK])\n",
    "    Returns:\n",
    "        corrupted_dialogue, labels_per_utt  (1 = permuted (สลับบทพูด), 0 = original)\n",
    "    \"\"\"\n",
    "    # 1) split back into utterances\n",
    "    utts = [u.strip() for u in dialogue.split(\"[SEP]\") if u.strip()]\n",
    "    idxs = list(range(len(utts)))\n",
    "\n",
    "    # 2) pick indices to permute\n",
    "    perm_idx = [i for i in idxs if rng.random() < pu]\n",
    "    shuffled = perm_idx.copy()\n",
    "    rng.shuffle(shuffled)                 # in-place\n",
    "    perm_map = dict(zip(perm_idx, shuffled))\n",
    "\n",
    "    # 3) build new utterance list, labels\n",
    "    new_utts, labels = [], []\n",
    "    for i in idxs:\n",
    "        src = perm_map.get(i, i)          # swapped or same\n",
    "        u = utts[src]\n",
    "        # optionally mask speaker token ([S#]: → [MASK]:)\n",
    "        if rng.random() < pn:\n",
    "            u = re.sub(r\"^\\[S\\d+\\]\", \"[MASK]\", u)\n",
    "        new_utts.append(u)\n",
    "        labels.append(int(src != i))      # 1 if permuted\n",
    "    corrupted = \" [SEP] \".join(new_utts) + \" [SEP]\"\n",
    "    return corrupted, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d05b",
   "metadata": {},
   "source": [
    "## Switching-Utterance (SU) pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5949ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building SU train: 100%|██████████| 14732/14732 [00:07<00:00, 1850.04 examples/s]\n",
      "Building SU validation: 100%|██████████| 818/818 [00:00<00:00, 1841.35 examples/s]\n",
      "Building SU test: 100%|██████████| 819/819 [00:00<00:00, 1815.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 581300.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 204161.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 201261.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7) Create HF Datasets with tokenised inputs, attention,\n",
    "#    SEP positions, and per-utterance labels\n",
    "# ---------------------------------------------------------\n",
    "MAX_LEN = 512                          # paper setting\n",
    "Pu, Pn = 1.0, 0.0                      # best config in Table 2\n",
    "\n",
    "def preprocess_example(example, split):\n",
    "    # a) replace speakers & add SEP\n",
    "    dlg, _ = map_speakers(example[\"dialogue\"])\n",
    "    dlg = add_sep_every_utt(dlg)\n",
    "\n",
    "    # b) corruption\n",
    "    corrupted, labels = make_switching_utterance(dlg, Pu, Pn)\n",
    "\n",
    "    # c) tokenize (truncate if >512 tokens)\n",
    "    enc = tok(corrupted,\n",
    "              truncation=True, max_length=MAX_LEN,\n",
    "              padding=\"max_length\")\n",
    "    \n",
    "    # d) find SEP token positions (needed for loss later)\n",
    "    sep_id = tok(\"[SEP]\")[\"input_ids\"][0]\n",
    "    sep_positions = [i for i, id_ in enumerate(enc[\"input_ids\"])\n",
    "                     if id_ == sep_id][:len(labels)]  # clip if truncated\n",
    "\n",
    "    enc[\"labels\"] = labels[:len(sep_positions)]\n",
    "    enc[\"sep_positions\"] = sep_positions\n",
    "    enc[\"dialogue_len\"] = len(labels)\n",
    "    return enc\n",
    "\n",
    "su_ds = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    su_ds[split] = raw_ds[split].map(\n",
    "        preprocess_example,\n",
    "        fn_kwargs={\"split\": split},\n",
    "        remove_columns=raw_ds[split].column_names,\n",
    "        desc=f\"Building SU {split}\"\n",
    "    )\n",
    "\n",
    "su_ds.save_to_disk(\"data/samsum_switching_utterance\")\n",
    "print(su_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4605bbc",
   "metadata": {},
   "source": [
    "ไฟล์ Arrow ถูกบันทึกไว้ที่ data/samsum_switching_utterance/ พร้อมฟิลด์ input_ids / attention_mask / labels / sep_positions / dialogue_len."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b54f7",
   "metadata": {},
   "source": [
    "## Self-supervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c9c0",
   "metadata": {},
   "source": [
    "ใช้ Dataset เฉพาะส่วนของ train ของ SAMSum มาทำการ pre_train แล้วใช้ validation ไว้ดู early-stopping / tuning ส่วน test ต้องไม่ถูกแตะ เพื่อไม่ให้โมเดล “เห็น” บทสนทนาที่จะใช้วัด ROUGE ภายหลัง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363e9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc246a",
   "metadata": {},
   "source": [
    "Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da87342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # paper 128\n",
    "MAX_LEN    = 512 # paper 512\n",
    "LR         = 3e-5\n",
    "WARMUP     = 500\n",
    "MAX_STEPS  = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc898da",
   "metadata": {},
   "source": [
    "Dataset & collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5875dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_switching_utterance\")\n",
    "\n",
    "# -------------------------------\n",
    "# COLLATE FUNCTION\n",
    "# -------------------------------\n",
    "def collate_fn(batch):\n",
    "    keys = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "    inputs = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
    "    labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "    sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "    return inputs, labels, sep_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ca8c2",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f94a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MODEL\n",
    "# -------------------------------\n",
    "class SepClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, sep_positions):\n",
    "        hidden_states = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # Collect hidden states at each [SEP] position\n",
    "        # sep_vecs = [hidden_states[i, pos] for i, pos in enumerate(sep_positions)]\n",
    "        sep_vecs = []\n",
    "        for i, pos_tensor in enumerate(sep_positions):\n",
    "            pos_tensor = pos_tensor.to(hidden_states.device).long()  # <-- เพิ่มการ cast\n",
    "            sep_vecs.append(hidden_states[i].index_select(0, pos_tensor))  # (U_i, H)\n",
    "\n",
    "        sep_vecs = torch.cat(sep_vecs, dim=0)  # Shape: (total_seps, hidden_size)\n",
    "        logits = self.classifier(self.dropout(sep_vecs)).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307a9de",
   "metadata": {},
   "source": [
    "Training loop (train model until the train loss converged (upper bounded by 5k steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26145b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  100/920 | Loss: 0.4475\n",
      "Step  200/920 | Loss: 0.3658\n",
      "Step  300/920 | Loss: 0.3322\n",
      "Step  400/920 | Loss: 0.3086\n",
      "Step  500/920 | Loss: 0.2784\n",
      "Step  600/920 | Loss: 0.2073\n",
      "Step  700/920 | Loss: 0.1651\n",
      "Step  800/920 | Loss: 0.1274\n",
      "Step  900/920 | Loss: 0.1344\n",
      "Step 1000/920 | Loss: 0.1066\n",
      "Step 1100/920 | Loss: 0.0985\n",
      "Step 1200/920 | Loss: 0.0876\n",
      "Step 1300/920 | Loss: 0.1076\n",
      "Step 1400/920 | Loss: 0.1057\n",
      "Step 1500/920 | Loss: 0.1119\n",
      "Step 1600/920 | Loss: 0.1069\n",
      "Step 1700/920 | Loss: 0.1009\n",
      "Step 1800/920 | Loss: 0.0942\n",
      "Step 1900/920 | Loss: 0.0999\n",
      "Step 2000/920 | Loss: 0.1005\n",
      "Step 2100/920 | Loss: 0.1036\n",
      "Step 2200/920 | Loss: 0.0995\n",
      "Step 2300/920 | Loss: 0.0981\n",
      "Step 2400/920 | Loss: 0.1093\n",
      "Step 2500/920 | Loss: 0.1070\n",
      "Step 2600/920 | Loss: 0.0990\n",
      "Step 2700/920 | Loss: 0.0996\n",
      "Step 2800/920 | Loss: 0.1021\n",
      "Step 2900/920 | Loss: 0.0949\n",
      "Step 3000/920 | Loss: 0.1073\n",
      "Step 3100/920 | Loss: 0.1104\n",
      "Step 3200/920 | Loss: 0.1053\n",
      "Step 3300/920 | Loss: 0.0948\n",
      "Step 3400/920 | Loss: 0.1014\n",
      "Step 3500/920 | Loss: 0.1133\n",
      "Step 3600/920 | Loss: 0.1084\n",
      "Step 3700/920 | Loss: 0.0929\n",
      "Step 3800/920 | Loss: 0.1086\n",
      "Step 3900/920 | Loss: 0.1086\n",
      "Step 4000/920 | Loss: 0.0930\n",
      "Step 4100/920 | Loss: 0.1013\n",
      "Step 4200/920 | Loss: 0.0995\n",
      "Step 4300/920 | Loss: 0.1066\n",
      "Step 4400/920 | Loss: 0.1066\n",
      "Step 4500/920 | Loss: 0.1122\n",
      "Step 4600/920 | Loss: 0.0941\n",
      "Step 4700/920 | Loss: 0.1048\n",
      "Step 4800/920 | Loss: 0.1059\n",
      "Step 4900/920 | Loss: 0.1155\n",
      "Step 5000/920 | Loss: 0.1063\n",
      "Step 5100/920 | Loss: 0.1016\n",
      "Step 5200/920 | Loss: 0.0863\n",
      "Step 5300/920 | Loss: 0.1008\n",
      "Step 5400/920 | Loss: 0.1083\n",
      "Step 5500/920 | Loss: 0.1080\n",
      "Step 5600/920 | Loss: 0.1028\n",
      "Step 5700/920 | Loss: 0.1044\n",
      "Step 5800/920 | Loss: 0.1078\n",
      "Step 5900/920 | Loss: 0.0967\n",
      "Step 6000/920 | Loss: 0.0868\n",
      "Step 6100/920 | Loss: 0.0992\n",
      "Step 6200/920 | Loss: 0.1155\n",
      "Step 6300/920 | Loss: 0.1060\n",
      "Step 6400/920 | Loss: 0.1081\n",
      "Step 6500/920 | Loss: 0.1045\n",
      "Step 6600/920 | Loss: 0.1015\n",
      "Step 6700/920 | Loss: 0.1144\n",
      "Step 6800/920 | Loss: 0.1047\n",
      "Step 6900/920 | Loss: 0.1015\n",
      "Step 7000/920 | Loss: 0.0991\n",
      "Step 7100/920 | Loss: 0.0979\n",
      "Step 7200/920 | Loss: 0.1127\n",
      "Step 7300/920 | Loss: 0.0883\n",
      "Step 7400/920 | Loss: 0.0812\n",
      "Step 7500/920 | Loss: 0.1104\n",
      "Step 7600/920 | Loss: 0.1130\n",
      "Step 7700/920 | Loss: 0.1047\n",
      "Step 7800/920 | Loss: 0.0962\n",
      "Step 7900/920 | Loss: 0.1089\n",
      "Step 8000/920 | Loss: 0.0929\n",
      "Step 8100/920 | Loss: 0.1110\n",
      "Step 8200/920 | Loss: 0.0954\n",
      "Step 8300/920 | Loss: 0.0997\n",
      "Step 8400/920 | Loss: 0.0914\n",
      "Step 8500/920 | Loss: 0.0991\n",
      "Step 8600/920 | Loss: 0.1015\n",
      "Step 8700/920 | Loss: 0.1123\n",
      "Step 8800/920 | Loss: 0.1023\n",
      "Step 8900/920 | Loss: 0.1056\n",
      "Step 9000/920 | Loss: 0.0976\n",
      "Step 9100/920 | Loss: 0.1129\n",
      "Step 9200/920 | Loss: 0.1073\n",
      "Step 9300/920 | Loss: 0.1079\n",
      "Step 9400/920 | Loss: 0.1049\n",
      "Step 9500/920 | Loss: 0.1043\n",
      "Step 9600/920 | Loss: 0.1049\n",
      "Step 9700/920 | Loss: 0.0926\n",
      "Step 9800/920 | Loss: 0.1037\n",
      "Step 9900/920 | Loss: 0.1096\n",
      "Step 10000/920 | Loss: 0.1022\n",
      "Step 10100/920 | Loss: 0.0991\n",
      "Step 10200/920 | Loss: 0.1010\n",
      "Step 10300/920 | Loss: 0.1092\n",
      "Step 10400/920 | Loss: 0.1092\n",
      "Step 10500/920 | Loss: 0.1085\n",
      "Step 10600/920 | Loss: 0.0924\n",
      "Step 10700/920 | Loss: 0.1019\n",
      "Step 10800/920 | Loss: 0.0983\n",
      "Step 10900/920 | Loss: 0.0984\n",
      "Step 11000/920 | Loss: 0.1129\n",
      "Step 11100/920 | Loss: 0.0945\n",
      "Step 11200/920 | Loss: 0.1087\n",
      "Step 11300/920 | Loss: 0.1042\n",
      "Step 11400/920 | Loss: 0.1081\n",
      "Step 11500/920 | Loss: 0.1036\n",
      "Step 11600/920 | Loss: 0.0966\n",
      "Step 11700/920 | Loss: 0.1031\n",
      "Step 11800/920 | Loss: 0.1129\n",
      "Step 11900/920 | Loss: 0.1026\n",
      "Step 12000/920 | Loss: 0.0941\n",
      "Step 12100/920 | Loss: 0.0851\n",
      "Step 12200/920 | Loss: 0.1050\n",
      "Step 12300/920 | Loss: 0.1152\n",
      "Step 12400/920 | Loss: 0.1084\n",
      "Step 12500/920 | Loss: 0.1165\n",
      "Step 12600/920 | Loss: 0.0980\n",
      "Step 12700/920 | Loss: 0.1057\n",
      "Step 12800/920 | Loss: 0.0941\n",
      "Step 12900/920 | Loss: 0.1024\n",
      "Step 13000/920 | Loss: 0.0917\n",
      "Step 13100/920 | Loss: 0.1015\n",
      "Step 13200/920 | Loss: 0.1004\n",
      "Step 13300/920 | Loss: 0.1100\n",
      "Step 13400/920 | Loss: 0.1073\n",
      "Step 13500/920 | Loss: 0.0993\n",
      "Step 13600/920 | Loss: 0.1092\n",
      "Step 13700/920 | Loss: 0.1107\n",
      "Step 13800/920 | Loss: 0.0995\n",
      "Step 13900/920 | Loss: 0.1039\n",
      "Step 14000/920 | Loss: 0.1012\n",
      "Step 14100/920 | Loss: 0.1041\n",
      "Step 14200/920 | Loss: 0.1031\n",
      "Step 14300/920 | Loss: 0.1108\n",
      "Step 14400/920 | Loss: 0.0940\n",
      "Step 14500/920 | Loss: 0.1017\n",
      "Step 14600/920 | Loss: 0.1055\n",
      "Step 14700/920 | Loss: 0.1000\n",
      "Step 14800/920 | Loss: 0.1097\n",
      "Step 14900/920 | Loss: 0.0932\n",
      "Step 15000/920 | Loss: 0.1084\n",
      "Step 15100/920 | Loss: 0.1076\n",
      "Step 15200/920 | Loss: 0.0909\n",
      "Step 15300/920 | Loss: 0.1074\n",
      "Step 15400/920 | Loss: 0.1046\n",
      "Step 15500/920 | Loss: 0.1148\n",
      "Step 15600/920 | Loss: 0.0970\n",
      "Step 15700/920 | Loss: 0.1073\n",
      "Step 15800/920 | Loss: 0.0957\n",
      "Step 15900/920 | Loss: 0.1060\n",
      "Step 16000/920 | Loss: 0.1083\n",
      "Step 16100/920 | Loss: 0.1176\n",
      "Step 16200/920 | Loss: 0.1178\n",
      "Step 16300/920 | Loss: 0.0941\n",
      "Step 16400/920 | Loss: 0.1000\n",
      "Step 16500/920 | Loss: 0.1037\n",
      "Step 16600/920 | Loss: 0.1041\n",
      "Step 16700/920 | Loss: 0.0996\n",
      "Step 16800/920 | Loss: 0.0960\n",
      "Step 16900/920 | Loss: 0.1053\n",
      "Step 17000/920 | Loss: 0.0956\n",
      "Step 17100/920 | Loss: 0.1000\n",
      "Step 17200/920 | Loss: 0.1055\n",
      "Step 17300/920 | Loss: 0.1012\n",
      "Step 17400/920 | Loss: 0.1029\n",
      "Step 17500/920 | Loss: 0.1088\n",
      "Step 17600/920 | Loss: 0.1072\n",
      "Step 17700/920 | Loss: 0.1049\n",
      "Step 17800/920 | Loss: 0.0911\n",
      "Step 17900/920 | Loss: 0.0981\n",
      "Step 18000/920 | Loss: 0.1035\n",
      "Step 18100/920 | Loss: 0.0953\n",
      "Step 18200/920 | Loss: 0.1148\n",
      "Step 18300/920 | Loss: 0.1241\n",
      "Step 18400/920 | Loss: 0.0988\n",
      "Step 18500/920 | Loss: 0.0920\n",
      "Step 18600/920 | Loss: 0.0962\n",
      "Step 18700/920 | Loss: 0.1006\n",
      "Step 18800/920 | Loss: 0.1070\n",
      "Step 18900/920 | Loss: 0.0972\n",
      "Step 19000/920 | Loss: 0.1212\n",
      "Step 19100/920 | Loss: 0.1106\n",
      "Step 19200/920 | Loss: 0.1043\n",
      "Step 19300/920 | Loss: 0.1046\n",
      "Step 19400/920 | Loss: 0.1044\n",
      "Step 19500/920 | Loss: 0.0952\n",
      "Step 19600/920 | Loss: 0.1043\n",
      "Step 19700/920 | Loss: 0.1123\n",
      "Step 19800/920 | Loss: 0.1122\n",
      "Step 19900/920 | Loss: 0.0973\n",
      "Step 20000/920 | Loss: 0.0925\n",
      "Step 20100/920 | Loss: 0.0956\n",
      "Step 20200/920 | Loss: 0.1020\n",
      "Step 20300/920 | Loss: 0.1091\n",
      "Step 20400/920 | Loss: 0.1059\n",
      "Step 20500/920 | Loss: 0.1115\n",
      "Step 20600/920 | Loss: 0.1161\n",
      "Step 20700/920 | Loss: 0.1000\n",
      "Step 20800/920 | Loss: 0.1104\n",
      "Step 20900/920 | Loss: 0.1039\n",
      "Step 21000/920 | Loss: 0.0937\n",
      "Step 21100/920 | Loss: 0.0864\n",
      "Step 21200/920 | Loss: 0.0916\n",
      "Step 21300/920 | Loss: 0.1064\n",
      "Step 21400/920 | Loss: 0.1026\n",
      "Step 21500/920 | Loss: 0.1032\n",
      "Step 21600/920 | Loss: 0.1052\n",
      "Step 21700/920 | Loss: 0.1023\n",
      "Step 21800/920 | Loss: 0.0970\n",
      "Step 21900/920 | Loss: 0.1107\n",
      "Step 22000/920 | Loss: 0.1133\n",
      "Step 22100/920 | Loss: 0.0987\n",
      "Step 22200/920 | Loss: 0.1061\n",
      "Step 22300/920 | Loss: 0.0950\n",
      "Step 22400/920 | Loss: 0.1059\n",
      "Step 22500/920 | Loss: 0.1188\n",
      "Step 22600/920 | Loss: 0.1101\n",
      "Step 22700/920 | Loss: 0.0943\n",
      "Step 22800/920 | Loss: 0.1070\n",
      "Step 22900/920 | Loss: 0.0927\n",
      "Step 23000/920 | Loss: 0.0918\n",
      "Step 23100/920 | Loss: 0.0956\n",
      "Step 23200/920 | Loss: 0.1027\n",
      "Step 23300/920 | Loss: 0.1079\n",
      "Step 23400/920 | Loss: 0.0962\n",
      "Step 23500/920 | Loss: 0.1078\n",
      "Step 23600/920 | Loss: 0.0959\n",
      "Step 23700/920 | Loss: 0.1080\n",
      "Step 23800/920 | Loss: 0.1077\n",
      "Step 23900/920 | Loss: 0.1027\n",
      "Step 24000/920 | Loss: 0.0997\n",
      "Step 24100/920 | Loss: 0.0973\n",
      "Step 24200/920 | Loss: 0.1064\n",
      "Step 24300/920 | Loss: 0.1000\n",
      "Step 24400/920 | Loss: 0.1013\n",
      "Step 24500/920 | Loss: 0.1135\n",
      "Step 24600/920 | Loss: 0.0859\n",
      "Step 24700/920 | Loss: 0.1080\n",
      "Step 24800/920 | Loss: 0.1059\n",
      "Step 24900/920 | Loss: 0.0960\n",
      "Step 25000/920 | Loss: 0.1094\n",
      "Step 25100/920 | Loss: 0.1125\n",
      "Step 25200/920 | Loss: 0.0920\n",
      "Step 25300/920 | Loss: 0.1157\n",
      "Step 25400/920 | Loss: 0.1047\n",
      "Step 25500/920 | Loss: 0.0995\n",
      "Step 25600/920 | Loss: 0.0937\n",
      "Step 25700/920 | Loss: 0.1045\n",
      "Step 25800/920 | Loss: 0.1030\n",
      "Step 25900/920 | Loss: 0.1014\n",
      "Step 26000/920 | Loss: 0.1102\n",
      "Step 26100/920 | Loss: 0.0907\n",
      "Step 26200/920 | Loss: 0.1267\n",
      "Step 26300/920 | Loss: 0.0960\n",
      "Step 26400/920 | Loss: 0.1083\n",
      "Step 26500/920 | Loss: 0.1009\n",
      "Step 26600/920 | Loss: 0.0941\n",
      "Step 26700/920 | Loss: 0.0979\n",
      "Step 26800/920 | Loss: 0.1161\n",
      "Step 26900/920 | Loss: 0.1100\n",
      "Step 27000/920 | Loss: 0.1021\n",
      "Step 27100/920 | Loss: 0.0974\n",
      "Step 27200/920 | Loss: 0.0994\n",
      "Step 27300/920 | Loss: 0.1046\n",
      "Step 27400/920 | Loss: 0.1041\n",
      "Step 27500/920 | Loss: 0.1054\n",
      "Step 27600/920 | Loss: 0.0942\n",
      "Step 27700/920 | Loss: 0.1036\n",
      "Step 27800/920 | Loss: 0.0916\n",
      "Step 27900/920 | Loss: 0.1101\n",
      "Step 28000/920 | Loss: 0.1044\n",
      "Step 28100/920 | Loss: 0.1131\n",
      "Step 28200/920 | Loss: 0.0926\n",
      "Step 28300/920 | Loss: 0.1002\n",
      "Step 28400/920 | Loss: 0.1051\n",
      "Step 28500/920 | Loss: 0.1076\n",
      "Step 28600/920 | Loss: 0.1138\n",
      "Step 28700/920 | Loss: 0.1117\n",
      "Step 28800/920 | Loss: 0.0897\n",
      "Step 28900/920 | Loss: 0.1029\n",
      "Step 29000/920 | Loss: 0.0944\n",
      "Step 29100/920 | Loss: 0.1158\n",
      "Step 29200/920 | Loss: 0.0973\n",
      "Step 29300/920 | Loss: 0.1090\n",
      "Step 29400/920 | Loss: 0.0931\n",
      "Step 29500/920 | Loss: 0.0984\n",
      "Step 29600/920 | Loss: 0.1077\n",
      "Step 29700/920 | Loss: 0.1084\n",
      "Step 29800/920 | Loss: 0.0952\n",
      "Step 29900/920 | Loss: 0.1112\n",
      "Step 30000/920 | Loss: 0.1014\n",
      "Step 30100/920 | Loss: 0.1043\n",
      "Step 30200/920 | Loss: 0.1154\n",
      "Step 30300/920 | Loss: 0.1022\n",
      "Step 30400/920 | Loss: 0.0983\n",
      "Step 30500/920 | Loss: 0.0862\n",
      "Step 30600/920 | Loss: 0.1048\n",
      "Step 30700/920 | Loss: 0.0963\n",
      "Step 30800/920 | Loss: 0.1030\n",
      "Step 30900/920 | Loss: 0.0989\n",
      "Step 31000/920 | Loss: 0.1092\n",
      "Step 31100/920 | Loss: 0.1051\n",
      "Step 31200/920 | Loss: 0.1013\n",
      "Step 31300/920 | Loss: 0.1068\n",
      "Step 31400/920 | Loss: 0.1052\n",
      "Step 31500/920 | Loss: 0.1142\n",
      "Step 31600/920 | Loss: 0.1208\n",
      "Step 31700/920 | Loss: 0.0960\n",
      "Step 31800/920 | Loss: 0.1031\n",
      "Step 31900/920 | Loss: 0.0928\n",
      "Step 32000/920 | Loss: 0.1041\n",
      "Step 32100/920 | Loss: 0.0898\n",
      "Step 32200/920 | Loss: 0.1011\n",
      "Step 32300/920 | Loss: 0.0938\n",
      "Step 32400/920 | Loss: 0.1200\n",
      "Step 32500/920 | Loss: 0.1016\n",
      "Step 32600/920 | Loss: 0.1128\n",
      "Step 32700/920 | Loss: 0.0933\n",
      "Step 32800/920 | Loss: 0.1055\n",
      "Step 32900/920 | Loss: 0.1036\n",
      "Step 33000/920 | Loss: 0.1039\n",
      "Step 33100/920 | Loss: 0.1029\n",
      "Step 33200/920 | Loss: 0.1008\n",
      "Step 33300/920 | Loss: 0.1060\n",
      "Step 33400/920 | Loss: 0.0926\n",
      "Step 33500/920 | Loss: 0.0956\n",
      "Step 33600/920 | Loss: 0.1008\n",
      "Step 33700/920 | Loss: 0.1085\n",
      "Step 33800/920 | Loss: 0.0930\n",
      "Step 33900/920 | Loss: 0.0960\n",
      "Step 34000/920 | Loss: 0.1162\n",
      "Step 34100/920 | Loss: 0.1109\n",
      "Step 34200/920 | Loss: 0.1084\n",
      "Step 34300/920 | Loss: 0.1056\n",
      "Step 34400/920 | Loss: 0.1013\n",
      "Step 34500/920 | Loss: 0.1011\n",
      "Step 34600/920 | Loss: 0.1013\n",
      "Step 34700/920 | Loss: 0.0972\n",
      "Step 34800/920 | Loss: 0.1083\n",
      "Step 34900/920 | Loss: 0.1117\n",
      "Step 35000/920 | Loss: 0.1103\n",
      "Step 35100/920 | Loss: 0.0961\n",
      "Step 35200/920 | Loss: 0.0887\n",
      "Step 35300/920 | Loss: 0.1077\n",
      "Step 35400/920 | Loss: 0.1201\n",
      "Step 35500/920 | Loss: 0.0979\n",
      "Step 35600/920 | Loss: 0.1016\n",
      "Step 35700/920 | Loss: 0.0983\n",
      "Step 35800/920 | Loss: 0.1093\n",
      "Step 35900/920 | Loss: 0.0995\n",
      "Step 36000/920 | Loss: 0.0946\n",
      "Step 36100/920 | Loss: 0.1107\n",
      "Step 36200/920 | Loss: 0.1086\n",
      "Step 36300/920 | Loss: 0.1085\n",
      "Step 36400/920 | Loss: 0.1007\n",
      "Step 36500/920 | Loss: 0.0974\n",
      "Step 36600/920 | Loss: 0.1111\n",
      "Step 36700/920 | Loss: 0.0968\n",
      "Step 36800/920 | Loss: 0.0916\n",
      "Step 36900/920 | Loss: 0.0970\n",
      "Step 37000/920 | Loss: 0.1032\n",
      "Step 37100/920 | Loss: 0.0991\n",
      "Step 37200/920 | Loss: 0.1121\n",
      "Step 37300/920 | Loss: 0.0963\n",
      "Step 37400/920 | Loss: 0.1089\n",
      "Step 37500/920 | Loss: 0.1030\n",
      "Step 37600/920 | Loss: 0.0993\n",
      "Step 37700/920 | Loss: 0.1159\n",
      "Step 37800/920 | Loss: 0.0970\n",
      "Step 37900/920 | Loss: 0.1084\n",
      "Step 38000/920 | Loss: 0.0937\n",
      "Step 38100/920 | Loss: 0.1201\n",
      "Step 38200/920 | Loss: 0.1061\n",
      "Step 38300/920 | Loss: 0.1062\n",
      "Step 38400/920 | Loss: 0.0984\n",
      "Step 38500/920 | Loss: 0.0969\n",
      "Step 38600/920 | Loss: 0.1007\n",
      "Step 38700/920 | Loss: 0.1216\n",
      "Step 38800/920 | Loss: 0.1040\n",
      "Step 38900/920 | Loss: 0.1069\n",
      "Step 39000/920 | Loss: 0.0980\n",
      "Step 39100/920 | Loss: 0.1015\n",
      "Step 39200/920 | Loss: 0.1029\n",
      "Step 39300/920 | Loss: 0.1054\n",
      "Step 39400/920 | Loss: 0.0977\n",
      "Step 39500/920 | Loss: 0.1067\n",
      "Step 39600/920 | Loss: 0.0945\n",
      "Step 39700/920 | Loss: 0.1021\n",
      "Step 39800/920 | Loss: 0.1141\n",
      "Step 39900/920 | Loss: 0.0956\n",
      "Step 40000/920 | Loss: 0.1064\n",
      "Model saved to 'bert_su_pretrained.pt'\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# INITIALIZATION\n",
    "# -------------------------------\n",
    "model = SepClassifier().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "model.bert.resize_token_embeddings(len(tokenizer)) # Adjust embedding size for extended tokens\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = min(MAX_STEPS, len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):  # loop until MAX_STEPS reached\n",
    "    for inputs, label_lists, sep_lists in train_loader:\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        flat_labels = torch.cat(label_lists).to(DEVICE)\n",
    "\n",
    "        logits = model(**inputs, sep_positions=sep_lists)\n",
    "        loss = loss_fn(logits, flat_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step:4d}/{total_steps} | Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    if step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE MODEL\n",
    "# -------------------------------\n",
    "torch.save(model.state_dict(), \"bert_su_pretrained.pt\")\n",
    "print(\"Model saved to 'bert_su_pretrained.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "521faa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.bert.state_dict(), \"bert_su_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459a3fb",
   "metadata": {},
   "source": [
    "Validation & early-stop (optional)\n",
    "\n",
    "- Use the same DataLoader/loop on su_ds[\"validation\"], compute average BCE loss; if it plateaus you can stop earlier than 5 k steps (what the authors mean by “until train loss converged”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b890a7",
   "metadata": {},
   "source": [
    "# Create Summarization Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3b258",
   "metadata": {},
   "source": [
    "ขั้นตอนการทำ preprocess\n",
    "1. โหลดชุดข้อมูล SAMSum\n",
    "2. ทำ preprocessing:\n",
    "    - แทนชื่อ speaker ด้วย [S1]–[S10]\n",
    "    - เติม [SEP] ท้ายทุกประโยค\n",
    "    - ใช้ tokenizer เดิมจาก pretraining (tokenizer_samsum_su)\n",
    "    - truncate/pad ความยาวที่ max_length = 512\n",
    "3. แปลงให้อยู่ในรูปแบบที่พร้อมใช้สำหรับ Seq2SeqTrainer\n",
    "4. Save เป็นไฟล์ .pt หรือ DatasetDict ที่พร้อมใช้งาน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ae8f1",
   "metadata": {},
   "source": [
    "Load SAMSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d58290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/drl-68/.cache/huggingface/modules/datasets_modules/datasets/samsum/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e (last modified on Mon May  5 10:16:55 2025) since it couldn't be found locally at samsum, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e39b4",
   "metadata": {},
   "source": [
    "Load Pretrained Tokenizer (same as used during pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceaf5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7f9b0",
   "metadata": {},
   "source": [
    "Speaker Normalization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aa9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Replace speaker names with generic [S1], [S2], ... tokens.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        name_token = speaker_map.get(name, \"[SUNK]\")\n",
    "        new_lines.append(f\"{name_token}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n",
    "\n",
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c45323",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(example):\n",
    "    normed_dialogue, _ = map_speakers(example[\"dialogue\"])\n",
    "    sep_dialogue = add_sep_every_utt(normed_dialogue)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        sep_dialogue,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": targets[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb81812",
   "metadata": {},
   "source": [
    "Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ebaf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:08<00:00, 1649.04 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 1614.32 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 1669.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 323900.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 177123.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 159697.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to 'samsum_finetune_ready'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = raw_ds.map(preprocess_fn, batched=False)\n",
    "tokenized_ds.save_to_disk(\"samsum_finetune_ready\")\n",
    "print(\"Preprocessed dataset saved to 'samsum_finetune_ready'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Fine-tuning train: 100%|██████████| 14732/14732 [00:09<00:00, 1561.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 183818.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 129347.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 126710.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MAX_LEN = 512  # paper setting\n",
    "\n",
    "# def preprocess_example(example, split):\n",
    "#     # a) replace speakers & add SEP (same as pretraining)\n",
    "#     dlg, _ = map_speakers(example[\"dialogue\"])  # แปลงชื่อให้เป็น token สั้น ๆ เช่น <USR1>\n",
    "#     dlg = add_sep_every_utt(dlg)                # เพิ่ม [SEP] ทุกท้ายประโยค\n",
    "\n",
    "#     # b) tokenize dialogue input\n",
    "#     enc = tok_base(dlg,\n",
    "#               truncation=True,\n",
    "#               max_length=MAX_LEN,\n",
    "#               padding=\"max_length\")\n",
    "\n",
    "#     # c) tok_baseenize target summary\n",
    "#     with tok_base.as_target_tokenizer():\n",
    "#         summary = example[\"summary\"]\n",
    "#         summary_enc = tok_base(summary,\n",
    "#                           truncation=True,\n",
    "#                           max_length=MAX_LEN,\n",
    "#                           padding=\"max_length\")\n",
    "    \n",
    "#     # d) pack input and label\n",
    "#     enc[\"labels\"] = summary_enc[\"input_ids\"]\n",
    "#     return enc\n",
    "\n",
    "# # สร้าง dataset ใหม่สำหรับ fine-tune\n",
    "# finetune_ds = DatasetDict()\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     finetune_ds[split] = raw_ds[split].map(\n",
    "#         preprocess_example,\n",
    "#         fn_kwargs={\"split\": split},\n",
    "#         remove_columns=raw_ds[split].column_names,\n",
    "#         desc=f\"Building Fine-tuning {split}\"\n",
    "#     )\n",
    "\n",
    "# finetune_ds.save_to_disk(\"data/samsum_finetune\")\n",
    "# print(finetune_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e9327",
   "metadata": {},
   "source": [
    "# Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a3993",
   "metadata": {},
   "source": [
    "### BERT + SU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a197c0",
   "metadata": {},
   "source": [
    "\n",
    "**เทียบกับ Paper**\n",
    "\n",
    "| **Parameter**       | **Code**                          | **Paper (Section 3.2)**          | **เหมือน / ไม่เหมือน**        |\n",
    "| ------------------- | --------------------------------- | -------------------------------- | --------------       |\n",
    "| Model               | BERT2BERT (EncoderDecoderModel)   | BERT2BERT                        | เหมือน                 |\n",
    "| Tokenizer           | bert-base-uncased + custom tokens | ใช้ tokenizer ดัดแปลง              | เหมือน              |\n",
    "| Batch Size          | 8                                 | **16 (per step)**                | ไม่เหมือน → เล็กกว่า  |\n",
    "| Epochs              | 3                                 | 3                                | เหมือน              |\n",
    "| Learning Rate       | 5e-5                              | **3e-5**                         | ไม่เหมือน → สูงกว่า   |\n",
    "| Warmup Steps        | 500                               | ใช้ scheduler (แต่ไม่ระบุ exact)     | เหมือน (สมเหตุสมผล) |\n",
    "| Max Length (input)  | 512                               | 512                              | เหมือน              |\n",
    "| Max Length (output) | 128                               | 128                              | เหมือน              |\n",
    "| Beam Search         | 4                                 | 4                                | เหมือน              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3beebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/5526 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "  9%|▉         | 500/5526 [01:36<16:14,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4418, 'grad_norm': 0.3646565079689026, 'learning_rate': 4.96e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  9%|▉         | 501/5526 [01:41<2:22:17,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22444657981395721, 'eval_runtime': 5.0061, 'eval_samples_per_second': 163.402, 'eval_steps_per_second': 20.575, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1000/5526 [03:17<14:36,  5.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2172, 'grad_norm': 0.4135509133338928, 'learning_rate': 4.506565857540788e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 18%|█▊        | 1000/5526 [03:22<14:36,  5.17it/s]/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19904808700084686, 'eval_runtime': 5.0413, 'eval_samples_per_second': 162.259, 'eval_steps_per_second': 20.431, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 27%|██▋       | 1500/5526 [05:00<12:53,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2008, 'grad_norm': 0.274005264043808, 'learning_rate': 4.009152407481098e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 27%|██▋       | 1501/5526 [05:06<1:53:34,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1895398199558258, 'eval_runtime': 4.9978, 'eval_samples_per_second': 163.672, 'eval_steps_per_second': 20.609, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2000/5526 [06:42<11:18,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1864, 'grad_norm': 0.31300443410873413, 'learning_rate': 3.511738957421409e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 2000/5526 [06:47<11:18,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18359985947608948, 'eval_runtime': 4.9794, 'eval_samples_per_second': 164.278, 'eval_steps_per_second': 20.685, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 45%|████▌     | 2500/5526 [08:25<09:41,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17, 'grad_norm': 0.36899664998054504, 'learning_rate': 3.0143255073617192e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▌     | 2501/5526 [08:30<1:25:32,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17844413220882416, 'eval_runtime': 5.0089, 'eval_samples_per_second': 163.311, 'eval_steps_per_second': 20.564, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3000/5526 [10:06<08:05,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1689, 'grad_norm': 0.43778106570243835, 'learning_rate': 2.5169120573020293e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 54%|█████▍    | 3000/5526 [10:11<08:05,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17464406788349152, 'eval_runtime': 4.984, 'eval_samples_per_second': 164.126, 'eval_steps_per_second': 20.666, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 63%|██████▎   | 3500/5526 [11:50<06:25,  5.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1632, 'grad_norm': 0.3436298966407776, 'learning_rate': 2.01949860724234e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|██████▎   | 3501/5526 [11:55<56:57,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17154935002326965, 'eval_runtime': 4.9824, 'eval_samples_per_second': 164.179, 'eval_steps_per_second': 20.673, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 4000/5526 [13:31<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1487, 'grad_norm': 0.4152975380420685, 'learning_rate': 1.5220851571826503e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 4000/5526 [13:36<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17133557796478271, 'eval_runtime': 4.98, 'eval_samples_per_second': 164.259, 'eval_steps_per_second': 20.683, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 81%|████████▏ | 4500/5526 [15:14<03:17,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1356, 'grad_norm': 0.35845550894737244, 'learning_rate': 1.0246717071229607e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 81%|████████▏ | 4501/5526 [15:19<28:52,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1692614108324051, 'eval_runtime': 4.9857, 'eval_samples_per_second': 164.068, 'eval_steps_per_second': 20.659, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 5000/5526 [16:55<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1402, 'grad_norm': 0.3714558780193329, 'learning_rate': 5.27258257063271e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 5000/5526 [17:00<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1681322604417801, 'eval_runtime': 4.9793, 'eval_samples_per_second': 164.281, 'eval_steps_per_second': 20.686, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "100%|█████████▉| 5500/5526 [18:38<00:04,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1387, 'grad_norm': 0.4560355544090271, 'learning_rate': 2.984480700358138e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|█████████▉| 5501/5526 [18:43<00:42,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1668214201927185, 'eval_runtime': 4.9802, 'eval_samples_per_second': 164.251, 'eval_steps_per_second': 20.682, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5526/5526 [18:50<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1130.5819, 'train_samples_per_second': 39.091, 'train_steps_per_second': 4.888, 'train_loss': 0.28212739849194124, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su_finetune/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su_finetune/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su_finetune/vocab.txt',\n",
       " 'tokenizer_samsum_su_finetune/added_tokens.json',\n",
       " 'tokenizer_samsum_su_finetune/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# ------------------------------\n",
    "# Load processed dataset & tokenizer\n",
    "# ------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load pretrained EncoderDecoderModel\n",
    "# ------------------------------\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\", \"bert-base-uncased\"\n",
    ")\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load your pretrained encoder weights\n",
    "model.encoder.load_state_dict(torch.load(\"bert_su_pretrained.pt\", map_location=\"cpu\"))\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# ------------------------------\n",
    "# Define training arguments\n",
    "# ------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Data Collator & Trainer\n",
    "# ------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Start Training\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "model.save_pretrained(\"bert_samsum_finetuned\")\n",
    "tokenizer.save_pretrained(\"tokenizer_samsum_su_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b105d9",
   "metadata": {},
   "source": [
    "### Evaluation BERT+SU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed52141",
   "metadata": {},
   "source": [
    "1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) ใช้วัดความคล้ายกันระหว่างสรุปที่โมเดลสร้างขึ้นกับสรุปอ้างอิง โดยเน้นไปที่ recall เป็นหลัก\n",
    "\t- ROUGE-1 (R-1) = Unigram overlap (คำเดี่ยว)\n",
    "\t- ROUGE-2 (R-2) = Bigram overlap (คำติดกัน 2 คำ)\n",
    "\t- ROUGE-L (R-L) = ใช้ Longest common subsequence (LCS) ในการวัดความคล้ายเชิงลำดับคำที่ยาวที่สุดที่ปรากฏในทั้งสองสรุป โดยคำนึงถึงลำดับคำด้วย\n",
    "\n",
    "2. BLEU (Bilingual Evaluation Understudy) เดิมทีใช้ในงานแปลภาษา แต่ถูกประยุกต์ใช้ในงานสรุปข้อความได้เช่นกัน โดย BLEU จะเน้นการวัด precision คือดูว่า คำที่โมเดลสร้าง มีเท่าไรที่ตรงกับสรุปจริง ต่างจาก ROUGE ที่เน้น recall\n",
    "\t- BLEU วัดการทับซ้อนของ n-gram เช่น unigram, bigram, trigram\n",
    "\t- มีการใช้ brevity penalty หากสรุปสั้นกว่าที่ควรจะเป็น\n",
    "\n",
    "3. BERTScore (BS) ใช้ embedding จากโมเดล BERT หรือ Transformer ตัวอื่น ๆ ในการวัด semantic similarity (ความใกล้เคียงด้านความหมาย) ระหว่างสรุปของโมเดลกับสรุปจริง โดยไม่จำเป็นต้องใช้คำเหมือนกันเป๊ะเหมือนกับ ROUGE หรือ BLEU แต่ BERTScore จะวัดว่าคำหรือวลีมีความหมายใกล้เคียงกันหรือไม่\n",
    "\t- วัดความคล้ายกันของคำใน embedding space เช่น \"car\" vs \"vehicle\" ก็ยังถือว่าใกล้เคียง\n",
    "\t- ใช้ precision / recall / F1 score ตามระยะห่างของ vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc373f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "from bert_score import score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fb626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# โหลด dataset\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('tokenizer_samsum_su_finetune')\n",
    "model = EncoderDecoderModel.from_pretrained('bert_samsum_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453966ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ย้ายโมเดลและข้อมูลไปยังอุปกรณ์ที่เหมาะสม (GPU หรือ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# วิเคราะห์ข้อมูลและทำการ summary ด้วย bert_samsum_finetuned\n",
    "def generate_summary(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # ย้ายข้อมูลไปยัง device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            max_length=512, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=model.config.decoder_start_token_id,  # กำหนดที่นี่\n",
    "            pad_token_id=model.config.pad_token_id  # กำหนด pad_token_id ถ้าจำเป็น\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9eb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 819/819 [39:14<00:00,  2.88s/sample]\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.08103071050965537, 'rouge2': 0.005501493938462314, 'rougeL': 0.07293283175759344}\n",
      "BLEU Score: 8.859648156109322e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.8399370312690735 Recall: 0.8468782305717468 F1: 0.843207597732544\n"
     ]
    }
   ],
   "source": [
    "# 1. ROUGE Score Calculation\n",
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return {key: sum(value)/len(value) for key, value in scores.items()}\n",
    "\n",
    "# 2. BLEU Score Calculation\n",
    "def calculate_bleu(predictions, references):\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [ref.split()]\n",
    "        bleu_scores.append(sentence_bleu(ref_tokens, pred_tokens))\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# 3. BERTScore Calculation\n",
    "def calculate_bertscore(predictions, references):\n",
    "    P, R, F1 = score(predictions, references, lang='en')\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "# การทดสอบกับ dataset\n",
    "def evaluate_model(dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # ใช้ข้อมูลจาก train สำหรับทำนาย และข้อมูลจาก test สำหรับการเปรียบเทียบ\n",
    "    for i in tqdm(range(len(dataset['test'])), desc=\"Evaluating\", unit=\"sample\"):\n",
    "        # สร้างสรุปจากโมเดล\n",
    "        input_text = dataset['train'][i]['dialogue']  # ใช้ 'dialogue' จาก train เพื่อสร้างสรุป\n",
    "        reference_summary = dataset['test'][i]['summary']  # ใช้ 'summary' จาก test เป็นสรุปจริง\n",
    "        pred_summary = generate_summary(input_text)  # สร้างสรุปจากโมเดล\n",
    "        \n",
    "        predictions.append(pred_summary)\n",
    "        references.append(reference_summary)\n",
    "    \n",
    "    # ROUGE Score\n",
    "    rouge_scores = calculate_rouge(predictions, references)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu_score = calculate_bleu(predictions, references)\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = calculate_bertscore(predictions, references)\n",
    "    print(\"BERTScore - Precision:\", P, \"Recall:\", R, \"F1:\", F1)\n",
    "\n",
    "# เรียกใช้งาน\n",
    "evaluate_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb669ba",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b20cbd",
   "metadata": {},
   "source": [
    "\n",
    "**เทียบกับ Paper**\n",
    "\n",
    "| **Parameter**       | **Code**                          | **Paper (Section 3.2)**          | **เหมือน / ไม่เหมือน**        |\n",
    "| ------------------- | --------------------------------- | -------------------------------- | --------------       |\n",
    "| Model               | BERT2BERT (EncoderDecoderModel)   | BERT2BERT                        | เหมือน                 |\n",
    "| Tokenizer           | bert-base-uncased + custom tokens | ใช้ tokenizer ดัดแปลง              | เหมือน              |\n",
    "| Batch Size          | 8                                 | **16 (per step)**                | ไม่เหมือน → เล็กกว่า  |\n",
    "| Epochs              | 3                                 | 3                                | เหมือน              |\n",
    "| Learning Rate       | 5e-5                              | **3e-5**                         | ไม่เหมือน → สูงกว่า   |\n",
    "| Warmup Steps        | 500                               | ใช้ scheduler (แต่ไม่ระบุ exact)     | เหมือน (สมเหตุสมผล) |\n",
    "| Max Length (input)  | 512                               | 512                              | เหมือน              |\n",
    "| Max Length (output) | 128                               | 128                              | เหมือน              |\n",
    "| Beam Search         | 4                                 | 4                                | เหมือน              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d28c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc; gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                    \n",
      "  0%|          | 89/22098 [02:08<1:05:53,  5.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5135, 'learning_rate': 5e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "\n",
      "  0%|          | 89/22098 [02:25<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25244662165641785, 'eval_runtime': 16.8899, 'eval_samples_per_second': 48.431, 'eval_steps_per_second': 24.216, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 89/22098 [03:54<1:05:53,  5.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2455, 'learning_rate': 4.884248541531624e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                   \n",
      "\n",
      "  0%|          | 89/22098 [04:11<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[ARemoved shared tensor {'decoder.cls.predictions.decoder.weight', 'decoder.bert.encoder.layer.7.attention.self.key.bias', 'decoder.bert.encoder.layer.10.attention.self.key.weight', 'decoder.bert.encoder.layer.10.intermediate.dense.weight', 'decoder.bert.encoder.layer.2.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.self.value.bias', 'decoder.bert.encoder.layer.2.attention.output.dense.weight', 'decoder.bert.encoder.layer.6.attention.self.query.bias', 'decoder.bert.embeddings.token_type_embeddings.weight', 'decoder.bert.encoder.layer.0.attention.output.dense.weight', 'decoder.bert.encoder.layer.0.intermediate.dense.weight', 'decoder.bert.encoder.layer.5.attention.self.value.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.weight', 'decoder.bert.encoder.layer.8.intermediate.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.attention.self.key.weight', 'decoder.bert.encoder.layer.7.attention.self.query.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.attention.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.output.dense.bias', 'decoder.bert.encoder.layer.5.attention.self.key.weight', 'decoder.bert.encoder.layer.2.attention.self.value.bias', 'decoder.bert.encoder.layer.6.intermediate.dense.weight', 'decoder.bert.encoder.layer.11.attention.self.query.weight', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.self.value.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'decoder.bert.embeddings.LayerNorm.bias', 'decoder.bert.encoder.layer.8.attention.self.key.bias', 'decoder.bert.encoder.layer.7.output.dense.bias', 'decoder.bert.encoder.layer.6.intermediate.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.query.weight', 'decoder.bert.encoder.layer.11.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.key.bias', 'decoder.bert.encoder.layer.3.attention.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.query.bias', 'decoder.bert.encoder.layer.3.output.dense.weight', 'decoder.bert.encoder.layer.6.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.bias', 'decoder.bert.encoder.layer.1.output.dense.weight', 'decoder.bert.encoder.layer.7.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.self.query.weight', 'decoder.bert.encoder.layer.8.attention.self.query.weight', 'decoder.bert.encoder.layer.8.attention.self.value.weight', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.attention.self.value.weight', 'decoder.bert.encoder.layer.8.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.self.value.bias', 'decoder.bert.encoder.layer.3.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.query.bias', 'decoder.bert.encoder.layer.6.attention.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.key.bias', 'decoder.bert.encoder.layer.3.attention.self.value.weight', 'decoder.bert.encoder.layer.0.output.dense.bias', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.attention.self.value.bias', 'decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.attention.self.key.bias', 'decoder.bert.encoder.layer.4.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.intermediate.dense.bias', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.self.value.bias', 'decoder.bert.encoder.layer.10.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.self.value.weight', 'decoder.bert.encoder.layer.5.attention.self.query.weight', 'decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.attention.self.query.bias', 'decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.attention.self.value.weight', 'decoder.bert.encoder.layer.8.intermediate.dense.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.output.dense.bias', 'decoder.bert.encoder.layer.8.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.output.dense.weight', 'decoder.bert.encoder.layer.4.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.intermediate.dense.weight', 'decoder.bert.embeddings.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.weight', 'decoder.bert.encoder.layer.0.attention.self.key.weight', 'decoder.bert.encoder.layer.1.attention.self.query.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.weight', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.self.key.bias', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.self.value.weight', 'decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.self.query.weight', 'decoder.bert.encoder.layer.6.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.self.query.bias', 'decoder.bert.encoder.layer.11.attention.self.value.bias', 'decoder.bert.encoder.layer.2.output.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.bert.embeddings.word_embeddings.weight', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.query.bias', 'decoder.bert.encoder.layer.8.output.dense.weight', 'decoder.bert.encoder.layer.5.output.dense.weight', 'decoder.bert.encoder.layer.7.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.attention.self.value.weight', 'decoder.cls.predictions.decoder.bias', 'decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.attention.self.value.weight', 'decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'decoder.bert.encoder.layer.4.attention.self.key.weight', 'decoder.bert.encoder.layer.2.attention.self.key.bias', 'decoder.bert.encoder.layer.9.attention.self.query.weight', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.value.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.query.weight', 'decoder.bert.encoder.layer.11.output.dense.weight', 'decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.weight', 'decoder.bert.encoder.layer.0.attention.output.dense.bias', 'decoder.bert.encoder.layer.1.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.self.key.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.attention.output.dense.bias', 'decoder.bert.encoder.layer.10.attention.self.value.weight', 'decoder.bert.encoder.layer.8.attention.output.dense.weight', 'decoder.bert.encoder.layer.7.output.dense.weight', 'decoder.bert.encoder.layer.0.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.intermediate.dense.weight', 'decoder.bert.encoder.layer.2.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.query.weight', 'decoder.bert.encoder.layer.7.intermediate.dense.weight', 'decoder.bert.encoder.layer.4.attention.self.value.weight', 'decoder.bert.embeddings.position_embeddings.weight', 'decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.key.weight', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.attention.output.dense.bias', 'decoder.bert.encoder.layer.9.output.dense.weight', 'decoder.bert.encoder.layer.2.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.value.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.key.weight', 'decoder.bert.encoder.layer.3.intermediate.dense.bias', 'decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.self.query.bias', 'decoder.bert.encoder.layer.0.attention.self.query.weight', 'decoder.bert.encoder.layer.5.attention.self.key.bias', 'decoder.bert.encoder.layer.6.attention.output.dense.weight', 'decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.intermediate.dense.weight', 'decoder.bert.encoder.layer.10.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.attention.self.value.weight', 'decoder.bert.encoder.layer.0.attention.self.query.bias', 'decoder.bert.encoder.layer.11.output.dense.bias', 'decoder.bert.encoder.layer.4.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.key.bias', 'decoder.bert.encoder.layer.5.output.dense.bias', 'decoder.bert.encoder.layer.10.output.dense.weight', 'decoder.bert.encoder.layer.2.intermediate.dense.weight', 'decoder.bert.encoder.layer.3.attention.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.query.weight', 'decoder.bert.encoder.layer.9.intermediate.dense.bias', 'decoder.bert.encoder.layer.10.output.dense.bias', 'decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.output.dense.bias', 'decoder.bert.encoder.layer.8.output.dense.bias', 'decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.attention.self.key.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.attention.self.key.weight', 'decoder.bert.encoder.layer.11.attention.self.key.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.self.key.weight', 'decoder.bert.encoder.layer.3.attention.self.query.bias', 'decoder.bert.encoder.layer.2.attention.self.key.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22957713901996613, 'eval_runtime': 16.7844, 'eval_samples_per_second': 48.736, 'eval_steps_per_second': 24.368, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "                                                    \n",
      "  0%|          | 89/22098 [05:41<1:05:53,  5.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.236, 'learning_rate': 4.768497083063247e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                   \n",
      "\n",
      "  0%|          | 89/22098 [05:58<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2200525552034378, 'eval_runtime': 16.7523, 'eval_samples_per_second': 48.829, 'eval_steps_per_second': 24.415, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 89/22098 [07:31<1:05:53,  5.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2269, 'learning_rate': 4.6527456245948706e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                   \n",
      "\n",
      "  0%|          | 89/22098 [07:48<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21332186460494995, 'eval_runtime': 17.0525, 'eval_samples_per_second': 47.969, 'eval_steps_per_second': 23.985, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "                                                    \n",
      "  0%|          | 89/22098 [09:18<1:05:53,  5.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2156, 'learning_rate': 4.5369941661264933e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                   \n",
      "\n",
      "  0%|          | 89/22098 [09:34<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20742909610271454, 'eval_runtime': 16.3043, 'eval_samples_per_second': 50.171, 'eval_steps_per_second': 25.085, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 89/22098 [11:01<1:05:53,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2085, 'learning_rate': 4.421242707658116e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "\n",
      "  0%|          | 89/22098 [11:17<1:05:53,  5.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20444822311401367, 'eval_runtime': 15.8336, 'eval_samples_per_second': 51.662, 'eval_steps_per_second': 25.831, 'epoch': 0.41}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# ------------------------------\n",
    "# Load processed dataset & tokenizer\n",
    "# ------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load pretrained EncoderDecoderModel\n",
    "# ------------------------------\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\",         # encoder\n",
    "    \"bert-base-uncased\",         # decoder (ใช้ weights เดิม-ชุดเดียวกัน)\n",
    "    tie_encoder_decoder=True     # แชร์น้ำหนัก – เหมือนใน paper\n",
    ")\n",
    "\n",
    "# ปรับขนาด embedding หลังเพิ่ม emoji + speaker tokens\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model.tie_weights()  \n",
    "\n",
    "# Load your pretrained encoder weights\n",
    "# model.encoder.load_state_dict(torch.load(\"bert_su_pretrained.pt\", map_location=\"cpu\"))\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# ------------------------------\n",
    "# Define training arguments\n",
    "# ------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    # fp16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ------------------------------\n",
    "# Data Collator & Trainer\n",
    "# ------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Start Training\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "model.save_pretrained(\"bert_samsum_finetuned_no_SU\")\n",
    "tokenizer.save_pretrained(\"tokenizer_samsum_finetune_no_SU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed05e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from inspect import signature\n",
    "import transformers\n",
    "print(transformers.__version__)               # 4.52.1\n",
    "print(\"evaluation_strategy\" in signature(transformers.Seq2SeqTrainingArguments).parameters)\n",
    "# ต้องได้ True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsum-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
