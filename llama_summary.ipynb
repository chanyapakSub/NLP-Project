{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4aff11",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c2228",
   "metadata": {},
   "source": [
    "## Environment & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd19806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 0)  Install (first‐time only) & import libs\n",
    "# ---------------------------------------------\n",
    "# !pip install -q datasets transformers emoji==2.10.0 tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import emoji\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef5e48",
   "metadata": {},
   "source": [
    "## Load SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1a8833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/drl-68/.cache/huggingface/modules/datasets_modules/datasets/samsum/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e (last modified on Mon May  5 10:16:55 2025) since it couldn't be found locally at samsum, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1) Load SAMSum — 14 732 / 819 / 818 dialogues\n",
    "# ---------------------------------------------------------\n",
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35306eb1",
   "metadata": {},
   "source": [
    "## Build an emoji vocabulary and speaker token & Build / extend the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa85040",
   "metadata": {},
   "source": [
    "count [UNK] occurrences in one HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80902ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def count_unk(ds, tokenizer, field=\"dialogue\", batch_size=1024):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    total_unk, total_tokens = 0, 0\n",
    "\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc=\"Tokenising\"):\n",
    "        batch_texts = ds[i : i + batch_size][field]\n",
    "        enc = tokenizer(batch_texts, add_special_tokens=True, padding=False, truncation=False)\n",
    "        for ids in enc[\"input_ids\"]:\n",
    "            arr = np.array(ids)\n",
    "            total_unk += np.sum(arr == unk_id)\n",
    "            total_tokens += len(arr)\n",
    "    return total_unk, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468907",
   "metadata": {},
   "source": [
    "BEFORE adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0622d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 26.71it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 42.07it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 41.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts BEFORE adding emoji tokens\n",
      "train     :        0  (0.000% of tokens)\n",
      "validation:        0  (0.000% of tokens)\n",
      "test      :        0  (0.000% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_base = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "unk_stats_before = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_before[split] = count_unk(raw_ds[split], tok_base)\n",
    "print(\"\\n[UNK] counts BEFORE adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_before.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc3a6b",
   "metadata": {},
   "source": [
    "สร้าง EMOJI_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aeea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emojis found: 305\n"
     ]
    }
   ],
   "source": [
    "# ถ้า kernel เพิ่งรีสตาร์ต ตัวแปรจะหายหมด\n",
    "# สร้างชุด emoji ใหม่จาก raw_ds\n",
    "from typing import List\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    return [ch for ch in text if ch in emoji.EMOJI_DATA]\n",
    "\n",
    "emoji_set = set()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for dlg in raw_ds[split][\"dialogue\"]:\n",
    "        emoji_set.update(extract_emojis(dlg))\n",
    "\n",
    "EMOJI_TOKENS = sorted(emoji_set)          # ≈ 300-320 รายการ\n",
    "print(f\"Unique emojis found: {len(EMOJI_TOKENS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b121db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aebd7e",
   "metadata": {},
   "source": [
    "Extend tokenizer with emojis + speaker tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef28354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size : 32000\n",
      "Added new tokens     : 315  (emoji = 305, speaker = 10)\n",
      "New vocab size       : 32310\n",
      "\n",
      "First 20 emoji tokens: ['‼', '⏱', '☀', '☂', '☔', '☕', '☘', '☝', '☠', '☢', '☹', '☺', '♀', '♂', '♥', '♻', '⚪', '⚫', '⚰', '⚽']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su/tokenizer.model',\n",
       " 'tokenizer_samsum_su/added_tokens.json',\n",
       " 'tokenizer_samsum_su/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------- 1) โหลด tokenizer ดั้งเดิม ----------\n",
    "tok_base = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "vocab_orig = len(tok_base)\n",
    "\n",
    "# ---------- 2) เตรียมชุด token ใหม่ ----------\n",
    "#   • EMOJI_TOKENS  : ทุกอิโมจิที่ “พบอย่างน้อย 1 ครั้ง” ใน SAMSum\n",
    "#   • SPEAKER_TOKENS: [S1] – [S10]\n",
    "SPEAKER_TOKENS = [f\"[S{i}]\" for i in range(1, 11)]\n",
    "new_tokens = EMOJI_TOKENS + SPEAKER_TOKENS\n",
    "\n",
    "# ---------- 3) สร้าง tokenizer สำเนาแล้วเพิ่ม token ----------\n",
    "tok_ext = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "added = tok_ext.add_tokens(new_tokens)\n",
    "vocab_new = len(tok_ext)\n",
    "\n",
    "# ---------- 4) แสดงผล ----------\n",
    "print(f\"Original vocab size : {vocab_orig}\")\n",
    "print(f\"Added new tokens     : {added}  \"\n",
    "      f\"(emoji = {len(EMOJI_TOKENS)}, speaker = {len(SPEAKER_TOKENS)})\")\n",
    "print(f\"New vocab size       : {vocab_new}\")\n",
    "\n",
    "# (Optional) พิมพ์ตัวอย่างอิโมจิ 20 ตัวแรก\n",
    "print(\"\\nFirst 20 emoji tokens:\", EMOJI_TOKENS[:20])\n",
    "\n",
    "tok_ext.save_pretrained(\"tokenizer_samsum_su\")   # โฟลเดอร์ใหม่"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc38ac",
   "metadata": {},
   "source": [
    "AFTER adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bc9fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 28.52it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 41.31it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 38.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts AFTER adding emoji tokens\n",
      "train     :        0  (0.000% of tokens)\n",
      "validation:        0  (0.000% of tokens)\n",
      "test      :        0  (0.000% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unk_stats_after = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_after[split] = count_unk(raw_ds[split], tok_ext)\n",
    "print(\"\\n[UNK] counts AFTER adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_after.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb039",
   "metadata": {},
   "source": [
    "reduction check in UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8480431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Δ [UNK] (before ➜ after):\n",
      "train     : +0  fewer UNKs  (↓nan%)\n",
      "validation: +0  fewer UNKs  (↓nan%)\n",
      "test      : +0  fewer UNKs  (↓nan%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619269/1937097288.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"{split:<10}: {u0-u1:+d}  fewer UNKs  (↓{(u0-u1)/u0:.2%})\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nΔ [UNK] (before ➜ after):\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    u0, _ = unk_stats_before[split]\n",
    "    u1, _ = unk_stats_after[split]\n",
    "    print(f\"{split:<10}: {u0-u1:+d}  fewer UNKs  (↓{(u0-u1)/u0:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd47d8f",
   "metadata": {},
   "source": [
    "## Preprocess SAMSum Dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d4138",
   "metadata": {},
   "source": [
    "Speaker-name mapping → [S#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20077039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) Helper to replace speaker names by [S#]\n",
    "# ---------------------------------------------------------\n",
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10\n",
    "                 ) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns dialogue with names replaced by [S#] and a mapping dict.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:                # safety – keep line as is\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:      # truncate extra speakers\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        new_lines.append(f\"{speaker_map.get(name, '[SUNK]')}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad9bc",
   "metadata": {},
   "source": [
    "Insert [SEP] after every utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c65bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02e888",
   "metadata": {},
   "source": [
    "Switching-Utterance corruption\n",
    "- Hyper-parameters: Pu = 1.0, Pn = 0/1\n",
    "\n",
    "โดยที่\n",
    "\n",
    "Pu (permute-utterance prob.) ความน่าจะเป็นที่ แต่ละ utterance จะถูกเลือก ใส่ลงในชุดที่นำไปสับตำแหน่ง\n",
    "\n",
    "- pu = 1.0 แสดงว่าบังคับเลือกทุกบรรทัดแล้วค่อยสับคำแบบสุ่ม\n",
    "\n",
    "Pn (name-mask prob.) ความน่าจะเป็นที่ token [S#] ด้านหน้าจะถูกเปลี่ยนเป็น [MASK]\n",
    "\n",
    "- pn = 0.0 แสดงว่า ไม่ mask, โมเดลเห็น speaker tag\n",
    "\n",
    "- pn = 1.0 แสดงว่า mask หมด, บังคับดู context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85ef01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_switching_utterance(dialogue: str,\n",
    "                             pu: float = 1.0,\n",
    "                             pn: float = 0.0,\n",
    "                             rng: random.Random = random\n",
    "                            ) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    • dialogue  - speaker-tokenised, SEP-inserted string\n",
    "    • pu        - prob. an utterance is selected for permutation\n",
    "    • pn        - prob. we MASK the speaker token (⇒ [MASK])\n",
    "    Returns:\n",
    "        corrupted_dialogue, labels_per_utt  (1 = permuted (สลับบทพูด), 0 = original)\n",
    "    \"\"\"\n",
    "    # 1) split back into utterances\n",
    "    utts = [u.strip() for u in dialogue.split(\"[SEP]\") if u.strip()]\n",
    "    idxs = list(range(len(utts)))\n",
    "\n",
    "    # 2) pick indices to permute\n",
    "    perm_idx = [i for i in idxs if rng.random() < pu]\n",
    "    shuffled = perm_idx.copy()\n",
    "    rng.shuffle(shuffled)                 # in-place\n",
    "    perm_map = dict(zip(perm_idx, shuffled))\n",
    "\n",
    "    # 3) build new utterance list, labels\n",
    "    new_utts, labels = [], []\n",
    "    for i in idxs:\n",
    "        src = perm_map.get(i, i)          # swapped or same\n",
    "        u = utts[src]\n",
    "        # optionally mask speaker token ([S#]: → [MASK]:)\n",
    "        if rng.random() < pn:\n",
    "            u = re.sub(r\"^\\[S\\d+\\]\", \"[MASK]\", u)\n",
    "        new_utts.append(u)\n",
    "        labels.append(int(src != i))      # 1 if permuted\n",
    "    corrupted = \" [SEP] \".join(new_utts) + \" [SEP]\"\n",
    "    return corrupted, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d05b",
   "metadata": {},
   "source": [
    "## Switching-Utterance (SU) pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5949ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building SU train: 100%|██████████| 14732/14732 [00:06<00:00, 2314.91 examples/s]\n",
      "Building SU validation: 100%|██████████| 818/818 [00:00<00:00, 2314.77 examples/s]\n",
      "Building SU test: 100%|██████████| 819/819 [00:00<00:00, 1859.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 455587.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 212916.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 221435.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7) Create HF Datasets with tokenised inputs, attention,\n",
    "#    SEP positions, and per-utterance labels\n",
    "# ---------------------------------------------------------\n",
    "MAX_LEN = 512                          # paper setting\n",
    "Pu, Pn = 1.0, 0.0                      # best config in Table 2\n",
    "\n",
    "\n",
    "def preprocess_example(example, split):\n",
    "\n",
    "    if tok_ext.pad_token is None:\n",
    "        tok_ext.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "    # a) replace speakers & add SEP\n",
    "    dlg, _ = map_speakers(example[\"dialogue\"])\n",
    "    dlg = add_sep_every_utt(dlg)\n",
    "\n",
    "    # b) corruption\n",
    "    corrupted, labels = make_switching_utterance(dlg, Pu, Pn)\n",
    "\n",
    "    # c) tokenize (truncate if >512 tokens)\n",
    "    enc = tok_ext(corrupted,\n",
    "              truncation=True, max_length=MAX_LEN,\n",
    "              padding=\"max_length\")\n",
    "    \n",
    "    # d) find SEP token positions (needed for loss later)\n",
    "    sep_id = tok_ext(\"[SEP]\")[\"input_ids\"][0]\n",
    "    sep_positions = [i for i, id_ in enumerate(enc[\"input_ids\"])\n",
    "                     if id_ == sep_id][:len(labels)]  # clip if truncated\n",
    "\n",
    "    enc[\"labels\"] = labels[:len(sep_positions)]\n",
    "    enc[\"sep_positions\"] = sep_positions\n",
    "    enc[\"dialogue_len\"] = len(labels)\n",
    "    return enc\n",
    "\n",
    "su_ds = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    su_ds[split] = raw_ds[split].map(\n",
    "        preprocess_example,\n",
    "        fn_kwargs={\"split\": split},\n",
    "        remove_columns=raw_ds[split].column_names,\n",
    "        desc=f\"Building SU {split}\"\n",
    "    )\n",
    "\n",
    "su_ds.save_to_disk(\"data/samsum_switching_utterance\")\n",
    "print(su_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4605bbc",
   "metadata": {},
   "source": [
    "ไฟล์ Arrow ถูกบันทึกไว้ที่ data/samsum_switching_utterance/ พร้อมฟิลด์ input_ids / attention_mask / labels / sep_positions / dialogue_len."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b54f7",
   "metadata": {},
   "source": [
    "## Self-supervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c9c0",
   "metadata": {},
   "source": [
    "ใช้ Dataset เฉพาะส่วนของ train ของ SAMSum มาทำการ pre_train แล้วใช้ validation ไว้ดู early-stopping / tuning ส่วน test ต้องไม่ถูกแตะ เพื่อไม่ให้โมเดล “เห็น” บทสนทนาที่จะใช้วัด ROUGE ภายหลัง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363e9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc246a",
   "metadata": {},
   "source": [
    "Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da87342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4 # paper 128\n",
    "MAX_LEN    = 512 # paper 512\n",
    "LR         = 3e-5\n",
    "WARMUP     = 500\n",
    "MAX_STEPS  = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc898da",
   "metadata": {},
   "source": [
    "Dataset & collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5875dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_switching_utterance\")\n",
    "\n",
    "# -------------------------------\n",
    "# COLLATE FUNCTION\n",
    "# -------------------------------\n",
    "# def collate_fn(batch):\n",
    "#     keys = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "#     inputs = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
    "#     labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "#     sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "#     return inputs, labels, sep_pos\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    inputs = {\n",
    "        k: torch.stack([torch.tensor(b[k]) if not isinstance(b[k], torch.Tensor) else b[k] for b in batch])\n",
    "        for k in keys if k not in [\"labels\", \"sep_positions\"]\n",
    "    }\n",
    "    labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "    sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "    return inputs, labels, sep_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ca8c2",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f94a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MODEL\n",
    "# -------------------------------\n",
    "class SepClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama_v1.1\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.llama = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            # torch_dtype=torch.float16  # ใช้ float16 เฉพาะ LLaMA model\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)  \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sep_positions, **kwargs):\n",
    "        # เรียก model โดยให้คืนค่า hidden states ทั้งหมด\n",
    "        outputs = self.llama(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        # เอา hidden state ชั้นสุดท้ายมาใช้\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        # ดึง hidden vectors ที่ตำแหน่ง sep ของแต่ละ sample\n",
    "        sep_vecs = []\n",
    "        for i, pos_tensor in enumerate(sep_positions):\n",
    "            if pos_tensor.numel() == 0:\n",
    "                continue  # ข้าม sample ที่ไม่มี SEP\n",
    "            pos_tensor = pos_tensor.to(hidden_states.device).long()\n",
    "            if torch.any(pos_tensor >= hidden_states.shape[1]):\n",
    "                raise ValueError(f\"Invalid sep_position index: {pos_tensor.tolist()} exceeds hidden size {hidden_states.shape[1]}\")\n",
    "            sep_vecs.append(hidden_states[i].index_select(0, pos_tensor))\n",
    "\n",
    "        if not sep_vecs:\n",
    "            raise ValueError(\"All sep_positions are empty. Cannot compute logits.\")\n",
    "\n",
    "        sep_vecs = torch.cat(sep_vecs, dim=0)\n",
    "        sep_vecs = torch.nan_to_num(sep_vecs, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        sep_vecs = torch.clamp(sep_vecs, -1e4, 1e4)  # limit range\n",
    "\n",
    "        # Apply dropout และ classifier\n",
    "        logits = self.classifier(self.dropout(sep_vecs)).squeeze(-1)  # (total_seps,)\n",
    "        logits = torch.clamp(logits, -30, 30)  # clamp เพื่อป้องกัน NaN ใน BCEWithLogitsLoss\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a065c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# เคลียร์ cache ของ CUDA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# เคลียร์ object ที่ไม่ใช้งานแล้ว\n",
    "gc.collect()\n",
    "\n",
    "# (Optional) หากใช้ AMP หรือ context manager อื่นๆ\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.ipc_collect()  # สำหรับเคลียร์ inter-process memory (เฉพาะบางเคส)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307a9de",
   "metadata": {},
   "source": [
    "Training loop (train model until the train loss converged (upper bounded by 5k steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26145b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   3%|▎         | 101/3683 [00:10<06:02,  9.88it/s, avg_loss=0.69, loss=0.69, step=101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  100/40000] Avg Loss: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   5%|▌         | 201/3683 [00:20<05:52,  9.88it/s, avg_loss=0.686, loss=0.686, step=201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  200/40000] Avg Loss: 0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   8%|▊         | 301/3683 [00:30<05:41,  9.90it/s, avg_loss=0.693, loss=0.693, step=301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  300/40000] Avg Loss: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  11%|█         | 401/3683 [00:40<05:33,  9.85it/s, avg_loss=0.687, loss=0.687, step=401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  400/40000] Avg Loss: 0.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  14%|█▎        | 501/3683 [00:50<05:19,  9.95it/s, avg_loss=0.678, loss=0.678, step=501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  500/40000] Avg Loss: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  16%|█▋        | 601/3683 [01:00<05:10,  9.92it/s, avg_loss=0.673, loss=0.673, step=601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  600/40000] Avg Loss: 0.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  19%|█▉        | 702/3683 [01:10<04:50, 10.27it/s, avg_loss=0.669, loss=0.669, step=702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  700/40000] Avg Loss: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  22%|██▏       | 802/3683 [01:20<04:38, 10.34it/s, avg_loss=0.665, loss=0.665, step=802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  800/40000] Avg Loss: 0.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  24%|██▍       | 902/3683 [01:29<04:26, 10.45it/s, avg_loss=0.669, loss=0.678, step=902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  900/40000] Avg Loss: 0.6713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  27%|██▋       | 1002/3683 [01:39<04:21, 10.24it/s, avg_loss=0.667, loss=0.658, step=1002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1000/40000] Avg Loss: 0.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  30%|██▉       | 1102/3683 [01:48<04:05, 10.52it/s, avg_loss=0.664, loss=0.674, step=1102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1100/40000] Avg Loss: 0.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  33%|███▎      | 1202/3683 [01:58<04:00, 10.33it/s, avg_loss=0.651, loss=0.651, step=1202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1200/40000] Avg Loss: 0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  35%|███▌      | 1302/3683 [02:08<03:48, 10.40it/s, avg_loss=0.66, loss=0.649, step=1302] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1300/40000] Avg Loss: 0.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  38%|███▊      | 1402/3683 [02:17<03:38, 10.44it/s, avg_loss=0.682, loss=0.67, step=1402] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1400/40000] Avg Loss: 0.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  41%|████      | 1502/3683 [02:27<03:26, 10.54it/s, avg_loss=0.656, loss=0.669, step=1502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1500/40000] Avg Loss: 0.6588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  43%|████▎     | 1602/3683 [02:36<03:20, 10.37it/s, avg_loss=0.655, loss=0.668, step=1602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1600/40000] Avg Loss: 0.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  46%|████▌     | 1702/3683 [02:46<03:11, 10.35it/s, avg_loss=0.653, loss=0.639, step=1702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1700/40000] Avg Loss: 0.6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  49%|████▉     | 1802/3683 [02:56<03:02, 10.32it/s, avg_loss=0.651, loss=0.666, step=1802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1800/40000] Avg Loss: 0.6517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  52%|█████▏    | 1902/3683 [03:05<02:51, 10.40it/s, avg_loss=0.649, loss=0.665, step=1902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1900/40000] Avg Loss: 0.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  54%|█████▍    | 2002/3683 [03:15<02:41, 10.43it/s, avg_loss=0.648, loss=0.632, step=2002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2000/40000] Avg Loss: 0.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  57%|█████▋    | 2102/3683 [03:25<02:33, 10.31it/s, avg_loss=0.631, loss=0.631, step=2102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2100/40000] Avg Loss: 0.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  60%|█████▉    | 2202/3683 [03:34<02:22, 10.42it/s, avg_loss=0.63, loss=0.63, step=2202]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2200/40000] Avg Loss: 0.6483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  63%|██████▎   | 2302/3683 [03:44<02:12, 10.39it/s, avg_loss=0.662, loss=0.662, step=2302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2300/40000] Avg Loss: 0.6467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  65%|██████▌   | 2402/3683 [03:53<02:03, 10.37it/s, avg_loss=0.628, loss=0.628, step=2402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2400/40000] Avg Loss: 0.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  68%|██████▊   | 2502/3683 [04:03<01:52, 10.46it/s, avg_loss=0.644, loss=0.627, step=2502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2500/40000] Avg Loss: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  71%|███████   | 2602/3683 [04:13<01:43, 10.43it/s, avg_loss=0.644, loss=0.626, step=2602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2600/40000] Avg Loss: 0.6455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  73%|███████▎  | 2702/3683 [04:22<01:34, 10.37it/s, avg_loss=0.643, loss=0.661, step=2702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2700/40000] Avg Loss: 0.6466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  76%|███████▌  | 2802/3683 [04:32<01:25, 10.29it/s, avg_loss=0.643, loss=0.66, step=2802] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2800/40000] Avg Loss: 0.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  79%|███████▉  | 2902/3683 [04:41<01:14, 10.50it/s, avg_loss=0.66, loss=0.66, step=2902]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2900/40000] Avg Loss: 0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  82%|████████▏ | 3002/3683 [04:51<01:05, 10.42it/s, avg_loss=0.659, loss=0.659, step=3002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3000/40000] Avg Loss: 0.6420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  84%|████████▍ | 3102/3683 [05:01<00:56, 10.36it/s, avg_loss=0.622, loss=0.622, step=3102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3100/40000] Avg Loss: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  87%|████████▋ | 3202/3683 [05:10<00:46, 10.35it/s, avg_loss=0.658, loss=0.696, step=3202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3200/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  90%|████████▉ | 3302/3683 [05:20<00:36, 10.46it/s, avg_loss=0.677, loss=0.696, step=3302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3300/40000] Avg Loss: 0.6393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  92%|█████████▏| 3402/3683 [05:30<00:27, 10.40it/s, avg_loss=0.658, loss=0.696, step=3402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3400/40000] Avg Loss: 0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  95%|█████████▌| 3502/3683 [05:39<00:17, 10.37it/s, avg_loss=0.619, loss=0.619, step=3502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3500/40000] Avg Loss: 0.6390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  98%|█████████▊| 3602/3683 [05:49<00:07, 10.44it/s, avg_loss=0.638, loss=0.619, step=3602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3600/40000] Avg Loss: 0.6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 3683/3683 [05:57<00:00, 10.31it/s, avg_loss=0.642, loss=0.658, step=3683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 19/3683 [00:01<05:56, 10.29it/s, avg_loss=0.619, loss=0.619, step=3702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3700/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 119/3683 [00:10<05:08, 11.55it/s, avg_loss=0.638, loss=0.619, step=3802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3800/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 219/3683 [00:19<04:58, 11.59it/s, avg_loss=0.638, loss=0.619, step=3902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3900/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▊         | 319/3683 [00:28<04:50, 11.57it/s, avg_loss=0.619, loss=0.619, step=4002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4000/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  11%|█▏        | 419/3683 [00:36<04:41, 11.58it/s, avg_loss=0.619, loss=0.619, step=4102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4100/40000] Avg Loss: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  14%|█▍        | 519/3683 [00:45<04:33, 11.58it/s, avg_loss=0.619, loss=0.619, step=4202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4200/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  17%|█▋        | 619/3683 [00:54<04:24, 11.58it/s, avg_loss=0.658, loss=0.696, step=4302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4300/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  20%|█▉        | 719/3683 [01:02<04:15, 11.58it/s, avg_loss=0.638, loss=0.658, step=4402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4400/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  22%|██▏       | 819/3683 [01:11<04:07, 11.59it/s, avg_loss=0.638, loss=0.619, step=4502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4500/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  25%|██▍       | 919/3683 [01:20<03:59, 11.56it/s, avg_loss=0.677, loss=0.658, step=4602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4600/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  28%|██▊       | 1019/3683 [01:28<03:49, 11.59it/s, avg_loss=0.638, loss=0.619, step=4702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4700/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  30%|███       | 1119/3683 [01:37<03:41, 11.60it/s, avg_loss=0.658, loss=0.658, step=4802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4800/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  33%|███▎      | 1219/3683 [01:46<03:32, 11.59it/s, avg_loss=0.638, loss=0.619, step=4902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4900/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  36%|███▌      | 1319/3683 [01:54<03:24, 11.58it/s, avg_loss=0.638, loss=0.658, step=5002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5000/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  39%|███▊      | 1419/3683 [02:03<03:15, 11.59it/s, avg_loss=0.638, loss=0.619, step=5102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5100/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  41%|████      | 1519/3683 [02:11<03:07, 11.57it/s, avg_loss=0.638, loss=0.658, step=5202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5200/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  44%|████▍     | 1619/3683 [02:20<02:58, 11.58it/s, avg_loss=0.638, loss=0.658, step=5302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5300/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  47%|████▋     | 1719/3683 [02:29<02:49, 11.57it/s, avg_loss=0.619, loss=0.619, step=5402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5400/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  49%|████▉     | 1819/3683 [02:37<02:40, 11.60it/s, avg_loss=0.638, loss=0.658, step=5502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5500/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  52%|█████▏    | 1919/3683 [02:46<02:32, 11.60it/s, avg_loss=0.619, loss=0.619, step=5602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5600/40000] Avg Loss: 0.6457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  55%|█████▍    | 2019/3683 [02:55<02:23, 11.57it/s, avg_loss=0.638, loss=0.619, step=5702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5700/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  58%|█████▊    | 2119/3683 [03:03<02:15, 11.56it/s, avg_loss=0.696, loss=0.696, step=5802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  60%|██████    | 2219/3683 [03:12<02:06, 11.60it/s, avg_loss=0.638, loss=0.619, step=5902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5900/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  63%|██████▎   | 2319/3683 [03:21<01:57, 11.59it/s, avg_loss=0.619, loss=0.619, step=6002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6000/40000] Avg Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  66%|██████▌   | 2419/3683 [03:29<01:49, 11.55it/s, avg_loss=0.619, loss=0.619, step=6102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6100/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  68%|██████▊   | 2519/3683 [03:38<01:40, 11.59it/s, avg_loss=0.638, loss=0.619, step=6202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6200/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  71%|███████   | 2619/3683 [03:47<01:32, 11.56it/s, avg_loss=0.619, loss=0.619, step=6302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6300/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  74%|███████▍  | 2719/3683 [03:55<01:23, 11.58it/s, avg_loss=0.638, loss=0.619, step=6402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6400/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  77%|███████▋  | 2819/3683 [04:04<01:14, 11.58it/s, avg_loss=0.619, loss=0.619, step=6502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6500/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  79%|███████▉  | 2919/3683 [04:12<01:05, 11.58it/s, avg_loss=0.638, loss=0.619, step=6602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6600/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  82%|████████▏ | 3019/3683 [04:21<00:57, 11.60it/s, avg_loss=0.638, loss=0.658, step=6702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6700/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  85%|████████▍ | 3119/3683 [04:30<00:48, 11.59it/s, avg_loss=0.638, loss=0.619, step=6802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  87%|████████▋ | 3219/3683 [04:38<00:40, 11.55it/s, avg_loss=0.638, loss=0.658, step=6902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6900/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  90%|█████████ | 3319/3683 [04:47<00:31, 11.57it/s, avg_loss=0.638, loss=0.619, step=7002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7000/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  93%|█████████▎| 3419/3683 [04:56<00:22, 11.57it/s, avg_loss=0.658, loss=0.696, step=7102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7100/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  96%|█████████▌| 3519/3683 [05:04<00:14, 11.57it/s, avg_loss=0.658, loss=0.696, step=7202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7200/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  98%|█████████▊| 3619/3683 [05:13<00:05, 11.59it/s, avg_loss=0.619, loss=0.619, step=7302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7300/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3683/3683 [05:19<00:00, 11.54it/s, avg_loss=0.637, loss=0.696, step=7366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   1%|          | 35/3683 [00:03<05:15, 11.57it/s, avg_loss=0.638, loss=0.619, step=7402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7400/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   4%|▎         | 135/3683 [00:11<05:06, 11.56it/s, avg_loss=0.715, loss=0.735, step=7502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7500/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   6%|▋         | 235/3683 [00:20<04:57, 11.58it/s, avg_loss=0.619, loss=0.619, step=7602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7600/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   9%|▉         | 335/3683 [00:29<04:49, 11.58it/s, avg_loss=0.619, loss=0.619, step=7702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7700/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  12%|█▏        | 435/3683 [00:37<04:40, 11.56it/s, avg_loss=0.638, loss=0.658, step=7802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  15%|█▍        | 535/3683 [00:46<04:32, 11.57it/s, avg_loss=0.638, loss=0.619, step=7902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7900/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  17%|█▋        | 635/3683 [00:55<04:23, 11.56it/s, avg_loss=0.638, loss=0.658, step=8002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8000/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  20%|█▉        | 735/3683 [01:03<04:15, 11.52it/s, avg_loss=0.677, loss=0.658, step=8102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8100/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  23%|██▎       | 835/3683 [01:12<04:05, 11.58it/s, avg_loss=0.638, loss=0.619, step=8202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8200/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  25%|██▌       | 935/3683 [01:21<03:57, 11.57it/s, avg_loss=0.638, loss=0.619, step=8302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8300/40000] Avg Loss: 0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  28%|██▊       | 1035/3683 [01:29<03:48, 11.59it/s, avg_loss=0.658, loss=0.619, step=8402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8400/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  31%|███       | 1135/3683 [01:38<03:40, 11.58it/s, avg_loss=0.638, loss=0.619, step=8502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8500/40000] Avg Loss: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  34%|███▎      | 1235/3683 [01:47<03:31, 11.56it/s, avg_loss=0.638, loss=0.658, step=8602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8600/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  36%|███▌      | 1335/3683 [01:55<03:23, 11.56it/s, avg_loss=0.638, loss=0.658, step=8702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8700/40000] Avg Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  39%|███▉      | 1435/3683 [02:04<03:14, 11.57it/s, avg_loss=0.638, loss=0.619, step=8802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8800/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  42%|████▏     | 1535/3683 [02:12<03:05, 11.57it/s, avg_loss=0.638, loss=0.619, step=8902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8900/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  44%|████▍     | 1635/3683 [02:21<02:56, 11.61it/s, avg_loss=0.638, loss=0.619, step=9002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9000/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  47%|████▋     | 1735/3683 [02:30<02:48, 11.56it/s, avg_loss=0.619, loss=0.619, step=9102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9100/40000] Avg Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  50%|████▉     | 1835/3683 [02:38<02:39, 11.56it/s, avg_loss=0.619, loss=0.619, step=9202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9200/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  53%|█████▎    | 1935/3683 [02:47<02:31, 11.58it/s, avg_loss=0.638, loss=0.658, step=9302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9300/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  55%|█████▌    | 2035/3683 [02:56<02:22, 11.58it/s, avg_loss=0.638, loss=0.619, step=9402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9400/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  58%|█████▊    | 2135/3683 [03:04<02:13, 11.57it/s, avg_loss=0.619, loss=0.619, step=9502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9500/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  61%|██████    | 2235/3683 [03:13<02:05, 11.57it/s, avg_loss=0.619, loss=0.619, step=9602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9600/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  63%|██████▎   | 2335/3683 [03:22<01:56, 11.56it/s, avg_loss=0.619, loss=0.619, step=9702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9700/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  66%|██████▌   | 2435/3683 [03:30<01:47, 11.58it/s, avg_loss=0.619, loss=0.619, step=9802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9800/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  69%|██████▉   | 2535/3683 [03:39<01:39, 11.59it/s, avg_loss=0.638, loss=0.658, step=9902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9900/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  72%|███████▏  | 2635/3683 [03:48<01:30, 11.58it/s, avg_loss=0.638, loss=0.619, step=1e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10000/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  74%|███████▍  | 2735/3683 [03:56<01:21, 11.58it/s, avg_loss=0.619, loss=0.619, step=10102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10100/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  77%|███████▋  | 2835/3683 [04:05<01:13, 11.57it/s, avg_loss=0.638, loss=0.619, step=10202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10200/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  80%|███████▉  | 2935/3683 [04:13<01:04, 11.56it/s, avg_loss=0.619, loss=0.619, step=10302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10300/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  82%|████████▏ | 3035/3683 [04:22<00:55, 11.59it/s, avg_loss=0.677, loss=0.696, step=10402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10400/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  85%|████████▌ | 3135/3683 [04:31<00:47, 11.59it/s, avg_loss=0.619, loss=0.619, step=10502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10500/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  88%|████████▊ | 3235/3683 [04:39<00:38, 11.56it/s, avg_loss=0.658, loss=0.658, step=10602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10600/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  91%|█████████ | 3335/3683 [04:48<00:30, 11.59it/s, avg_loss=0.658, loss=0.696, step=10702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10700/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  93%|█████████▎| 3435/3683 [04:57<00:21, 11.57it/s, avg_loss=0.696, loss=0.696, step=10802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  96%|█████████▌| 3535/3683 [05:05<00:12, 11.57it/s, avg_loss=0.638, loss=0.619, step=10902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10900/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  99%|█████████▊| 3635/3683 [05:14<00:04, 11.60it/s, avg_loss=0.619, loss=0.619, step=11002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11000/40000] Avg Loss: 0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3683/3683 [05:18<00:00, 11.56it/s, avg_loss=0.635, loss=0.658, step=11049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   1%|▏         | 53/3683 [00:04<05:13, 11.59it/s, avg_loss=0.658, loss=0.658, step=11102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11100/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   4%|▍         | 153/3683 [00:13<05:04, 11.58it/s, avg_loss=0.638, loss=0.619, step=11202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11200/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   7%|▋         | 253/3683 [00:21<04:55, 11.60it/s, avg_loss=0.619, loss=0.619, step=11302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11300/40000] Avg Loss: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  10%|▉         | 353/3683 [00:30<04:47, 11.59it/s, avg_loss=0.619, loss=0.619, step=11402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11400/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  12%|█▏        | 453/3683 [00:39<04:39, 11.56it/s, avg_loss=0.619, loss=0.619, step=11502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11500/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  15%|█▌        | 553/3683 [00:47<04:29, 11.60it/s, avg_loss=0.619, loss=0.619, step=11602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11600/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  18%|█▊        | 653/3683 [00:56<04:21, 11.57it/s, avg_loss=0.677, loss=0.696, step=11702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11700/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  20%|██        | 753/3683 [01:05<04:12, 11.59it/s, avg_loss=0.658, loss=0.619, step=11802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11800/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  23%|██▎       | 853/3683 [01:13<04:05, 11.55it/s, avg_loss=0.658, loss=0.696, step=11902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 11900/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  26%|██▌       | 953/3683 [01:22<03:55, 11.58it/s, avg_loss=0.658, loss=0.658, step=12002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12000/40000] Avg Loss: 0.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  29%|██▊       | 1053/3683 [01:31<03:47, 11.58it/s, avg_loss=0.638, loss=0.658, step=12102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12100/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  31%|███▏      | 1153/3683 [01:39<03:38, 11.55it/s, avg_loss=0.638, loss=0.658, step=12202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12200/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  34%|███▍      | 1253/3683 [01:48<03:30, 11.56it/s, avg_loss=0.619, loss=0.619, step=12302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12300/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  37%|███▋      | 1353/3683 [01:57<03:21, 11.58it/s, avg_loss=0.619, loss=0.619, step=12402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12400/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  39%|███▉      | 1453/3683 [02:05<03:12, 11.56it/s, avg_loss=0.619, loss=0.619, step=12502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12500/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  42%|████▏     | 1553/3683 [02:14<03:04, 11.56it/s, avg_loss=0.638, loss=0.658, step=12602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12600/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  45%|████▍     | 1653/3683 [02:23<02:55, 11.58it/s, avg_loss=0.638, loss=0.658, step=12702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12700/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  48%|████▊     | 1753/3683 [02:31<02:46, 11.57it/s, avg_loss=0.638, loss=0.619, step=12802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12800/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  50%|█████     | 1853/3683 [02:40<02:38, 11.57it/s, avg_loss=0.677, loss=0.658, step=12902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 12900/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  53%|█████▎    | 1953/3683 [02:48<02:29, 11.58it/s, avg_loss=0.696, loss=0.696, step=13002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13000/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  56%|█████▌    | 2053/3683 [02:57<02:20, 11.58it/s, avg_loss=0.619, loss=0.619, step=13102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13100/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  58%|█████▊    | 2153/3683 [03:06<02:12, 11.56it/s, avg_loss=0.619, loss=0.619, step=13202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13200/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  61%|██████    | 2253/3683 [03:14<02:03, 11.56it/s, avg_loss=0.638, loss=0.658, step=13302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13300/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  64%|██████▍   | 2353/3683 [03:23<01:55, 11.55it/s, avg_loss=0.619, loss=0.619, step=13402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13400/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  67%|██████▋   | 2453/3683 [03:32<01:46, 11.56it/s, avg_loss=0.619, loss=0.619, step=13502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13500/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  69%|██████▉   | 2553/3683 [03:40<01:37, 11.58it/s, avg_loss=0.677, loss=0.696, step=13602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13600/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  72%|███████▏  | 2653/3683 [03:49<01:28, 11.58it/s, avg_loss=0.677, loss=0.696, step=13702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13700/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  75%|███████▍  | 2753/3683 [03:58<01:20, 11.61it/s, avg_loss=0.619, loss=0.619, step=13802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13800/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  77%|███████▋  | 2853/3683 [04:06<01:11, 11.58it/s, avg_loss=0.619, loss=0.619, step=13902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13900/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  80%|████████  | 2953/3683 [04:15<01:03, 11.56it/s, avg_loss=0.619, loss=0.619, step=14002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14000/40000] Avg Loss: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  83%|████████▎ | 3053/3683 [04:24<00:54, 11.58it/s, avg_loss=0.619, loss=0.619, step=14102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14100/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  86%|████████▌ | 3153/3683 [04:32<00:45, 11.59it/s, avg_loss=0.619, loss=0.619, step=14202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14200/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  88%|████████▊ | 3253/3683 [04:41<00:37, 11.56it/s, avg_loss=0.619, loss=0.619, step=14302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14300/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  91%|█████████ | 3353/3683 [04:49<00:28, 11.57it/s, avg_loss=0.638, loss=0.658, step=14402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14400/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  94%|█████████▍| 3453/3683 [04:58<00:19, 11.60it/s, avg_loss=0.638, loss=0.619, step=14502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14500/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  96%|█████████▋| 3553/3683 [05:07<00:11, 11.58it/s, avg_loss=0.658, loss=0.658, step=14602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14600/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  99%|█████████▉| 3653/3683 [05:15<00:02, 11.58it/s, avg_loss=0.619, loss=0.619, step=14702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14700/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3683/3683 [05:18<00:00, 11.56it/s, avg_loss=0.635, loss=0.619, step=14732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:   2%|▏         | 69/3683 [00:06<05:11, 11.61it/s, avg_loss=0.658, loss=0.658, step=14802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14800/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:   5%|▍         | 169/3683 [00:14<05:02, 11.61it/s, avg_loss=0.638, loss=0.658, step=14902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 14900/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:   7%|▋         | 269/3683 [00:23<04:54, 11.59it/s, avg_loss=0.619, loss=0.619, step=15002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15000/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  10%|█         | 369/3683 [00:32<04:46, 11.59it/s, avg_loss=0.638, loss=0.658, step=15102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15100/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  13%|█▎        | 469/3683 [00:40<04:37, 11.60it/s, avg_loss=0.658, loss=0.696, step=15202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15200/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  15%|█▌        | 569/3683 [00:49<04:27, 11.62it/s, avg_loss=0.638, loss=0.619, step=15302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15300/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  18%|█▊        | 669/3683 [00:57<04:20, 11.56it/s, avg_loss=0.658, loss=0.658, step=15402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15400/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  21%|██        | 769/3683 [01:06<04:10, 11.61it/s, avg_loss=0.638, loss=0.658, step=15502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15500/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  24%|██▎       | 869/3683 [01:15<04:02, 11.58it/s, avg_loss=0.658, loss=0.658, step=15602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15600/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  26%|██▋       | 969/3683 [01:23<03:54, 11.59it/s, avg_loss=0.619, loss=0.619, step=15702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15700/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  29%|██▉       | 1069/3683 [01:32<03:45, 11.61it/s, avg_loss=0.638, loss=0.619, step=15802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15800/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  32%|███▏      | 1169/3683 [01:41<03:37, 11.57it/s, avg_loss=0.619, loss=0.619, step=15902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 15900/40000] Avg Loss: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  34%|███▍      | 1269/3683 [01:49<03:28, 11.59it/s, avg_loss=0.638, loss=0.658, step=16002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16000/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  37%|███▋      | 1369/3683 [01:58<03:19, 11.59it/s, avg_loss=0.638, loss=0.619, step=16102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16100/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  40%|███▉      | 1469/3683 [02:07<03:11, 11.58it/s, avg_loss=0.619, loss=0.619, step=16202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16200/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  43%|████▎     | 1569/3683 [02:15<03:02, 11.58it/s, avg_loss=0.696, loss=0.735, step=16302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16300/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  45%|████▌     | 1669/3683 [02:24<02:54, 11.57it/s, avg_loss=0.638, loss=0.658, step=16402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16400/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  48%|████▊     | 1769/3683 [02:32<02:45, 11.59it/s, avg_loss=0.638, loss=0.619, step=16502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16500/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  51%|█████     | 1869/3683 [02:41<02:36, 11.61it/s, avg_loss=0.638, loss=0.619, step=16602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16600/40000] Avg Loss: 0.6354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  53%|█████▎    | 1969/3683 [02:50<02:27, 11.60it/s, avg_loss=0.619, loss=0.619, step=16702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16700/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  56%|█████▌    | 2069/3683 [02:58<02:19, 11.60it/s, avg_loss=0.638, loss=0.619, step=16802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16800/40000] Avg Loss: 0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  59%|█████▉    | 2169/3683 [03:07<02:10, 11.60it/s, avg_loss=0.638, loss=0.619, step=16902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16900/40000] Avg Loss: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  62%|██████▏   | 2269/3683 [03:16<02:02, 11.58it/s, avg_loss=0.658, loss=0.658, step=17002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17000/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  64%|██████▍   | 2369/3683 [03:24<01:53, 11.60it/s, avg_loss=0.638, loss=0.619, step=17102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17100/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  67%|██████▋   | 2469/3683 [03:33<01:44, 11.59it/s, avg_loss=0.638, loss=0.658, step=17202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17200/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  70%|██████▉   | 2569/3683 [03:41<01:36, 11.58it/s, avg_loss=0.619, loss=0.619, step=17302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17300/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  72%|███████▏  | 2669/3683 [03:50<01:27, 11.62it/s, avg_loss=0.638, loss=0.658, step=17402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17400/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  75%|███████▌  | 2769/3683 [03:59<01:18, 11.61it/s, avg_loss=0.619, loss=0.619, step=17502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17500/40000] Avg Loss: 0.6457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  78%|███████▊  | 2869/3683 [04:07<01:10, 11.58it/s, avg_loss=0.658, loss=0.658, step=17602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17600/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  81%|████████  | 2969/3683 [04:16<01:01, 11.61it/s, avg_loss=0.638, loss=0.658, step=17702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17700/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  83%|████████▎ | 3069/3683 [04:25<00:52, 11.59it/s, avg_loss=0.658, loss=0.696, step=17802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17800/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  86%|████████▌ | 3169/3683 [04:33<00:44, 11.59it/s, avg_loss=0.677, loss=0.658, step=17902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 17900/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  89%|████████▉ | 3269/3683 [04:42<00:35, 11.56it/s, avg_loss=0.658, loss=0.658, step=18002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18000/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  91%|█████████▏| 3369/3683 [04:51<00:27, 11.59it/s, avg_loss=0.619, loss=0.619, step=18102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18100/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  94%|█████████▍| 3469/3683 [04:59<00:18, 11.59it/s, avg_loss=0.638, loss=0.619, step=18202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18200/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  97%|█████████▋| 3569/3683 [05:08<00:09, 11.59it/s, avg_loss=0.677, loss=0.735, step=18302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18300/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|█████████▉| 3669/3683 [05:16<00:01, 11.55it/s, avg_loss=0.619, loss=0.619, step=18402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18400/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 3683/3683 [05:18<00:00, 11.58it/s, avg_loss=0.64, loss=0.619, step=18415] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:   2%|▏         | 87/3683 [00:07<05:11, 11.55it/s, avg_loss=0.619, loss=0.619, step=18502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18500/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:   5%|▌         | 187/3683 [00:16<05:00, 11.62it/s, avg_loss=0.638, loss=0.619, step=18602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18600/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:   8%|▊         | 287/3683 [00:24<04:52, 11.62it/s, avg_loss=0.658, loss=0.619, step=18702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18700/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  11%|█         | 387/3683 [00:33<04:44, 11.57it/s, avg_loss=0.619, loss=0.619, step=18802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18800/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  13%|█▎        | 487/3683 [00:42<04:36, 11.58it/s, avg_loss=0.638, loss=0.658, step=18902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 18900/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  16%|█▌        | 587/3683 [00:50<04:27, 11.59it/s, avg_loss=0.638, loss=0.658, step=19002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19000/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  19%|█▊        | 687/3683 [00:59<04:18, 11.58it/s, avg_loss=0.658, loss=0.619, step=19102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19100/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  21%|██▏       | 787/3683 [01:08<04:10, 11.55it/s, avg_loss=0.658, loss=0.658, step=19202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19200/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  24%|██▍       | 887/3683 [01:16<04:01, 11.59it/s, avg_loss=0.638, loss=0.658, step=19302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19300/40000] Avg Loss: 0.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  27%|██▋       | 987/3683 [01:25<03:52, 11.59it/s, avg_loss=0.619, loss=0.619, step=19402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19400/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  30%|██▉       | 1087/3683 [01:33<03:44, 11.57it/s, avg_loss=0.619, loss=0.619, step=19502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19500/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  32%|███▏      | 1187/3683 [01:42<03:36, 11.55it/s, avg_loss=0.658, loss=0.619, step=19602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19600/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  35%|███▍      | 1287/3683 [01:51<03:26, 11.58it/s, avg_loss=0.619, loss=0.619, step=19702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19700/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  38%|███▊      | 1387/3683 [01:59<03:18, 11.57it/s, avg_loss=0.638, loss=0.619, step=19802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19800/40000] Avg Loss: 0.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  40%|████      | 1487/3683 [02:08<03:10, 11.54it/s, avg_loss=0.638, loss=0.658, step=19902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 19900/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  43%|████▎     | 1587/3683 [02:17<03:00, 11.58it/s, avg_loss=0.619, loss=0.619, step=2e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20000/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  46%|████▌     | 1687/3683 [02:25<02:52, 11.59it/s, avg_loss=0.638, loss=0.619, step=20102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20100/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  49%|████▊     | 1787/3683 [02:34<02:43, 11.58it/s, avg_loss=0.638, loss=0.619, step=20202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20200/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  51%|█████     | 1887/3683 [02:43<02:35, 11.58it/s, avg_loss=0.619, loss=0.619, step=20302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20300/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  54%|█████▍    | 1987/3683 [02:51<02:26, 11.59it/s, avg_loss=0.638, loss=0.619, step=20402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20400/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  57%|█████▋    | 2087/3683 [03:00<02:17, 11.58it/s, avg_loss=0.638, loss=0.658, step=20502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20500/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  59%|█████▉    | 2187/3683 [03:08<02:09, 11.58it/s, avg_loss=0.619, loss=0.619, step=20602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20600/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  62%|██████▏   | 2287/3683 [03:17<02:00, 11.61it/s, avg_loss=0.619, loss=0.619, step=20702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20700/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  65%|██████▍   | 2387/3683 [03:26<01:51, 11.59it/s, avg_loss=0.619, loss=0.619, step=20802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20800/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  68%|██████▊   | 2487/3683 [03:34<01:43, 11.58it/s, avg_loss=0.677, loss=0.658, step=20902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20900/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  70%|███████   | 2587/3683 [03:43<01:34, 11.59it/s, avg_loss=0.638, loss=0.658, step=21002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21000/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  73%|███████▎  | 2687/3683 [03:52<01:25, 11.60it/s, avg_loss=0.638, loss=0.619, step=21102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21100/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  76%|███████▌  | 2787/3683 [04:00<01:17, 11.59it/s, avg_loss=0.638, loss=0.619, step=21202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21200/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  78%|███████▊  | 2887/3683 [04:09<01:08, 11.58it/s, avg_loss=0.619, loss=0.619, step=21302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21300/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  81%|████████  | 2987/3683 [04:18<01:00, 11.59it/s, avg_loss=0.619, loss=0.619, step=21402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21400/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  84%|████████▍ | 3087/3683 [04:26<00:51, 11.58it/s, avg_loss=0.619, loss=0.619, step=21502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21500/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  87%|████████▋ | 3187/3683 [04:35<00:42, 11.61it/s, avg_loss=0.619, loss=0.619, step=21602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21600/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  89%|████████▉ | 3287/3683 [04:43<00:34, 11.60it/s, avg_loss=0.696, loss=0.696, step=21702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21700/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  92%|█████████▏| 3387/3683 [04:52<00:25, 11.57it/s, avg_loss=0.619, loss=0.619, step=21802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  95%|█████████▍| 3487/3683 [05:01<00:16, 11.59it/s, avg_loss=0.638, loss=0.619, step=21902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 21900/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  97%|█████████▋| 3587/3683 [05:09<00:08, 11.58it/s, avg_loss=0.638, loss=0.619, step=22002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22000/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 3683/3683 [05:18<00:00, 11.57it/s, avg_loss=0.645, loss=0.658, step=22098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:   0%|          | 3/3683 [00:00<07:32,  8.14it/s, avg_loss=0.638, loss=0.658, step=22102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22100/40000] Avg Loss: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:   3%|▎         | 103/3683 [00:09<05:09, 11.56it/s, avg_loss=0.638, loss=0.619, step=22202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22200/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:   6%|▌         | 203/3683 [00:17<05:01, 11.53it/s, avg_loss=0.619, loss=0.619, step=22302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22300/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:   8%|▊         | 303/3683 [00:26<04:53, 11.53it/s, avg_loss=0.619, loss=0.619, step=22402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22400/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  11%|█         | 403/3683 [00:35<04:44, 11.54it/s, avg_loss=0.619, loss=0.619, step=22502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22500/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  14%|█▎        | 503/3683 [00:43<04:35, 11.54it/s, avg_loss=0.619, loss=0.619, step=22602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22600/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  16%|█▋        | 603/3683 [00:52<04:26, 11.56it/s, avg_loss=0.638, loss=0.619, step=22702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22700/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  19%|█▉        | 703/3683 [01:01<04:17, 11.56it/s, avg_loss=0.677, loss=0.658, step=22802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  22%|██▏       | 803/3683 [01:09<04:08, 11.58it/s, avg_loss=0.619, loss=0.619, step=22902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 22900/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  25%|██▍       | 903/3683 [01:18<04:00, 11.54it/s, avg_loss=0.619, loss=0.619, step=23002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23000/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  27%|██▋       | 1003/3683 [01:27<03:51, 11.57it/s, avg_loss=0.658, loss=0.619, step=23102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23100/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  30%|██▉       | 1103/3683 [01:35<03:43, 11.56it/s, avg_loss=0.658, loss=0.696, step=23202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23200/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  33%|███▎      | 1203/3683 [01:44<03:34, 11.54it/s, avg_loss=0.638, loss=0.619, step=23302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23300/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  35%|███▌      | 1303/3683 [01:53<03:25, 11.55it/s, avg_loss=0.619, loss=0.619, step=23402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23400/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  38%|███▊      | 1403/3683 [02:01<03:17, 11.56it/s, avg_loss=0.658, loss=0.619, step=23502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23500/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  41%|████      | 1503/3683 [02:10<03:08, 11.59it/s, avg_loss=0.638, loss=0.619, step=23602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23600/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  44%|████▎     | 1603/3683 [02:18<03:00, 11.55it/s, avg_loss=0.619, loss=0.619, step=23702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23700/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  46%|████▌     | 1703/3683 [02:27<02:51, 11.55it/s, avg_loss=0.619, loss=0.619, step=23802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23800/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  49%|████▉     | 1803/3683 [02:36<02:42, 11.58it/s, avg_loss=0.638, loss=0.658, step=23902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 23900/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  52%|█████▏    | 1903/3683 [02:44<02:33, 11.59it/s, avg_loss=0.619, loss=0.619, step=24002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24000/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  54%|█████▍    | 2003/3683 [02:53<02:25, 11.53it/s, avg_loss=0.638, loss=0.619, step=24102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24100/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  57%|█████▋    | 2103/3683 [03:02<02:16, 11.57it/s, avg_loss=0.638, loss=0.619, step=24202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24200/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  60%|█████▉    | 2203/3683 [03:10<02:07, 11.57it/s, avg_loss=0.658, loss=0.658, step=24302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24300/40000] Avg Loss: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  63%|██████▎   | 2303/3683 [03:19<01:59, 11.54it/s, avg_loss=0.677, loss=0.658, step=24402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24400/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  65%|██████▌   | 2403/3683 [03:28<01:50, 11.60it/s, avg_loss=0.658, loss=0.658, step=24502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24500/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  68%|██████▊   | 2503/3683 [03:36<01:41, 11.62it/s, avg_loss=0.638, loss=0.619, step=24602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24600/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  71%|███████   | 2603/3683 [03:45<01:32, 11.62it/s, avg_loss=0.619, loss=0.619, step=24702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24700/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  73%|███████▎  | 2703/3683 [03:54<01:24, 11.60it/s, avg_loss=0.638, loss=0.619, step=24802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24800/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  76%|███████▌  | 2803/3683 [04:02<01:15, 11.60it/s, avg_loss=0.658, loss=0.658, step=24902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 24900/40000] Avg Loss: 0.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  79%|███████▉  | 2903/3683 [04:11<01:07, 11.61it/s, avg_loss=0.638, loss=0.658, step=25002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25000/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  82%|████████▏ | 3003/3683 [04:19<00:58, 11.59it/s, avg_loss=0.619, loss=0.619, step=25102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25100/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  84%|████████▍ | 3103/3683 [04:28<00:49, 11.60it/s, avg_loss=0.658, loss=0.658, step=25202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25200/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  87%|████████▋ | 3203/3683 [04:37<00:41, 11.61it/s, avg_loss=0.638, loss=0.619, step=25302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25300/40000] Avg Loss: 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  90%|████████▉ | 3303/3683 [04:45<00:32, 11.63it/s, avg_loss=0.658, loss=0.696, step=25402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25400/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  92%|█████████▏| 3403/3683 [04:54<00:24, 11.61it/s, avg_loss=0.619, loss=0.619, step=25502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25500/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  95%|█████████▌| 3503/3683 [05:02<00:15, 11.62it/s, avg_loss=0.619, loss=0.619, step=25602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25600/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  98%|█████████▊| 3603/3683 [05:11<00:06, 11.60it/s, avg_loss=0.658, loss=0.619, step=25702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25700/40000] Avg Loss: 0.6330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 3683/3683 [05:18<00:00, 11.57it/s, avg_loss=0.642, loss=0.619, step=25781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   1%|          | 21/3683 [00:01<05:18, 11.51it/s, avg_loss=0.619, loss=0.619, step=25802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25800/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   3%|▎         | 121/3683 [00:10<05:06, 11.64it/s, avg_loss=0.638, loss=0.658, step=25902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25900/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   6%|▌         | 221/3683 [00:19<04:57, 11.64it/s, avg_loss=0.658, loss=0.658, step=26002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26000/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   9%|▊         | 321/3683 [00:27<04:48, 11.64it/s, avg_loss=0.658, loss=0.658, step=26102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26100/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  11%|█▏        | 421/3683 [00:36<04:40, 11.63it/s, avg_loss=0.638, loss=0.658, step=26202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26200/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  14%|█▍        | 521/3683 [00:44<04:32, 11.61it/s, avg_loss=0.619, loss=0.619, step=26302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26300/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  17%|█▋        | 621/3683 [00:53<04:23, 11.62it/s, avg_loss=0.658, loss=0.658, step=26402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26400/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  20%|█▉        | 721/3683 [01:02<04:14, 11.65it/s, avg_loss=0.638, loss=0.658, step=26502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26500/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  22%|██▏       | 821/3683 [01:10<04:05, 11.64it/s, avg_loss=0.677, loss=0.696, step=26602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26600/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  25%|██▌       | 921/3683 [01:19<03:58, 11.60it/s, avg_loss=0.619, loss=0.619, step=26702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26700/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  28%|██▊       | 1021/3683 [01:27<03:49, 11.61it/s, avg_loss=0.638, loss=0.658, step=26802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26800/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  30%|███       | 1121/3683 [01:36<03:40, 11.63it/s, avg_loss=0.638, loss=0.658, step=26902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26900/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  33%|███▎      | 1221/3683 [01:45<03:31, 11.62it/s, avg_loss=0.658, loss=0.619, step=27002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27000/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  36%|███▌      | 1321/3683 [01:53<03:23, 11.62it/s, avg_loss=0.638, loss=0.619, step=27102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27100/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  39%|███▊      | 1421/3683 [02:02<03:14, 11.64it/s, avg_loss=0.619, loss=0.619, step=27202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27200/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  41%|████▏     | 1521/3683 [02:10<03:05, 11.64it/s, avg_loss=0.638, loss=0.619, step=27302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27300/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  44%|████▍     | 1621/3683 [02:19<02:57, 11.62it/s, avg_loss=0.619, loss=0.619, step=27402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27400/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  47%|████▋     | 1721/3683 [02:28<02:48, 11.62it/s, avg_loss=0.638, loss=0.619, step=27502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27500/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  49%|████▉     | 1821/3683 [02:36<02:40, 11.61it/s, avg_loss=0.658, loss=0.658, step=27602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27600/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  52%|█████▏    | 1921/3683 [02:45<02:31, 11.65it/s, avg_loss=0.619, loss=0.619, step=27702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27700/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  55%|█████▍    | 2021/3683 [02:53<02:23, 11.60it/s, avg_loss=0.677, loss=0.696, step=27802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27800/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  58%|█████▊    | 2121/3683 [03:02<02:14, 11.63it/s, avg_loss=0.658, loss=0.658, step=27902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 27900/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  60%|██████    | 2221/3683 [03:11<02:05, 11.63it/s, avg_loss=0.658, loss=0.658, step=28002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28000/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  63%|██████▎   | 2321/3683 [03:19<01:57, 11.63it/s, avg_loss=0.658, loss=0.658, step=28102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28100/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  66%|██████▌   | 2421/3683 [03:28<01:48, 11.61it/s, avg_loss=0.677, loss=0.619, step=28202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28200/40000] Avg Loss: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  68%|██████▊   | 2521/3683 [03:37<01:39, 11.62it/s, avg_loss=0.619, loss=0.619, step=28302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28300/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  71%|███████   | 2621/3683 [03:45<01:31, 11.62it/s, avg_loss=0.638, loss=0.619, step=28402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28400/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  74%|███████▍  | 2721/3683 [03:54<01:22, 11.62it/s, avg_loss=0.619, loss=0.619, step=28502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28500/40000] Avg Loss: 0.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  77%|███████▋  | 2821/3683 [04:02<01:14, 11.63it/s, avg_loss=0.619, loss=0.619, step=28602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28600/40000] Avg Loss: 0.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  79%|███████▉  | 2921/3683 [04:11<01:05, 11.64it/s, avg_loss=0.619, loss=0.619, step=28702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28700/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  82%|████████▏ | 3021/3683 [04:20<00:57, 11.50it/s, avg_loss=0.619, loss=0.619, step=28802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28800/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  85%|████████▍ | 3121/3683 [04:28<00:48, 11.64it/s, avg_loss=0.638, loss=0.619, step=28902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 28900/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  87%|████████▋ | 3221/3683 [04:37<00:39, 11.61it/s, avg_loss=0.638, loss=0.658, step=29002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29000/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  90%|█████████ | 3321/3683 [04:45<00:31, 11.64it/s, avg_loss=0.658, loss=0.619, step=29102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29100/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  93%|█████████▎| 3421/3683 [04:54<00:22, 11.63it/s, avg_loss=0.619, loss=0.619, step=29202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29200/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  96%|█████████▌| 3521/3683 [05:03<00:13, 11.61it/s, avg_loss=0.619, loss=0.619, step=29302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29300/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  98%|█████████▊| 3621/3683 [05:11<00:05, 11.63it/s, avg_loss=0.638, loss=0.619, step=29402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29400/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 3683/3683 [05:17<00:00, 11.62it/s, avg_loss=0.643, loss=0.619, step=29464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:   1%|          | 37/3683 [00:03<05:14, 11.58it/s, avg_loss=0.638, loss=0.619, step=29502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29500/40000] Avg Loss: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:   4%|▎         | 137/3683 [00:12<05:05, 11.59it/s, avg_loss=0.638, loss=0.619, step=29602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29600/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:   6%|▋         | 237/3683 [00:20<04:56, 11.62it/s, avg_loss=0.619, loss=0.619, step=29702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29700/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:   9%|▉         | 337/3683 [00:29<04:48, 11.60it/s, avg_loss=0.677, loss=0.696, step=29802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29800/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  12%|█▏        | 437/3683 [00:37<04:39, 11.63it/s, avg_loss=0.638, loss=0.658, step=29902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 29900/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  15%|█▍        | 537/3683 [00:46<04:30, 11.62it/s, avg_loss=0.677, loss=0.658, step=3e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30000/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  17%|█▋        | 637/3683 [00:55<04:22, 11.59it/s, avg_loss=0.638, loss=0.658, step=30102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30100/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  20%|██        | 737/3683 [01:03<04:13, 11.61it/s, avg_loss=0.658, loss=0.658, step=30202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30200/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  23%|██▎       | 837/3683 [01:12<04:05, 11.61it/s, avg_loss=0.658, loss=0.696, step=30302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30300/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  25%|██▌       | 937/3683 [01:20<03:56, 11.62it/s, avg_loss=0.638, loss=0.658, step=30402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30400/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  28%|██▊       | 1037/3683 [01:29<03:47, 11.61it/s, avg_loss=0.638, loss=0.658, step=30502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30500/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  31%|███       | 1137/3683 [01:38<03:39, 11.59it/s, avg_loss=0.638, loss=0.658, step=30602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30600/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  34%|███▎      | 1237/3683 [01:46<03:30, 11.63it/s, avg_loss=0.638, loss=0.658, step=30702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30700/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  36%|███▋      | 1337/3683 [01:55<03:22, 11.61it/s, avg_loss=0.619, loss=0.619, step=30802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30800/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  39%|███▉      | 1437/3683 [02:04<03:13, 11.61it/s, avg_loss=0.638, loss=0.658, step=30902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30900/40000] Avg Loss: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  42%|████▏     | 1537/3683 [02:12<03:04, 11.62it/s, avg_loss=0.638, loss=0.619, step=31002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31000/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  44%|████▍     | 1637/3683 [02:21<02:56, 11.59it/s, avg_loss=0.658, loss=0.658, step=31102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31100/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  47%|████▋     | 1737/3683 [02:29<02:47, 11.62it/s, avg_loss=0.658, loss=0.696, step=31202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31200/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  50%|████▉     | 1837/3683 [02:38<02:39, 11.60it/s, avg_loss=0.619, loss=0.619, step=31302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31300/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  53%|█████▎    | 1937/3683 [02:47<02:30, 11.61it/s, avg_loss=0.619, loss=0.619, step=31402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31400/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  55%|█████▌    | 2037/3683 [02:55<02:21, 11.61it/s, avg_loss=0.638, loss=0.619, step=31502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31500/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  58%|█████▊    | 2137/3683 [03:04<02:13, 11.60it/s, avg_loss=0.658, loss=0.658, step=31602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31600/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  61%|██████    | 2237/3683 [03:12<02:04, 11.58it/s, avg_loss=0.638, loss=0.658, step=31702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31700/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  63%|██████▎   | 2337/3683 [03:21<01:55, 11.62it/s, avg_loss=0.638, loss=0.619, step=31802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31800/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  66%|██████▌   | 2437/3683 [03:30<01:47, 11.60it/s, avg_loss=0.638, loss=0.658, step=31902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 31900/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  69%|██████▉   | 2537/3683 [03:38<01:38, 11.60it/s, avg_loss=0.619, loss=0.619, step=32002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32000/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  72%|███████▏  | 2637/3683 [03:47<01:30, 11.61it/s, avg_loss=0.658, loss=0.658, step=32102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32100/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  74%|███████▍  | 2737/3683 [03:56<01:21, 11.58it/s, avg_loss=0.619, loss=0.619, step=32202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32200/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  77%|███████▋  | 2837/3683 [04:04<01:12, 11.62it/s, avg_loss=0.677, loss=0.696, step=32302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32300/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  80%|███████▉  | 2937/3683 [04:13<01:04, 11.61it/s, avg_loss=0.619, loss=0.619, step=32402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32400/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  82%|████████▏ | 3037/3683 [04:21<00:55, 11.61it/s, avg_loss=0.619, loss=0.619, step=32502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32500/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  85%|████████▌ | 3137/3683 [04:30<00:47, 11.59it/s, avg_loss=0.638, loss=0.658, step=32602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32600/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  88%|████████▊ | 3237/3683 [04:39<00:38, 11.60it/s, avg_loss=0.619, loss=0.619, step=32702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32700/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  91%|█████████ | 3337/3683 [04:47<00:29, 11.62it/s, avg_loss=0.638, loss=0.658, step=32802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32800/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  93%|█████████▎| 3437/3683 [04:56<00:21, 11.61it/s, avg_loss=0.619, loss=0.619, step=32902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 32900/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  96%|█████████▌| 3537/3683 [05:05<00:12, 11.58it/s, avg_loss=0.619, loss=0.619, step=33002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33000/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  99%|█████████▉| 3637/3683 [05:13<00:03, 11.60it/s, avg_loss=0.638, loss=0.619, step=33102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33100/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 3683/3683 [05:17<00:00, 11.60it/s, avg_loss=0.632, loss=0.619, step=33147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:   1%|▏         | 55/3683 [00:04<05:12, 11.61it/s, avg_loss=0.658, loss=0.658, step=33202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33200/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:   4%|▍         | 155/3683 [00:13<05:04, 11.59it/s, avg_loss=0.638, loss=0.658, step=33302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33300/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:   7%|▋         | 255/3683 [00:22<04:55, 11.61it/s, avg_loss=0.638, loss=0.658, step=33402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33400/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  10%|▉         | 355/3683 [00:30<04:46, 11.61it/s, avg_loss=0.638, loss=0.619, step=33502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33500/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  12%|█▏        | 455/3683 [00:39<04:38, 11.60it/s, avg_loss=0.619, loss=0.619, step=33602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33600/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  15%|█▌        | 555/3683 [00:47<04:29, 11.61it/s, avg_loss=0.619, loss=0.619, step=33702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33700/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  18%|█▊        | 655/3683 [00:56<04:21, 11.59it/s, avg_loss=0.638, loss=0.658, step=33802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33800/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  20%|██        | 755/3683 [01:05<04:13, 11.56it/s, avg_loss=0.619, loss=0.619, step=33902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 33900/40000] Avg Loss: 0.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  23%|██▎       | 855/3683 [01:13<04:03, 11.62it/s, avg_loss=0.619, loss=0.619, step=34002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34000/40000] Avg Loss: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  26%|██▌       | 955/3683 [01:22<03:55, 11.61it/s, avg_loss=0.638, loss=0.619, step=34102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34100/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  29%|██▊       | 1055/3683 [01:31<03:45, 11.63it/s, avg_loss=0.619, loss=0.619, step=34202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34200/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  31%|███▏      | 1155/3683 [01:39<03:37, 11.61it/s, avg_loss=0.638, loss=0.658, step=34302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34300/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  34%|███▍      | 1255/3683 [01:48<03:29, 11.60it/s, avg_loss=0.677, loss=0.735, step=34402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34400/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  37%|███▋      | 1355/3683 [01:56<03:20, 11.61it/s, avg_loss=0.677, loss=0.658, step=34502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34500/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  40%|███▉      | 1455/3683 [02:05<03:12, 11.57it/s, avg_loss=0.638, loss=0.619, step=34602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34600/40000] Avg Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  42%|████▏     | 1555/3683 [02:14<03:03, 11.62it/s, avg_loss=0.658, loss=0.619, step=34702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34700/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  45%|████▍     | 1655/3683 [02:22<02:54, 11.61it/s, avg_loss=0.619, loss=0.619, step=34802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34800/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  48%|████▊     | 1755/3683 [02:31<02:46, 11.58it/s, avg_loss=0.638, loss=0.619, step=34902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 34900/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  50%|█████     | 1855/3683 [02:40<02:37, 11.59it/s, avg_loss=0.619, loss=0.619, step=35002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35000/40000] Avg Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  53%|█████▎    | 1955/3683 [02:48<02:28, 11.60it/s, avg_loss=0.638, loss=0.619, step=35102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35100/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  56%|█████▌    | 2055/3683 [02:57<02:20, 11.57it/s, avg_loss=0.658, loss=0.658, step=35202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35200/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  59%|█████▊    | 2155/3683 [03:05<02:11, 11.60it/s, avg_loss=0.658, loss=0.619, step=35302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35300/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  61%|██████    | 2255/3683 [03:14<02:03, 11.61it/s, avg_loss=0.658, loss=0.658, step=35402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35400/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  64%|██████▍   | 2355/3683 [03:23<01:54, 11.61it/s, avg_loss=0.619, loss=0.619, step=35502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35500/40000] Avg Loss: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  67%|██████▋   | 2455/3683 [03:31<01:45, 11.60it/s, avg_loss=0.619, loss=0.619, step=35602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35600/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  69%|██████▉   | 2555/3683 [03:40<01:37, 11.59it/s, avg_loss=0.638, loss=0.658, step=35702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35700/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  72%|███████▏  | 2655/3683 [03:49<01:28, 11.60it/s, avg_loss=0.638, loss=0.658, step=35802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35800/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  75%|███████▍  | 2755/3683 [03:57<01:19, 11.60it/s, avg_loss=0.638, loss=0.658, step=35902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 35900/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  78%|███████▊  | 2855/3683 [04:06<01:11, 11.62it/s, avg_loss=0.619, loss=0.619, step=36002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36000/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  80%|████████  | 2955/3683 [04:14<01:02, 11.61it/s, avg_loss=0.658, loss=0.658, step=36102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36100/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  83%|████████▎ | 3055/3683 [04:23<00:54, 11.58it/s, avg_loss=0.658, loss=0.696, step=36202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36200/40000] Avg Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  86%|████████▌ | 3155/3683 [04:32<00:45, 11.60it/s, avg_loss=0.638, loss=0.619, step=36302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36300/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  88%|████████▊ | 3255/3683 [04:40<00:36, 11.60it/s, avg_loss=0.619, loss=0.619, step=36402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36400/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  91%|█████████ | 3355/3683 [04:49<00:28, 11.58it/s, avg_loss=0.658, loss=0.658, step=36502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36500/40000] Avg Loss: 0.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  94%|█████████▍| 3455/3683 [04:58<00:19, 11.62it/s, avg_loss=0.658, loss=0.658, step=36602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36600/40000] Avg Loss: 0.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  97%|█████████▋| 3555/3683 [05:06<00:11, 11.58it/s, avg_loss=0.638, loss=0.658, step=36702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36700/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  99%|█████████▉| 3655/3683 [05:15<00:02, 11.60it/s, avg_loss=0.638, loss=0.658, step=36802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36800/40000] Avg Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 3683/3683 [05:17<00:00, 11.59it/s, avg_loss=0.641, loss=0.658, step=36830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:   2%|▏         | 71/3683 [00:06<05:10, 11.64it/s, avg_loss=0.658, loss=0.658, step=36902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 36900/40000] Avg Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:   5%|▍         | 171/3683 [00:14<05:02, 11.63it/s, avg_loss=0.658, loss=0.658, step=37002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37000/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:   7%|▋         | 271/3683 [00:23<04:53, 11.62it/s, avg_loss=0.638, loss=0.619, step=37102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37100/40000] Avg Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  10%|█         | 371/3683 [00:32<04:44, 11.64it/s, avg_loss=0.677, loss=0.658, step=37202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37200/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  13%|█▎        | 471/3683 [00:40<04:36, 11.62it/s, avg_loss=0.619, loss=0.619, step=37302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37300/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  16%|█▌        | 571/3683 [00:49<04:28, 11.57it/s, avg_loss=0.638, loss=0.658, step=37402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37400/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  18%|█▊        | 671/3683 [00:57<04:18, 11.63it/s, avg_loss=0.638, loss=0.619, step=37502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37500/40000] Avg Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  21%|██        | 771/3683 [01:06<04:10, 11.63it/s, avg_loss=0.638, loss=0.658, step=37602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37600/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  24%|██▎       | 871/3683 [01:15<04:01, 11.63it/s, avg_loss=0.638, loss=0.658, step=37702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37700/40000] Avg Loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  26%|██▋       | 971/3683 [01:23<03:53, 11.62it/s, avg_loss=0.658, loss=0.619, step=37802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37800/40000] Avg Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  29%|██▉       | 1071/3683 [01:32<03:44, 11.63it/s, avg_loss=0.638, loss=0.619, step=37902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 37900/40000] Avg Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  32%|███▏      | 1171/3683 [01:40<03:35, 11.63it/s, avg_loss=0.638, loss=0.619, step=38002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38000/40000] Avg Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  35%|███▍      | 1271/3683 [01:49<03:27, 11.63it/s, avg_loss=0.638, loss=0.619, step=38102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38100/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  37%|███▋      | 1371/3683 [01:58<03:18, 11.64it/s, avg_loss=0.658, loss=0.696, step=38202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38200/40000] Avg Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  40%|███▉      | 1471/3683 [02:06<03:10, 11.62it/s, avg_loss=0.638, loss=0.619, step=38302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38300/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  43%|████▎     | 1571/3683 [02:15<03:01, 11.64it/s, avg_loss=0.619, loss=0.619, step=38402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38400/40000] Avg Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  45%|████▌     | 1671/3683 [02:24<02:53, 11.63it/s, avg_loss=0.619, loss=0.619, step=38502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38500/40000] Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  48%|████▊     | 1771/3683 [02:32<02:44, 11.64it/s, avg_loss=0.638, loss=0.619, step=38602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38600/40000] Avg Loss: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  51%|█████     | 1871/3683 [02:41<02:35, 11.62it/s, avg_loss=0.619, loss=0.619, step=38702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38700/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  54%|█████▎    | 1971/3683 [02:49<02:27, 11.60it/s, avg_loss=0.638, loss=0.658, step=38802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38800/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  56%|█████▌    | 2071/3683 [02:58<02:18, 11.63it/s, avg_loss=0.658, loss=0.658, step=38902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 38900/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  59%|█████▉    | 2171/3683 [03:07<02:10, 11.62it/s, avg_loss=0.677, loss=0.619, step=39002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39000/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  62%|██████▏   | 2271/3683 [03:15<02:01, 11.59it/s, avg_loss=0.619, loss=0.619, step=39102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39100/40000] Avg Loss: 0.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  64%|██████▍   | 2371/3683 [03:24<01:52, 11.62it/s, avg_loss=0.638, loss=0.658, step=39202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39200/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  67%|██████▋   | 2471/3683 [03:32<01:44, 11.64it/s, avg_loss=0.658, loss=0.658, step=39302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39300/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  70%|██████▉   | 2571/3683 [03:41<01:35, 11.60it/s, avg_loss=0.638, loss=0.658, step=39402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39400/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  73%|███████▎  | 2671/3683 [03:50<01:27, 11.60it/s, avg_loss=0.658, loss=0.658, step=39502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39500/40000] Avg Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  75%|███████▌  | 2771/3683 [03:58<01:18, 11.61it/s, avg_loss=0.658, loss=0.696, step=39602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39600/40000] Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  78%|███████▊  | 2871/3683 [04:07<01:10, 11.60it/s, avg_loss=0.619, loss=0.619, step=39702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39700/40000] Avg Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  81%|████████  | 2971/3683 [04:15<01:01, 11.64it/s, avg_loss=0.619, loss=0.619, step=39802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39800/40000] Avg Loss: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  83%|████████▎ | 3071/3683 [04:24<00:52, 11.63it/s, avg_loss=0.658, loss=0.619, step=39902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 39900/40000] Avg Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  86%|████████▌ | 3170/3683 [04:32<00:44, 11.61it/s, avg_loss=63.8, loss=0.658, step=4e+4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 40000/40000] Avg Loss: 0.6384\n",
      "Model saved to 'llama_su_pretrained.pt'\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# -------------------------------\n",
    "# INITIALIZATION\n",
    "# -------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "# Load tokenizer and ensure it has all required special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# Add special tokens if missing\n",
    "special_tokens = {}\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens[\"pad_token\"] = \"[PAD]\"\n",
    "if tokenizer.sep_token is None:\n",
    "    special_tokens[\"sep_token\"] = \"[SEP]\"\n",
    "\n",
    "if special_tokens:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Initialize model\n",
    "model = SepClassifier()\n",
    "for param in model.llama.parameters():  \n",
    "    param.requires_grad = False\n",
    "\n",
    "# Resize token embeddings to match new tokenizer length\n",
    "model.llama.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Setup DataLoader\n",
    "# train_loader = DataLoader(\n",
    "#     dataset[\"train\"],\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,  # increase if you're using CPU-heavy preprocessing\n",
    "    pin_memory=True  # if using GPU\n",
    ")\n",
    "\n",
    "\n",
    "# Optimizer, scheduler, loss\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "total_steps = min(10000, len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(next(model.parameters()).device)\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler(device='cuda')  # <-- add before your loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Training Epoch {epoch}\") as t_loader:\n",
    "        for inputs, label_lists, sep_lists in t_loader:\n",
    "            if step >= MAX_STEPS:\n",
    "                break\n",
    "\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            sep_lists = [s.to(device) for s in sep_lists]\n",
    "            flat_labels = torch.cat(label_lists).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(**inputs, sep_positions=sep_lists)\n",
    "                loss = loss_fn(logits, flat_labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            step += 1\n",
    "\n",
    "            t_loader.set_postfix(step=step, loss=loss.item(), avg_loss=running_loss / max(1, step % 100))\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[Step {step:4d}/{MAX_STEPS}] Avg Loss: {running_loss / 100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    if step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "# ------------------------\n",
    "# SAVE MODEL\n",
    "# ------------------------\n",
    "torch.save(model.state_dict(), \"llama_su_pretrained.pt\")\n",
    "print(\"Model saved to 'llama_su_pretrained.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "521faa58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SepClassifier' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_su_pretrained.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SepClassifier' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "torch.save(model.bert.state_dict(), \"llama_su_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459a3fb",
   "metadata": {},
   "source": [
    "Validation & early-stop (optional)\n",
    "\n",
    "- Use the same DataLoader/loop on su_ds[\"validation\"], compute average BCE loss; if it plateaus you can stop earlier than 5 k steps (what the authors mean by “until train loss converged”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b890a7",
   "metadata": {},
   "source": [
    "# Create Summarization Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3b258",
   "metadata": {},
   "source": [
    "ขั้นตอนการทำ preprocess\n",
    "1. โหลดชุดข้อมูล SAMSum\n",
    "2. ทำ preprocessing:\n",
    "    - แทนชื่อ speaker ด้วย [S1]–[S10]\n",
    "    - เติม [SEP] ท้ายทุกประโยค\n",
    "    - ใช้ tokenizer เดิมจาก pretraining (tokenizer_samsum_su)\n",
    "    - truncate/pad ความยาวที่ max_length = 512\n",
    "3. แปลงให้อยู่ในรูปแบบที่พร้อมใช้สำหรับ Seq2SeqTrainer\n",
    "4. Save เป็นไฟล์ .pt หรือ DatasetDict ที่พร้อมใช้งาน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ae8f1",
   "metadata": {},
   "source": [
    "Load SAMSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d58290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e39b4",
   "metadata": {},
   "source": [
    "Load Pretrained Tokenizer (same as used during pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaf5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7f9b0",
   "metadata": {},
   "source": [
    "Speaker Normalization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Replace speaker names with generic [S1], [S2], ... tokens.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        name_token = speaker_map.get(name, \"[SUNK]\")\n",
    "        new_lines.append(f\"{name_token}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n",
    "\n",
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c45323",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(example):\n",
    "    normed_dialogue, _ = map_speakers(example[\"dialogue\"])\n",
    "    sep_dialogue = add_sep_every_utt(normed_dialogue)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        sep_dialogue,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": targets[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb81812",
   "metadata": {},
   "source": [
    "Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebaf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:08<00:00, 1769.02 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 1693.37 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 1804.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 327887.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 172195.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 167281.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to 'samsum_finetune_ready'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = raw_ds.map(preprocess_fn, batched=False)\n",
    "tokenized_ds.save_to_disk(\"samsum_finetune_ready\")\n",
    "print(\"Preprocessed dataset saved to 'samsum_finetune_ready'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Fine-tuning train: 100%|██████████| 14732/14732 [00:09<00:00, 1561.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 183818.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 129347.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 126710.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MAX_LEN = 512  # paper setting\n",
    "\n",
    "# def preprocess_example(example, split):\n",
    "#     # a) replace speakers & add SEP (same as pretraining)\n",
    "#     dlg, _ = map_speakers(example[\"dialogue\"])  # แปลงชื่อให้เป็น token สั้น ๆ เช่น <USR1>\n",
    "#     dlg = add_sep_every_utt(dlg)                # เพิ่ม [SEP] ทุกท้ายประโยค\n",
    "\n",
    "#     # b) tokenize dialogue input\n",
    "#     enc = tok_base(dlg,\n",
    "#               truncation=True,\n",
    "#               max_length=MAX_LEN,\n",
    "#               padding=\"max_length\")\n",
    "\n",
    "#     # c) tok_baseenize target summary\n",
    "#     with tok_base.as_target_tokenizer():\n",
    "#         summary = example[\"summary\"]\n",
    "#         summary_enc = tok_base(summary,\n",
    "#                           truncation=True,\n",
    "#                           max_length=MAX_LEN,\n",
    "#                           padding=\"max_length\")\n",
    "    \n",
    "#     # d) pack input and label\n",
    "#     enc[\"labels\"] = summary_enc[\"input_ids\"]\n",
    "#     return enc\n",
    "\n",
    "# # สร้าง dataset ใหม่สำหรับ fine-tune\n",
    "# finetune_ds = DatasetDict()\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     finetune_ds[split] = raw_ds[split].map(\n",
    "#         preprocess_example,\n",
    "#         fn_kwargs={\"split\": split},\n",
    "#         remove_columns=raw_ds[split].column_names,\n",
    "#         desc=f\"Building Fine-tuning {split}\"\n",
    "#     )\n",
    "\n",
    "# finetune_ds.save_to_disk(\"data/samsum_finetune\")\n",
    "# print(finetune_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e9327",
   "metadata": {},
   "source": [
    "# Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a197c0",
   "metadata": {},
   "source": [
    "\n",
    "**เทียบกับ Paper**\n",
    "\n",
    "| **Parameter**       | **Code**                          | **Paper (Section 3.2)**          | **เหมือน / ไม่เหมือน**        |\n",
    "| ------------------- | --------------------------------- | -------------------------------- | --------------       |\n",
    "| Model               | BERT2BERT (EncoderDecoderModel)   | BERT2BERT                        | เหมือน                 |\n",
    "| Tokenizer           | bert-base-uncased + custom tokens | ใช้ tokenizer ดัดแปลง              | เหมือน              |\n",
    "| Batch Size          | 8                                 | **16 (per step)**                | ไม่เหมือน → เล็กกว่า  |\n",
    "| Epochs              | 3                                 | 3                                | เหมือน              |\n",
    "| Learning Rate       | 5e-5                              | **3e-5**                         | ไม่เหมือน → สูงกว่า   |\n",
    "| Warmup Steps        | 500                               | ใช้ scheduler (แต่ไม่ระบุ exact)     | เหมือน (สมเหตุสมผล) |\n",
    "| Max Length (input)  | 512                               | 512                              | เหมือน              |\n",
    "| Max Length (output) | 128                               | 128                              | เหมือน              |\n",
    "| Beam Search         | 4                                 | 4                                | เหมือน              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3beebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/5526 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "  9%|▉         | 500/5526 [01:36<16:14,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4418, 'grad_norm': 0.3646565079689026, 'learning_rate': 4.96e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  9%|▉         | 501/5526 [01:41<2:22:17,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22444657981395721, 'eval_runtime': 5.0061, 'eval_samples_per_second': 163.402, 'eval_steps_per_second': 20.575, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1000/5526 [03:17<14:36,  5.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2172, 'grad_norm': 0.4135509133338928, 'learning_rate': 4.506565857540788e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 18%|█▊        | 1000/5526 [03:22<14:36,  5.17it/s]/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19904808700084686, 'eval_runtime': 5.0413, 'eval_samples_per_second': 162.259, 'eval_steps_per_second': 20.431, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 27%|██▋       | 1500/5526 [05:00<12:53,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2008, 'grad_norm': 0.274005264043808, 'learning_rate': 4.009152407481098e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 27%|██▋       | 1501/5526 [05:06<1:53:34,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1895398199558258, 'eval_runtime': 4.9978, 'eval_samples_per_second': 163.672, 'eval_steps_per_second': 20.609, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2000/5526 [06:42<11:18,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1864, 'grad_norm': 0.31300443410873413, 'learning_rate': 3.511738957421409e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 2000/5526 [06:47<11:18,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18359985947608948, 'eval_runtime': 4.9794, 'eval_samples_per_second': 164.278, 'eval_steps_per_second': 20.685, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 45%|████▌     | 2500/5526 [08:25<09:41,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17, 'grad_norm': 0.36899664998054504, 'learning_rate': 3.0143255073617192e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▌     | 2501/5526 [08:30<1:25:32,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17844413220882416, 'eval_runtime': 5.0089, 'eval_samples_per_second': 163.311, 'eval_steps_per_second': 20.564, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3000/5526 [10:06<08:05,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1689, 'grad_norm': 0.43778106570243835, 'learning_rate': 2.5169120573020293e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 54%|█████▍    | 3000/5526 [10:11<08:05,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17464406788349152, 'eval_runtime': 4.984, 'eval_samples_per_second': 164.126, 'eval_steps_per_second': 20.666, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 63%|██████▎   | 3500/5526 [11:50<06:25,  5.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1632, 'grad_norm': 0.3436298966407776, 'learning_rate': 2.01949860724234e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|██████▎   | 3501/5526 [11:55<56:57,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17154935002326965, 'eval_runtime': 4.9824, 'eval_samples_per_second': 164.179, 'eval_steps_per_second': 20.673, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 4000/5526 [13:31<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1487, 'grad_norm': 0.4152975380420685, 'learning_rate': 1.5220851571826503e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 4000/5526 [13:36<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17133557796478271, 'eval_runtime': 4.98, 'eval_samples_per_second': 164.259, 'eval_steps_per_second': 20.683, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 81%|████████▏ | 4500/5526 [15:14<03:17,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1356, 'grad_norm': 0.35845550894737244, 'learning_rate': 1.0246717071229607e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 81%|████████▏ | 4501/5526 [15:19<28:52,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1692614108324051, 'eval_runtime': 4.9857, 'eval_samples_per_second': 164.068, 'eval_steps_per_second': 20.659, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 5000/5526 [16:55<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1402, 'grad_norm': 0.3714558780193329, 'learning_rate': 5.27258257063271e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 5000/5526 [17:00<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1681322604417801, 'eval_runtime': 4.9793, 'eval_samples_per_second': 164.281, 'eval_steps_per_second': 20.686, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "100%|█████████▉| 5500/5526 [18:38<00:04,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1387, 'grad_norm': 0.4560355544090271, 'learning_rate': 2.984480700358138e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|█████████▉| 5501/5526 [18:43<00:42,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1668214201927185, 'eval_runtime': 4.9802, 'eval_samples_per_second': 164.251, 'eval_steps_per_second': 20.682, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5526/5526 [18:50<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1130.5819, 'train_samples_per_second': 39.091, 'train_steps_per_second': 4.888, 'train_loss': 0.28212739849194124, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su_finetune/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su_finetune/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su_finetune/vocab.txt',\n",
       " 'tokenizer_samsum_su_finetune/added_tokens.json',\n",
       " 'tokenizer_samsum_su_finetune/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# ------------------------------\n",
    "# Load processed dataset & tokenizer\n",
    "# ------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load pretrained EncoderDecoderModel\n",
    "# ------------------------------\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\", \"bert-base-uncased\"\n",
    ")\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load your pretrained encoder weights\n",
    "model.encoder.load_state_dict(torch.load(\"bert_su_pretrained.pt\", map_location=\"cpu\"))\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# ------------------------------\n",
    "# Define training arguments\n",
    "# ------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Data Collator & Trainer\n",
    "# ------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Start Training\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "model.save_pretrained(\"bert_samsum_finetuned\")\n",
    "tokenizer.save_pretrained(\"tokenizer_samsum_su_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b105d9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed52141",
   "metadata": {},
   "source": [
    "1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) ใช้วัดความคล้ายกันระหว่างสรุปที่โมเดลสร้างขึ้นกับสรุปอ้างอิง โดยเน้นไปที่ recall เป็นหลัก\n",
    "\t- ROUGE-1 (R-1) = Unigram overlap (คำเดี่ยว)\n",
    "\t- ROUGE-2 (R-2) = Bigram overlap (คำติดกัน 2 คำ)\n",
    "\t- ROUGE-L (R-L) = ใช้ Longest common subsequence (LCS) ในการวัดความคล้ายเชิงลำดับคำที่ยาวที่สุดที่ปรากฏในทั้งสองสรุป โดยคำนึงถึงลำดับคำด้วย\n",
    "\n",
    "2. BLEU (Bilingual Evaluation Understudy) เดิมทีใช้ในงานแปลภาษา แต่ถูกประยุกต์ใช้ในงานสรุปข้อความได้เช่นกัน โดย BLEU จะเน้นการวัด precision คือดูว่า คำที่โมเดลสร้าง มีเท่าไรที่ตรงกับสรุปจริง ต่างจาก ROUGE ที่เน้น recall\n",
    "\t- BLEU วัดการทับซ้อนของ n-gram เช่น unigram, bigram, trigram\n",
    "\t- มีการใช้ brevity penalty หากสรุปสั้นกว่าที่ควรจะเป็น\n",
    "\n",
    "3. BERTScore (BS) ใช้ embedding จากโมเดล BERT หรือ Transformer ตัวอื่น ๆ ในการวัด semantic similarity (ความใกล้เคียงด้านความหมาย) ระหว่างสรุปของโมเดลกับสรุปจริง โดยไม่จำเป็นต้องใช้คำเหมือนกันเป๊ะเหมือนกับ ROUGE หรือ BLEU แต่ BERTScore จะวัดว่าคำหรือวลีมีความหมายใกล้เคียงกันหรือไม่\n",
    "\t- วัดความคล้ายกันของคำใน embedding space เช่น \"car\" vs \"vehicle\" ก็ยังถือว่าใกล้เคียง\n",
    "\t- ใช้ precision / recall / F1 score ตามระยะห่างของ vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc373f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "from bert_score import score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fb626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# โหลด dataset\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('tokenizer_samsum_su_finetune')\n",
    "model = EncoderDecoderModel.from_pretrained('bert_samsum_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453966ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ย้ายโมเดลและข้อมูลไปยังอุปกรณ์ที่เหมาะสม (GPU หรือ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# วิเคราะห์ข้อมูลและทำการ summary ด้วย bert_samsum_finetuned\n",
    "def generate_summary(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # ย้ายข้อมูลไปยัง device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            max_length=512, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=model.config.decoder_start_token_id,  # กำหนดที่นี่\n",
    "            pad_token_id=model.config.pad_token_id  # กำหนด pad_token_id ถ้าจำเป็น\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9eb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 819/819 [39:14<00:00,  2.88s/sample]\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.08103071050965537, 'rouge2': 0.005501493938462314, 'rougeL': 0.07293283175759344}\n",
      "BLEU Score: 8.859648156109322e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.8399370312690735 Recall: 0.8468782305717468 F1: 0.843207597732544\n"
     ]
    }
   ],
   "source": [
    "# 1. ROUGE Score Calculation\n",
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return {key: sum(value)/len(value) for key, value in scores.items()}\n",
    "\n",
    "# 2. BLEU Score Calculation\n",
    "def calculate_bleu(predictions, references):\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [ref.split()]\n",
    "        bleu_scores.append(sentence_bleu(ref_tokens, pred_tokens))\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# 3. BERTScore Calculation\n",
    "def calculate_bertscore(predictions, references):\n",
    "    P, R, F1 = score(predictions, references, lang='en')\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "# การทดสอบกับ dataset\n",
    "def evaluate_model(dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # ใช้ข้อมูลจาก train สำหรับทำนาย และข้อมูลจาก test สำหรับการเปรียบเทียบ\n",
    "    for i in tqdm(range(len(dataset['test'])), desc=\"Evaluating\", unit=\"sample\"):\n",
    "        # สร้างสรุปจากโมเดล\n",
    "        input_text = dataset['train'][i]['dialogue']  # ใช้ 'dialogue' จาก train เพื่อสร้างสรุป\n",
    "        reference_summary = dataset['test'][i]['summary']  # ใช้ 'summary' จาก test เป็นสรุปจริง\n",
    "        pred_summary = generate_summary(input_text)  # สร้างสรุปจากโมเดล\n",
    "        \n",
    "        predictions.append(pred_summary)\n",
    "        references.append(reference_summary)\n",
    "    \n",
    "    # ROUGE Score\n",
    "    rouge_scores = calculate_rouge(predictions, references)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu_score = calculate_bleu(predictions, references)\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = calculate_bertscore(predictions, references)\n",
    "    print(\"BERTScore - Precision:\", P, \"Recall:\", R, \"F1:\", F1)\n",
    "\n",
    "# เรียกใช้งาน\n",
    "evaluate_model(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsum-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
