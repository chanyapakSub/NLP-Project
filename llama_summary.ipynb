{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4aff11",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c2228",
   "metadata": {},
   "source": [
    "## Environment & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd19806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 0)  Install (first‐time only) & import libs\n",
    "# ---------------------------------------------\n",
    "# !pip install -q datasets transformers emoji==2.10.0 tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import emoji\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef5e48",
   "metadata": {},
   "source": [
    "## Load SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1a8833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1) Load SAMSum — 14 732 / 819 / 818 dialogues\n",
    "# ---------------------------------------------------------\n",
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35306eb1",
   "metadata": {},
   "source": [
    "## Build an emoji vocabulary and speaker token & Build / extend the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa85040",
   "metadata": {},
   "source": [
    "count [UNK] occurrences in one HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80902ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def count_unk(ds, tokenizer, field=\"dialogue\", batch_size=1024):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    total_unk, total_tokens = 0, 0\n",
    "\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc=\"Tokenising\"):\n",
    "        batch_texts = ds[i : i + batch_size][field]\n",
    "        enc = tokenizer(batch_texts, add_special_tokens=True, padding=False, truncation=False)\n",
    "        for ids in enc[\"input_ids\"]:\n",
    "            arr = np.array(ids)\n",
    "            total_unk += np.sum(arr == unk_id)\n",
    "            total_tokens += len(arr)\n",
    "    return total_unk, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468907",
   "metadata": {},
   "source": [
    "BEFORE adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0622d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenising:   0%|          | 0/15 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 22.20it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 37.81it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 39.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts BEFORE adding emoji tokens\n",
      "train     :     3758  (0.185% of tokens)\n",
      "validation:      191  (0.174% of tokens)\n",
      "test      :      195  (0.170% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_base = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "unk_stats_before = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_before[split] = count_unk(raw_ds[split], tok_base)\n",
    "print(\"\\n[UNK] counts BEFORE adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_before.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc3a6b",
   "metadata": {},
   "source": [
    "สร้าง EMOJI_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aeea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emojis found: 305\n"
     ]
    }
   ],
   "source": [
    "# ถ้า kernel เพิ่งรีสตาร์ต ตัวแปรจะหายหมด\n",
    "# สร้างชุด emoji ใหม่จาก raw_ds\n",
    "from typing import List\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    return [ch for ch in text if ch in emoji.EMOJI_DATA]\n",
    "\n",
    "emoji_set = set()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for dlg in raw_ds[split][\"dialogue\"]:\n",
    "        emoji_set.update(extract_emojis(dlg))\n",
    "\n",
    "EMOJI_TOKENS = sorted(emoji_set)          # ≈ 300-320 รายการ\n",
    "print(f\"Unique emojis found: {len(EMOJI_TOKENS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b121db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aebd7e",
   "metadata": {},
   "source": [
    "Extend tokenizer with emojis + speaker tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef28354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size : 128256\n",
      "Added new tokens     : 315  (emoji = 305, speaker = 10)\n",
      "New vocab size       : 128571\n",
      "\n",
      "First 20 emoji tokens: ['‼', '⏱', '☀', '☂', '☔', '☕', '☘', '☝', '☠', '☢', '☹', '☺', '♀', '♂', '♥', '♻', '⚪', '⚫', '⚰', '⚽']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------- 1) โหลด tokenizer ดั้งเดิม ----------\n",
    "tok_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "vocab_orig = len(tok_base)\n",
    "\n",
    "# ---------- 2) เตรียมชุด token ใหม่ ----------\n",
    "#   • EMOJI_TOKENS  : ทุกอิโมจิที่ “พบอย่างน้อย 1 ครั้ง” ใน SAMSum\n",
    "#   • SPEAKER_TOKENS: [S1] – [S10]\n",
    "SPEAKER_TOKENS = [f\"[S{i}]\" for i in range(1, 11)]\n",
    "new_tokens = EMOJI_TOKENS + SPEAKER_TOKENS\n",
    "\n",
    "# ---------- 3) สร้าง tokenizer สำเนาแล้วเพิ่ม token ----------\n",
    "tok_ext = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "added = tok_ext.add_tokens(new_tokens)\n",
    "vocab_new = len(tok_ext)\n",
    "\n",
    "# ---------- 4) แสดงผล ----------\n",
    "print(f\"Original vocab size : {vocab_orig}\")\n",
    "print(f\"Added new tokens     : {added}  \"\n",
    "      f\"(emoji = {len(EMOJI_TOKENS)}, speaker = {len(SPEAKER_TOKENS)})\")\n",
    "print(f\"New vocab size       : {vocab_new}\")\n",
    "\n",
    "# (Optional) พิมพ์ตัวอย่างอิโมจิ 20 ตัวแรก\n",
    "print(\"\\nFirst 20 emoji tokens:\", EMOJI_TOKENS[:20])\n",
    "\n",
    "tok_ext.save_pretrained(\"tokenizer_samsum_su\")   # โฟลเดอร์ใหม่"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc38ac",
   "metadata": {},
   "source": [
    "AFTER adding emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bc9fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising: 100%|██████████| 15/15 [00:00<00:00, 27.50it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 36.80it/s]\n",
      "Tokenising: 100%|██████████| 1/1 [00:00<00:00, 36.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[UNK] counts AFTER adding emoji tokens\n",
      "train     :        0  (0.000% of tokens)\n",
      "validation:        0  (0.000% of tokens)\n",
      "test      :        0  (0.000% of tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unk_stats_after = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    unk_stats_after[split] = count_unk(raw_ds[split], tok_ext)\n",
    "print(\"\\n[UNK] counts AFTER adding emoji tokens\")\n",
    "for split, (u, t) in unk_stats_after.items():\n",
    "    print(f\"{split:<10}: {u:8d}  ({u/t:.3%} of tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb039",
   "metadata": {},
   "source": [
    "reduction check in UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8480431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Δ [UNK] (before ➜ after):\n",
      "train     : +3758  fewer UNKs  (↓100.00%)\n",
      "validation: +191  fewer UNKs  (↓100.00%)\n",
      "test      : +195  fewer UNKs  (↓100.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nΔ [UNK] (before ➜ after):\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    u0, _ = unk_stats_before[split]\n",
    "    u1, _ = unk_stats_after[split]\n",
    "    print(f\"{split:<10}: {u0-u1:+d}  fewer UNKs  (↓{(u0-u1)/u0:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd47d8f",
   "metadata": {},
   "source": [
    "## Preprocess SAMSum Dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d4138",
   "metadata": {},
   "source": [
    "Speaker-name mapping → [S#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20077039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) Helper to replace speaker names by [S#]\n",
    "# ---------------------------------------------------------\n",
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10\n",
    "                 ) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns dialogue with names replaced by [S#] and a mapping dict.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:                # safety – keep line as is\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:      # truncate extra speakers\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        new_lines.append(f\"{speaker_map.get(name, '[SUNK]')}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad9bc",
   "metadata": {},
   "source": [
    "Insert [SEP] after every utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c65bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02e888",
   "metadata": {},
   "source": [
    "Switching-Utterance corruption\n",
    "- Hyper-parameters: Pu = 1.0, Pn = 0/1\n",
    "\n",
    "โดยที่\n",
    "\n",
    "Pu (permute-utterance prob.) ความน่าจะเป็นที่ แต่ละ utterance จะถูกเลือก ใส่ลงในชุดที่นำไปสับตำแหน่ง\n",
    "\n",
    "- pu = 1.0 แสดงว่าบังคับเลือกทุกบรรทัดแล้วค่อยสับคำแบบสุ่ม\n",
    "\n",
    "Pn (name-mask prob.) ความน่าจะเป็นที่ token [S#] ด้านหน้าจะถูกเปลี่ยนเป็น [MASK]\n",
    "\n",
    "- pn = 0.0 แสดงว่า ไม่ mask, โมเดลเห็น speaker tag\n",
    "\n",
    "- pn = 1.0 แสดงว่า mask หมด, บังคับดู context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85ef01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_switching_utterance(dialogue: str,\n",
    "                             pu: float = 1.0,\n",
    "                             pn: float = 0.0,\n",
    "                             rng: random.Random = random\n",
    "                            ) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    • dialogue  - speaker-tokenised, SEP-inserted string\n",
    "    • pu        - prob. an utterance is selected for permutation\n",
    "    • pn        - prob. we MASK the speaker token (⇒ [MASK])\n",
    "    Returns:\n",
    "        corrupted_dialogue, labels_per_utt  (1 = permuted (สลับบทพูด), 0 = original)\n",
    "    \"\"\"\n",
    "    # 1) split back into utterances\n",
    "    utts = [u.strip() for u in dialogue.split(\"[SEP]\") if u.strip()]\n",
    "    idxs = list(range(len(utts)))\n",
    "\n",
    "    # 2) pick indices to permute\n",
    "    perm_idx = [i for i in idxs if rng.random() < pu]\n",
    "    shuffled = perm_idx.copy()\n",
    "    rng.shuffle(shuffled)                 # in-place\n",
    "    perm_map = dict(zip(perm_idx, shuffled))\n",
    "\n",
    "    # 3) build new utterance list, labels\n",
    "    new_utts, labels = [], []\n",
    "    for i in idxs:\n",
    "        src = perm_map.get(i, i)          # swapped or same\n",
    "        u = utts[src]\n",
    "        # optionally mask speaker token ([S#]: → [MASK]:)\n",
    "        if rng.random() < pn:\n",
    "            u = re.sub(r\"^\\[S\\d+\\]\", \"[MASK]\", u)\n",
    "        new_utts.append(u)\n",
    "        labels.append(int(src != i))      # 1 if permuted\n",
    "    corrupted = \" [SEP] \".join(new_utts) + \" [SEP]\"\n",
    "    return corrupted, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d05b",
   "metadata": {},
   "source": [
    "## Switching-Utterance (SU) pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5949ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 551464.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 206422.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 228172.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'sep_positions', 'dialogue_len'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7) Create HF Datasets with tokenised inputs, attention,\n",
    "#    SEP positions, and per-utterance labels\n",
    "# ---------------------------------------------------------\n",
    "MAX_LEN = 512                          # paper setting\n",
    "Pu, Pn = 1.0, 0.0                      # best config in Table 2\n",
    "\n",
    "\n",
    "def preprocess_example(example, split):\n",
    "\n",
    "    if tok_ext.pad_token is None:\n",
    "        tok_ext.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "    # a) replace speakers & add SEP\n",
    "    dlg, _ = map_speakers(example[\"dialogue\"])\n",
    "    dlg = add_sep_every_utt(dlg)\n",
    "\n",
    "    # b) corruption\n",
    "    corrupted, labels = make_switching_utterance(dlg, Pu, Pn)\n",
    "\n",
    "    # c) tokenize (truncate if >512 tokens)\n",
    "    enc = tok_ext(corrupted,\n",
    "              truncation=True, max_length=MAX_LEN,\n",
    "              padding=\"max_length\")\n",
    "    \n",
    "    # d) find SEP token positions (needed for loss later)\n",
    "    sep_id = tok_ext(\"[SEP]\")[\"input_ids\"][0]\n",
    "    sep_positions = [i for i, id_ in enumerate(enc[\"input_ids\"])\n",
    "                     if id_ == sep_id][:len(labels)]  # clip if truncated\n",
    "\n",
    "    enc[\"labels\"] = labels[:len(sep_positions)]\n",
    "    enc[\"sep_positions\"] = sep_positions\n",
    "    enc[\"dialogue_len\"] = len(labels)\n",
    "    return enc\n",
    "\n",
    "su_ds = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    su_ds[split] = raw_ds[split].map(\n",
    "        preprocess_example,\n",
    "        fn_kwargs={\"split\": split},\n",
    "        remove_columns=raw_ds[split].column_names,\n",
    "        desc=f\"Building SU {split}\"\n",
    "    )\n",
    "\n",
    "su_ds.save_to_disk(\"data/samsum_switching_utterance\")\n",
    "print(su_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4605bbc",
   "metadata": {},
   "source": [
    "ไฟล์ Arrow ถูกบันทึกไว้ที่ data/samsum_switching_utterance/ พร้อมฟิลด์ input_ids / attention_mask / labels / sep_positions / dialogue_len."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b54f7",
   "metadata": {},
   "source": [
    "## Self-supervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c9c0",
   "metadata": {},
   "source": [
    "ใช้ Dataset เฉพาะส่วนของ train ของ SAMSum มาทำการ pre_train แล้วใช้ validation ไว้ดู early-stopping / tuning ส่วน test ต้องไม่ถูกแตะ เพื่อไม่ให้โมเดล “เห็น” บทสนทนาที่จะใช้วัด ROUGE ภายหลัง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363e9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc246a",
   "metadata": {},
   "source": [
    "Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da87342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # paper 128\n",
    "MAX_LEN    = 512 # paper 512\n",
    "LR         = 3e-5\n",
    "WARMUP     = 500\n",
    "MAX_STEPS  = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc898da",
   "metadata": {},
   "source": [
    "Dataset & collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5875dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_switching_utterance\")\n",
    "\n",
    "# -------------------------------\n",
    "# COLLATE FUNCTION\n",
    "# -------------------------------\n",
    "# def collate_fn(batch):\n",
    "#     keys = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "#     inputs = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
    "#     labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "#     sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "#     return inputs, labels, sep_pos\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    inputs = {\n",
    "        k: torch.stack([torch.tensor(b[k]) if not isinstance(b[k], torch.Tensor) else b[k] for b in batch])\n",
    "        for k in keys if k not in [\"labels\", \"sep_positions\"]\n",
    "    }\n",
    "    labels = [torch.tensor(b[\"labels\"], dtype=torch.float) for b in batch]\n",
    "    sep_pos = [torch.tensor(b[\"sep_positions\"]) for b in batch]\n",
    "    return inputs, labels, sep_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ca8c2",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f94a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MODEL\n",
    "# -------------------------------\n",
    "class SepClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-3.2-1B\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.llama = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            torch_dtype=torch.float16  # use float16 to reduce memory usage\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sep_positions, **kwargs):\n",
    "        hidden_states = self.llama(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # Collect hidden states at each [SEP] position\n",
    "        sep_vecs = []\n",
    "        for i, pos_tensor in enumerate(sep_positions):\n",
    "            pos_tensor = pos_tensor.to(hidden_states.device).long()  # <-- Ensure position tensor is long\n",
    "            sep_vecs.append(hidden_states[i].index_select(0, pos_tensor))  # (U_i, H)\n",
    "\n",
    "        sep_vecs = torch.cat(sep_vecs, dim=0)  # Shape: (total_seps, hidden_size)\n",
    "        logits = self.classifier(self.dropout(sep_vecs)).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307a9de",
   "metadata": {},
   "source": [
    "Training loop (train model until the train loss converged (upper bounded by 5k steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26145b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep : 0\n",
      "Step 0 is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput ID exceeds model embedding size (vocab_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Forward + loss + backward\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_lists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, flat_labels)\n\u001b[1;32m     75\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mSepClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, sep_positions, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, sep_positions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Collect hidden states at each [SEP] position\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     sep_vecs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# INITIALIZATION\n",
    "# -------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load tokenizer and ensure it has all required special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# Add special tokens if missing\n",
    "special_tokens = {}\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens[\"pad_token\"] = \"[PAD]\"\n",
    "if tokenizer.sep_token is None:\n",
    "    special_tokens[\"sep_token\"] = \"[SEP]\"\n",
    "\n",
    "if special_tokens:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Initialize model\n",
    "model = SepClassifier()\n",
    "\n",
    "# Resize token embeddings to match new tokenizer length\n",
    "model.llama.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Setup DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer, scheduler, loss\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = min(MAX_STEPS, len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):  # loop until MAX_STEPS reached\n",
    "    print(f\"Ep : {epoch}\")\n",
    "    for inputs, label_lists, sep_lists in train_loader:\n",
    "        print(f\"Step {step} is training\")\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        # Get model's device\n",
    "        model_device = model.llama.device if hasattr(model.llama, 'device') else next(model.parameters()).device\n",
    "\n",
    "        # Move inputs\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "        sep_lists = [s.to(model_device) for s in sep_lists]\n",
    "        flat_labels = torch.cat(label_lists).to(model_device)\n",
    "\n",
    "        # Safety check: ensure all input IDs are within vocab\n",
    "        vocab_size = model.llama.get_input_embeddings().weight.shape[0]\n",
    "        if (inputs[\"input_ids\"] >= vocab_size).any():\n",
    "            raise ValueError(f\"Input ID exceeds model embedding size (vocab_size={vocab_size})\")\n",
    "\n",
    "        # Forward + loss + backward\n",
    "        logits = model(**inputs, sep_positions=sep_lists)\n",
    "        loss = loss_fn(logits, flat_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        print(f\"Step {step:4d}/{total_steps} | Loss: {running_loss}\")\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step:4d}/{total_steps} | AVG Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    if step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE MODEL\n",
    "# -------------------------------\n",
    "torch.save(model.state_dict(), \"llama_su_pretrained.pt\")\n",
    "print(\"Model saved to 'llama_su_pretrained.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521faa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.bert.state_dict(), \"llama_su_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459a3fb",
   "metadata": {},
   "source": [
    "Validation & early-stop (optional)\n",
    "\n",
    "- Use the same DataLoader/loop on su_ds[\"validation\"], compute average BCE loss; if it plateaus you can stop earlier than 5 k steps (what the authors mean by “until train loss converged”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b890a7",
   "metadata": {},
   "source": [
    "# Create Summarization Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3b258",
   "metadata": {},
   "source": [
    "ขั้นตอนการทำ preprocess\n",
    "1. โหลดชุดข้อมูล SAMSum\n",
    "2. ทำ preprocessing:\n",
    "    - แทนชื่อ speaker ด้วย [S1]–[S10]\n",
    "    - เติม [SEP] ท้ายทุกประโยค\n",
    "    - ใช้ tokenizer เดิมจาก pretraining (tokenizer_samsum_su)\n",
    "    - truncate/pad ความยาวที่ max_length = 512\n",
    "3. แปลงให้อยู่ในรูปแบบที่พร้อมใช้สำหรับ Seq2SeqTrainer\n",
    "4. Save เป็นไฟล์ .pt หรือ DatasetDict ที่พร้อมใช้งาน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ae8f1",
   "metadata": {},
   "source": [
    "Load SAMSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d58290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14732, 'test': 819, 'validation': 818}\n"
     ]
    }
   ],
   "source": [
    "raw_ds: DatasetDict = load_dataset(\"samsum\")\n",
    "print({k: len(v) for k, v in raw_ds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e39b4",
   "metadata": {},
   "source": [
    "Load Pretrained Tokenizer (same as used during pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaf5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_samsum_su\")\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7f9b0",
   "metadata": {},
   "source": [
    "Speaker Normalization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_RE = re.compile(r\"^([^:]+):\\s*(.*)$\")\n",
    "\n",
    "def map_speakers(dialogue: str, max_speakers: int = 10) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Replace speaker names with generic [S1], [S2], ... tokens.\n",
    "    \"\"\"\n",
    "    speaker_map, next_id = {}, 1\n",
    "    new_lines = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        m = SPEAKER_RE.match(line)\n",
    "        if not m:\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "        name, utt = m.groups()\n",
    "        if name not in speaker_map:\n",
    "            if next_id > max_speakers:\n",
    "                name_token = \"[SUNK]\"\n",
    "            else:\n",
    "                name_token = f\"[S{next_id}]\"\n",
    "                speaker_map[name] = name_token\n",
    "                next_id += 1\n",
    "        name_token = speaker_map.get(name, \"[SUNK]\")\n",
    "        new_lines.append(f\"{name_token}: {utt}\")\n",
    "    return \"\\n\".join(new_lines), speaker_map\n",
    "\n",
    "def add_sep_every_utt(dialogue: str) -> str:\n",
    "    lines = [l + \" [SEP]\" for l in dialogue.split(\"\\n\") if l.strip()]\n",
    "    return \" \".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c45323",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(example):\n",
    "    normed_dialogue, _ = map_speakers(example[\"dialogue\"])\n",
    "    sep_dialogue = add_sep_every_utt(normed_dialogue)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        sep_dialogue,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": targets[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb81812",
   "metadata": {},
   "source": [
    "Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebaf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:08<00:00, 1769.02 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 1693.37 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 1804.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 327887.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 172195.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 167281.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to 'samsum_finetune_ready'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = raw_ds.map(preprocess_fn, batched=False)\n",
    "tokenized_ds.save_to_disk(\"samsum_finetune_ready\")\n",
    "print(\"Preprocessed dataset saved to 'samsum_finetune_ready'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Fine-tuning train: 100%|██████████| 14732/14732 [00:09<00:00, 1561.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 183818.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 129347.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 126710.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MAX_LEN = 512  # paper setting\n",
    "\n",
    "# def preprocess_example(example, split):\n",
    "#     # a) replace speakers & add SEP (same as pretraining)\n",
    "#     dlg, _ = map_speakers(example[\"dialogue\"])  # แปลงชื่อให้เป็น token สั้น ๆ เช่น <USR1>\n",
    "#     dlg = add_sep_every_utt(dlg)                # เพิ่ม [SEP] ทุกท้ายประโยค\n",
    "\n",
    "#     # b) tokenize dialogue input\n",
    "#     enc = tok_base(dlg,\n",
    "#               truncation=True,\n",
    "#               max_length=MAX_LEN,\n",
    "#               padding=\"max_length\")\n",
    "\n",
    "#     # c) tok_baseenize target summary\n",
    "#     with tok_base.as_target_tokenizer():\n",
    "#         summary = example[\"summary\"]\n",
    "#         summary_enc = tok_base(summary,\n",
    "#                           truncation=True,\n",
    "#                           max_length=MAX_LEN,\n",
    "#                           padding=\"max_length\")\n",
    "    \n",
    "#     # d) pack input and label\n",
    "#     enc[\"labels\"] = summary_enc[\"input_ids\"]\n",
    "#     return enc\n",
    "\n",
    "# # สร้าง dataset ใหม่สำหรับ fine-tune\n",
    "# finetune_ds = DatasetDict()\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     finetune_ds[split] = raw_ds[split].map(\n",
    "#         preprocess_example,\n",
    "#         fn_kwargs={\"split\": split},\n",
    "#         remove_columns=raw_ds[split].column_names,\n",
    "#         desc=f\"Building Fine-tuning {split}\"\n",
    "#     )\n",
    "\n",
    "# finetune_ds.save_to_disk(\"data/samsum_finetune\")\n",
    "# print(finetune_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e9327",
   "metadata": {},
   "source": [
    "# Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a197c0",
   "metadata": {},
   "source": [
    "\n",
    "**เทียบกับ Paper**\n",
    "\n",
    "| **Parameter**       | **Code**                          | **Paper (Section 3.2)**          | **เหมือน / ไม่เหมือน**        |\n",
    "| ------------------- | --------------------------------- | -------------------------------- | --------------       |\n",
    "| Model               | BERT2BERT (EncoderDecoderModel)   | BERT2BERT                        | เหมือน                 |\n",
    "| Tokenizer           | bert-base-uncased + custom tokens | ใช้ tokenizer ดัดแปลง              | เหมือน              |\n",
    "| Batch Size          | 8                                 | **16 (per step)**                | ไม่เหมือน → เล็กกว่า  |\n",
    "| Epochs              | 3                                 | 3                                | เหมือน              |\n",
    "| Learning Rate       | 5e-5                              | **3e-5**                         | ไม่เหมือน → สูงกว่า   |\n",
    "| Warmup Steps        | 500                               | ใช้ scheduler (แต่ไม่ระบุ exact)     | เหมือน (สมเหตุสมผล) |\n",
    "| Max Length (input)  | 512                               | 512                              | เหมือน              |\n",
    "| Max Length (output) | 128                               | 128                              | เหมือน              |\n",
    "| Beam Search         | 4                                 | 4                                | เหมือน              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3beebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/5526 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "  9%|▉         | 500/5526 [01:36<16:14,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4418, 'grad_norm': 0.3646565079689026, 'learning_rate': 4.96e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  9%|▉         | 501/5526 [01:41<2:22:17,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22444657981395721, 'eval_runtime': 5.0061, 'eval_samples_per_second': 163.402, 'eval_steps_per_second': 20.575, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1000/5526 [03:17<14:36,  5.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2172, 'grad_norm': 0.4135509133338928, 'learning_rate': 4.506565857540788e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 18%|█▊        | 1000/5526 [03:22<14:36,  5.17it/s]/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19904808700084686, 'eval_runtime': 5.0413, 'eval_samples_per_second': 162.259, 'eval_steps_per_second': 20.431, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 27%|██▋       | 1500/5526 [05:00<12:53,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2008, 'grad_norm': 0.274005264043808, 'learning_rate': 4.009152407481098e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 27%|██▋       | 1501/5526 [05:06<1:53:34,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1895398199558258, 'eval_runtime': 4.9978, 'eval_samples_per_second': 163.672, 'eval_steps_per_second': 20.609, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2000/5526 [06:42<11:18,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1864, 'grad_norm': 0.31300443410873413, 'learning_rate': 3.511738957421409e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 2000/5526 [06:47<11:18,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18359985947608948, 'eval_runtime': 4.9794, 'eval_samples_per_second': 164.278, 'eval_steps_per_second': 20.685, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 45%|████▌     | 2500/5526 [08:25<09:41,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17, 'grad_norm': 0.36899664998054504, 'learning_rate': 3.0143255073617192e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▌     | 2501/5526 [08:30<1:25:32,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17844413220882416, 'eval_runtime': 5.0089, 'eval_samples_per_second': 163.311, 'eval_steps_per_second': 20.564, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3000/5526 [10:06<08:05,  5.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1689, 'grad_norm': 0.43778106570243835, 'learning_rate': 2.5169120573020293e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 54%|█████▍    | 3000/5526 [10:11<08:05,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17464406788349152, 'eval_runtime': 4.984, 'eval_samples_per_second': 164.126, 'eval_steps_per_second': 20.666, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 63%|██████▎   | 3500/5526 [11:50<06:25,  5.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1632, 'grad_norm': 0.3436298966407776, 'learning_rate': 2.01949860724234e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|██████▎   | 3501/5526 [11:55<56:57,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17154935002326965, 'eval_runtime': 4.9824, 'eval_samples_per_second': 164.179, 'eval_steps_per_second': 20.673, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 4000/5526 [13:31<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1487, 'grad_norm': 0.4152975380420685, 'learning_rate': 1.5220851571826503e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 4000/5526 [13:36<04:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17133557796478271, 'eval_runtime': 4.98, 'eval_samples_per_second': 164.259, 'eval_steps_per_second': 20.683, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 81%|████████▏ | 4500/5526 [15:14<03:17,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1356, 'grad_norm': 0.35845550894737244, 'learning_rate': 1.0246717071229607e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 81%|████████▏ | 4501/5526 [15:19<28:52,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1692614108324051, 'eval_runtime': 4.9857, 'eval_samples_per_second': 164.068, 'eval_steps_per_second': 20.659, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 5000/5526 [16:55<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1402, 'grad_norm': 0.3714558780193329, 'learning_rate': 5.27258257063271e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 5000/5526 [17:00<01:41,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1681322604417801, 'eval_runtime': 4.9793, 'eval_samples_per_second': 164.281, 'eval_steps_per_second': 20.686, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "100%|█████████▉| 5500/5526 [18:38<00:04,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1387, 'grad_norm': 0.4560355544090271, 'learning_rate': 2.984480700358138e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|█████████▉| 5501/5526 [18:43<00:42,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1668214201927185, 'eval_runtime': 4.9802, 'eval_samples_per_second': 164.251, 'eval_steps_per_second': 20.682, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5526/5526 [18:50<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1130.5819, 'train_samples_per_second': 39.091, 'train_steps_per_second': 4.888, 'train_loss': 0.28212739849194124, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_samsum_su_finetune/tokenizer_config.json',\n",
       " 'tokenizer_samsum_su_finetune/special_tokens_map.json',\n",
       " 'tokenizer_samsum_su_finetune/vocab.txt',\n",
       " 'tokenizer_samsum_su_finetune/added_tokens.json',\n",
       " 'tokenizer_samsum_su_finetune/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# ------------------------------\n",
    "# Load processed dataset & tokenizer\n",
    "# ------------------------------\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer_samsum_su\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load pretrained EncoderDecoderModel\n",
    "# ------------------------------\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\", \"bert-base-uncased\"\n",
    ")\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load your pretrained encoder weights\n",
    "model.encoder.load_state_dict(torch.load(\"bert_su_pretrained.pt\", map_location=\"cpu\"))\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# ------------------------------\n",
    "# Define training arguments\n",
    "# ------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Data Collator & Trainer\n",
    "# ------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Start Training\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "model.save_pretrained(\"bert_samsum_finetuned\")\n",
    "tokenizer.save_pretrained(\"tokenizer_samsum_su_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b105d9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed52141",
   "metadata": {},
   "source": [
    "1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) ใช้วัดความคล้ายกันระหว่างสรุปที่โมเดลสร้างขึ้นกับสรุปอ้างอิง โดยเน้นไปที่ recall เป็นหลัก\n",
    "\t- ROUGE-1 (R-1) = Unigram overlap (คำเดี่ยว)\n",
    "\t- ROUGE-2 (R-2) = Bigram overlap (คำติดกัน 2 คำ)\n",
    "\t- ROUGE-L (R-L) = ใช้ Longest common subsequence (LCS) ในการวัดความคล้ายเชิงลำดับคำที่ยาวที่สุดที่ปรากฏในทั้งสองสรุป โดยคำนึงถึงลำดับคำด้วย\n",
    "\n",
    "2. BLEU (Bilingual Evaluation Understudy) เดิมทีใช้ในงานแปลภาษา แต่ถูกประยุกต์ใช้ในงานสรุปข้อความได้เช่นกัน โดย BLEU จะเน้นการวัด precision คือดูว่า คำที่โมเดลสร้าง มีเท่าไรที่ตรงกับสรุปจริง ต่างจาก ROUGE ที่เน้น recall\n",
    "\t- BLEU วัดการทับซ้อนของ n-gram เช่น unigram, bigram, trigram\n",
    "\t- มีการใช้ brevity penalty หากสรุปสั้นกว่าที่ควรจะเป็น\n",
    "\n",
    "3. BERTScore (BS) ใช้ embedding จากโมเดล BERT หรือ Transformer ตัวอื่น ๆ ในการวัด semantic similarity (ความใกล้เคียงด้านความหมาย) ระหว่างสรุปของโมเดลกับสรุปจริง โดยไม่จำเป็นต้องใช้คำเหมือนกันเป๊ะเหมือนกับ ROUGE หรือ BLEU แต่ BERTScore จะวัดว่าคำหรือวลีมีความหมายใกล้เคียงกันหรือไม่\n",
    "\t- วัดความคล้ายกันของคำใน embedding space เช่น \"car\" vs \"vehicle\" ก็ยังถือว่าใกล้เคียง\n",
    "\t- ใช้ precision / recall / F1 score ตามระยะห่างของ vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc373f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "from bert_score import score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fb626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# โหลด dataset\n",
    "dataset = load_from_disk(\"data/samsum_finetune_ready\")\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('tokenizer_samsum_su_finetune')\n",
    "model = EncoderDecoderModel.from_pretrained('bert_samsum_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453966ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ย้ายโมเดลและข้อมูลไปยังอุปกรณ์ที่เหมาะสม (GPU หรือ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# วิเคราะห์ข้อมูลและทำการ summary ด้วย bert_samsum_finetuned\n",
    "def generate_summary(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # ย้ายข้อมูลไปยัง device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            max_length=512, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=model.config.decoder_start_token_id,  # กำหนดที่นี่\n",
    "            pad_token_id=model.config.pad_token_id  # กำหนด pad_token_id ถ้าจำเป็น\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9eb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 819/819 [39:14<00:00,  2.88s/sample]\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.08103071050965537, 'rouge2': 0.005501493938462314, 'rougeL': 0.07293283175759344}\n",
      "BLEU Score: 8.859648156109322e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drl-68/miniconda3/envs/samsum-bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.8399370312690735 Recall: 0.8468782305717468 F1: 0.843207597732544\n"
     ]
    }
   ],
   "source": [
    "# 1. ROUGE Score Calculation\n",
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return {key: sum(value)/len(value) for key, value in scores.items()}\n",
    "\n",
    "# 2. BLEU Score Calculation\n",
    "def calculate_bleu(predictions, references):\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [ref.split()]\n",
    "        bleu_scores.append(sentence_bleu(ref_tokens, pred_tokens))\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# 3. BERTScore Calculation\n",
    "def calculate_bertscore(predictions, references):\n",
    "    P, R, F1 = score(predictions, references, lang='en')\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "# การทดสอบกับ dataset\n",
    "def evaluate_model(dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # ใช้ข้อมูลจาก train สำหรับทำนาย และข้อมูลจาก test สำหรับการเปรียบเทียบ\n",
    "    for i in tqdm(range(len(dataset['test'])), desc=\"Evaluating\", unit=\"sample\"):\n",
    "        # สร้างสรุปจากโมเดล\n",
    "        input_text = dataset['train'][i]['dialogue']  # ใช้ 'dialogue' จาก train เพื่อสร้างสรุป\n",
    "        reference_summary = dataset['test'][i]['summary']  # ใช้ 'summary' จาก test เป็นสรุปจริง\n",
    "        pred_summary = generate_summary(input_text)  # สร้างสรุปจากโมเดล\n",
    "        \n",
    "        predictions.append(pred_summary)\n",
    "        references.append(reference_summary)\n",
    "    \n",
    "    # ROUGE Score\n",
    "    rouge_scores = calculate_rouge(predictions, references)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu_score = calculate_bleu(predictions, references)\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = calculate_bertscore(predictions, references)\n",
    "    print(\"BERTScore - Precision:\", P, \"Recall:\", R, \"F1:\", F1)\n",
    "\n",
    "# เรียกใช้งาน\n",
    "evaluate_model(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsum-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
